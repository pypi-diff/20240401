# Comparing `tmp/nemoguardrails-0.8.1-py3-none-any.whl.zip` & `tmp/nemoguardrails-0.8.2-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,336 +1,449 @@
-Zip file size: 2505430 bytes, number of entries: 334
--rw-r--r--  2.0 unx      240 b- defN 24-Mar-15 10:01 chat-ui/README.md
--rw-r--r--  2.0 unx     2512 b- defN 24-Mar-15 10:01 chat-ui/frontend/404.html
--rw-r--r--  2.0 unx    15406 b- defN 24-Mar-15 10:01 chat-ui/frontend/favicon.ico
--rw-r--r--  2.0 unx     2098 b- defN 24-Mar-15 10:01 chat-ui/frontend/index.html
--rw-r--r--  2.0 unx       88 b- defN 24-Mar-15 10:01 chat-ui/frontend/_next/data/c_3yGnIXISqv9z1fDmxxC/index.json
--rw-r--r--  2.0 unx      368 b- defN 24-Mar-15 10:01 chat-ui/frontend/_next/static/c_3yGnIXISqv9z1fDmxxC/_buildManifest.js
--rw-r--r--  2.0 unx       89 b- defN 24-Mar-15 10:01 chat-ui/frontend/_next/static/c_3yGnIXISqv9z1fDmxxC/_ssgManifest.js
--rw-r--r--  2.0 unx  2129821 b- defN 24-Mar-15 10:01 chat-ui/frontend/_next/static/chunks/105-4bc220fcbf235939.js
--rw-r--r--  2.0 unx   611276 b- defN 24-Mar-15 10:01 chat-ui/frontend/_next/static/chunks/542b50fd-07ebedc579cef971.js
--rw-r--r--  2.0 unx   141075 b- defN 24-Mar-15 10:01 chat-ui/frontend/_next/static/chunks/framework-73b8966a3c579ab0.js
--rw-r--r--  2.0 unx    90429 b- defN 24-Mar-15 10:01 chat-ui/frontend/_next/static/chunks/main-6260d066cf2cd7b1.js
--rw-r--r--  2.0 unx    91460 b- defN 24-Mar-15 10:01 chat-ui/frontend/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js
--rw-r--r--  2.0 unx     1607 b- defN 24-Mar-15 10:01 chat-ui/frontend/_next/static/chunks/webpack-59c5c889f52620d6.js
--rw-r--r--  2.0 unx    75294 b- defN 24-Mar-15 10:01 chat-ui/frontend/_next/static/chunks/pages/_app-fe4220f7110f9c0d.js
--rw-r--r--  2.0 unx      251 b- defN 24-Mar-15 10:01 chat-ui/frontend/_next/static/chunks/pages/_error-3f6d1c55bb8051ab.js
--rw-r--r--  2.0 unx    55866 b- defN 24-Mar-15 10:01 chat-ui/frontend/_next/static/chunks/pages/index-a45704f8dbae891c.js
--rw-r--r--  2.0 unx    39792 b- defN 24-Mar-15 10:01 chat-ui/frontend/_next/static/css/c15848365b5406ac.css
--rw-r--r--  2.0 unx    37780 b- defN 24-Mar-15 10:01 chat-ui/frontend/_next/static/media/2aaf0723e720e8b9-s.p.woff2
--rw-r--r--  2.0 unx    57244 b- defN 24-Mar-15 10:01 chat-ui/frontend/_next/static/media/9c4f34569c9b36ca-s.woff2
--rw-r--r--  2.0 unx    11924 b- defN 24-Mar-15 10:01 chat-ui/frontend/_next/static/media/ae9ae6716d4f8bf8-s.woff2
--rw-r--r--  2.0 unx     8652 b- defN 24-Mar-15 10:01 chat-ui/frontend/_next/static/media/b1db3e28af9ef94a-s.woff2
--rw-r--r--  2.0 unx    17040 b- defN 24-Mar-15 10:01 chat-ui/frontend/_next/static/media/b967158bc7d7a9fb-s.woff2
--rw-r--r--  2.0 unx    21960 b- defN 24-Mar-15 10:01 chat-ui/frontend/_next/static/media/c0f5ec5bbf5913b7-s.woff2
--rw-r--r--  2.0 unx    26728 b- defN 24-Mar-15 10:01 chat-ui/frontend/_next/static/media/d1d9458b69004127-s.woff2
--rw-r--r--  2.0 unx    15134 b- defN 24-Mar-15 10:01 chat-ui/frontend/static/bot-avatar.png
--rw-r--r--  2.0 unx    12124 b- defN 24-Mar-15 10:01 chat-ui/frontend/static/user-avatar.png
--rw-r--r--  2.0 unx      679 b- defN 24-Mar-15 10:01 examples/__init__.py
--rw-r--r--  2.0 unx     1159 b- defN 24-Mar-15 10:01 examples/sample_config.yml
--rw-r--r--  2.0 unx      398 b- defN 24-Mar-15 10:01 examples/bots/README.md
--rw-r--r--  2.0 unx     1641 b- defN 24-Mar-15 10:01 examples/bots/abc/README.md
--rw-r--r--  2.0 unx     1256 b- defN 24-Mar-15 10:01 examples/bots/abc/config.yml
--rw-r--r--  2.0 unx     1715 b- defN 24-Mar-15 10:01 examples/bots/abc/prompts.yml
--rw-r--r--  2.0 unx    14650 b- defN 24-Mar-15 10:01 examples/bots/abc/kb/employee-handbook.md
--rw-r--r--  2.0 unx     6993 b- defN 24-Mar-15 10:01 examples/bots/abc/rails/disallowed.co
--rw-r--r--  2.0 unx      771 b- defN 24-Mar-15 10:01 examples/bots/hello_world/README.md
--rw-r--r--  2.0 unx       76 b- defN 24-Mar-15 10:01 examples/bots/hello_world/config.yml
--rw-r--r--  2.0 unx      939 b- defN 24-Mar-15 10:01 examples/bots/hello_world/rails.co
--rw-r--r--  2.0 unx      238 b- defN 24-Mar-15 10:01 examples/configs/README.md
--rw-r--r--  2.0 unx      679 b- defN 24-Mar-15 10:01 examples/configs/__init__.py
--rw-r--r--  2.0 unx     2135 b- defN 24-Mar-15 10:01 examples/configs/guardrails_only/README.md
--rw-r--r--  2.0 unx     2532 b- defN 24-Mar-15 10:01 examples/configs/guardrails_only/demo.py
--rw-r--r--  2.0 unx      270 b- defN 24-Mar-15 10:01 examples/configs/guardrails_only/input/config.co
--rw-r--r--  2.0 unx      117 b- defN 24-Mar-15 10:01 examples/configs/guardrails_only/input/config.yml
--rw-r--r--  2.0 unx      342 b- defN 24-Mar-15 10:01 examples/configs/guardrails_only/output/config.co
--rw-r--r--  2.0 unx      521 b- defN 24-Mar-15 10:01 examples/configs/guardrails_only/output/config.yml
--rw-r--r--  2.0 unx      323 b- defN 24-Mar-15 10:01 examples/configs/jailbreak_detection/README.md
--rw-r--r--  2.0 unx      295 b- defN 24-Mar-15 10:01 examples/configs/jailbreak_detection/config.yml
--rw-r--r--  2.0 unx     1057 b- defN 24-Mar-15 10:01 examples/configs/jailbreak_detection/flows.co
--rw-r--r--  2.0 unx      916 b- defN 24-Mar-15 10:01 examples/configs/llama_guard/README.md
--rw-r--r--  2.0 unx      349 b- defN 24-Mar-15 10:01 examples/configs/llama_guard/config.yml
--rw-r--r--  2.0 unx    10157 b- defN 24-Mar-15 10:01 examples/configs/llama_guard/prompts.yml
--rw-r--r--  2.0 unx      776 b- defN 24-Mar-15 10:01 examples/configs/llm/README.md
--rw-r--r--  2.0 unx      679 b- defN 24-Mar-15 10:01 examples/configs/llm/__init__.py
--rw-r--r--  2.0 unx      626 b- defN 24-Mar-15 10:01 examples/configs/llm/hf_endpoint/README.md
--rw-r--r--  2.0 unx      433 b- defN 24-Mar-15 10:01 examples/configs/llm/hf_endpoint/config.yml
--rw-r--r--  2.0 unx      380 b- defN 24-Mar-15 10:01 examples/configs/llm/hf_endpoint/rails.co
--rw-r--r--  2.0 unx      844 b- defN 24-Mar-15 10:01 examples/configs/llm/hf_pipeline_dolly/README.md
--rw-r--r--  2.0 unx     2190 b- defN 24-Mar-15 10:01 examples/configs/llm/hf_pipeline_dolly/config.py
--rw-r--r--  2.0 unx     3625 b- defN 24-Mar-15 10:01 examples/configs/llm/hf_pipeline_dolly/config.yml
--rw-r--r--  2.0 unx      625 b- defN 24-Mar-15 10:01 examples/configs/llm/hf_pipeline_dolly/rails.co
--rw-r--r--  2.0 unx      785 b- defN 24-Mar-15 10:01 examples/configs/llm/hf_pipeline_falcon/README.md
--rw-r--r--  2.0 unx      679 b- defN 24-Mar-15 10:01 examples/configs/llm/hf_pipeline_falcon/__init__.py
--rw-r--r--  2.0 unx     1587 b- defN 24-Mar-15 10:01 examples/configs/llm/hf_pipeline_falcon/config.py
--rw-r--r--  2.0 unx     3885 b- defN 24-Mar-15 10:01 examples/configs/llm/hf_pipeline_falcon/config.yml
--rw-r--r--  2.0 unx      380 b- defN 24-Mar-15 10:01 examples/configs/llm/hf_pipeline_falcon/rails.co
--rw-r--r--  2.0 unx     1795 b- defN 24-Mar-15 10:01 examples/configs/llm/hf_pipeline_llama2/README.md
--rw-r--r--  2.0 unx      679 b- defN 24-Mar-15 10:01 examples/configs/llm/hf_pipeline_llama2/__init__.py
--rw-r--r--  2.0 unx     4494 b- defN 24-Mar-15 10:01 examples/configs/llm/hf_pipeline_llama2/config.py
--rw-r--r--  2.0 unx     4261 b- defN 24-Mar-15 10:01 examples/configs/llm/hf_pipeline_llama2/config.yml
--rw-r--r--  2.0 unx     7499 b- defN 24-Mar-15 10:01 examples/configs/llm/hf_pipeline_llama2/kb/report.md
--rw-r--r--  2.0 unx      228 b- defN 24-Mar-15 10:01 examples/configs/llm/hf_pipeline_llama2/rails/factcheck.co
--rw-r--r--  2.0 unx     1776 b- defN 24-Mar-15 10:01 examples/configs/llm/hf_pipeline_llama2/rails/general.co
--rw-r--r--  2.0 unx      763 b- defN 24-Mar-15 10:01 examples/configs/llm/hf_pipeline_mosaic/README.md
--rw-r--r--  2.0 unx     2406 b- defN 24-Mar-15 10:01 examples/configs/llm/hf_pipeline_mosaic/config.py
--rw-r--r--  2.0 unx     3885 b- defN 24-Mar-15 10:01 examples/configs/llm/hf_pipeline_mosaic/config.yml
--rw-r--r--  2.0 unx      380 b- defN 24-Mar-15 10:01 examples/configs/llm/hf_pipeline_mosaic/rails.co
--rw-r--r--  2.0 unx      899 b- defN 24-Mar-15 10:01 examples/configs/llm/hf_pipeline_vicuna/README.md
--rw-r--r--  2.0 unx     3900 b- defN 24-Mar-15 10:01 examples/configs/llm/hf_pipeline_vicuna/config.py
--rw-r--r--  2.0 unx     3885 b- defN 24-Mar-15 10:01 examples/configs/llm/hf_pipeline_vicuna/config.yml
--rw-r--r--  2.0 unx      380 b- defN 24-Mar-15 10:01 examples/configs/llm/hf_pipeline_vicuna/rails.co
--rw-r--r--  2.0 unx     1199 b- defN 24-Mar-15 10:01 examples/configs/llm/nemollm/README.md
--rw-r--r--  2.0 unx     1535 b- defN 24-Mar-15 10:01 examples/configs/llm/nemollm/config.yml
--rw-r--r--  2.0 unx      380 b- defN 24-Mar-15 10:01 examples/configs/llm/nemollm/rails.co
--rw-r--r--  2.0 unx      138 b- defN 24-Mar-15 10:01 examples/configs/rag/README.md
--rw-r--r--  2.0 unx      679 b- defN 24-Mar-15 10:01 examples/configs/rag/__init__.py
--rw-r--r--  2.0 unx     1670 b- defN 24-Mar-15 10:01 examples/configs/rag/custom_rag_output_rails/README.md
--rw-r--r--  2.0 unx     2495 b- defN 24-Mar-15 10:01 examples/configs/rag/custom_rag_output_rails/config.py
--rw-r--r--  2.0 unx      817 b- defN 24-Mar-15 10:01 examples/configs/rag/custom_rag_output_rails/config.yml
--rw-r--r--  2.0 unx     7499 b- defN 24-Mar-15 10:01 examples/configs/rag/custom_rag_output_rails/kb/report.md
--rw-r--r--  2.0 unx      314 b- defN 24-Mar-15 10:01 examples/configs/rag/custom_rag_output_rails/rails/output.co
--rw-r--r--  2.0 unx      608 b- defN 24-Mar-15 10:01 examples/configs/rag/fact_checking/README.md
--rw-r--r--  2.0 unx      242 b- defN 24-Mar-15 10:01 examples/configs/rag/fact_checking/config.yml
--rw-r--r--  2.0 unx     7499 b- defN 24-Mar-15 10:01 examples/configs/rag/fact_checking/kb/report.md
--rw-r--r--  2.0 unx     1276 b- defN 24-Mar-15 10:01 examples/configs/rag/fact_checking/rails/factcheck.co
--rw-r--r--  2.0 unx     1746 b- defN 24-Mar-15 10:01 examples/configs/rag/fact_checking/rails/general.co
--rw-r--r--  2.0 unx     2494 b- defN 24-Mar-15 10:01 examples/configs/rag/multi_kb/README.md
--rw-r--r--  2.0 unx      679 b- defN 24-Mar-15 10:01 examples/configs/rag/multi_kb/__init__.py
--rw-r--r--  2.0 unx    10507 b- defN 24-Mar-15 10:01 examples/configs/rag/multi_kb/config.py
--rw-r--r--  2.0 unx     1332 b- defN 24-Mar-15 10:01 examples/configs/rag/multi_kb/config.yml
--rw-r--r--  2.0 unx       66 b- defN 24-Mar-15 10:01 examples/configs/rag/multi_kb/requirements.txt
--rw-r--r--  2.0 unx     3348 b- defN 24-Mar-15 10:01 examples/configs/rag/multi_kb/tabular_llm.py
--rw-r--r--  2.0 unx       26 b- defN 24-Mar-15 10:01 examples/configs/rag/multi_kb/kb/README.md
--rw-r--r--  2.0 unx      261 b- defN 24-Mar-15 10:01 examples/configs/rag/multi_kb/rails/factcheck.co
--rw-r--r--  2.0 unx      759 b- defN 24-Mar-15 10:01 examples/configs/rag/multi_kb/rails/general.co
--rw-r--r--  2.0 unx     6256 b- defN 24-Mar-15 10:01 examples/configs/rag/pinecone/README.md
--rw-r--r--  2.0 unx      679 b- defN 24-Mar-15 10:01 examples/configs/rag/pinecone/__init__.py
--rw-r--r--  2.0 unx     3631 b- defN 24-Mar-15 10:01 examples/configs/rag/pinecone/config.py
--rw-r--r--  2.0 unx      905 b- defN 24-Mar-15 10:01 examples/configs/rag/pinecone/config.yml
--rw-r--r--  2.0 unx      657 b- defN 24-Mar-15 10:01 examples/configs/rag/pinecone/rails.co
--rw-r--r--  2.0 unx   124624 b- defN 24-Mar-15 10:01 examples/configs/rag/pinecone/kb/data-00000-of-00001.arrow
--rw-r--r--  2.0 unx      300 b- defN 24-Mar-15 10:01 examples/configs/rag/pinecone/kb/dataset_info.json
--rw-r--r--  2.0 unx  1641329 b- defN 24-Mar-15 10:01 examples/configs/rag/pinecone/kb/nvidia.pdf
--rw-r--r--  2.0 unx      248 b- defN 24-Mar-15 10:01 examples/configs/rag/pinecone/kb/state.json
--rw-r--r--  2.0 unx      537 b- defN 24-Mar-15 10:01 examples/configs/red-teaming/README.md
--rw-r--r--  2.0 unx      679 b- defN 24-Mar-15 10:01 examples/configs/red-teaming/__init__.py
--rw-r--r--  2.0 unx      664 b- defN 24-Mar-15 10:01 examples/configs/red-teaming/challenges.json
--rw-r--r--  2.0 unx     1927 b- defN 24-Mar-15 10:01 examples/configs/red-teaming/config.py
--rw-r--r--  2.0 unx       76 b- defN 24-Mar-15 10:01 examples/configs/red-teaming/config_1/config.yml
--rw-r--r--  2.0 unx       76 b- defN 24-Mar-15 10:01 examples/configs/red-teaming/config_2/config.yml
--rw-r--r--  2.0 unx      566 b- defN 24-Mar-15 10:01 examples/configs/sample/config.co
--rw-r--r--  2.0 unx      678 b- defN 24-Mar-15 10:01 examples/configs/sample/config.yml
--rw-r--r--  2.0 unx      570 b- defN 24-Mar-15 10:01 examples/configs/streaming/README.md
--rw-r--r--  2.0 unx      438 b- defN 24-Mar-15 10:01 examples/configs/streaming/config.co
--rw-r--r--  2.0 unx      500 b- defN 24-Mar-15 10:01 examples/configs/streaming/config.yml
--rw-r--r--  2.0 unx     2357 b- defN 24-Mar-15 10:01 examples/configs/threads/README.md
--rw-r--r--  2.0 unx      973 b- defN 24-Mar-15 10:01 examples/configs/threads/config.py
--rw-r--r--  2.0 unx      205 b- defN 24-Mar-15 10:01 examples/configs/threads/config_1/config.yml
--rw-r--r--  2.0 unx      241 b- defN 24-Mar-15 10:01 examples/configs/threads/config_1/rails.co
--rw-r--r--  2.0 unx     5305 b- defN 24-Mar-15 10:01 examples/notebooks/generate_events_and_streaming.ipynb
--rw-r--r--  2.0 unx      679 b- defN 24-Mar-15 10:01 examples/scripts/__init__.py
--rw-r--r--  2.0 unx     3281 b- defN 24-Mar-15 10:01 examples/scripts/demo_llama_index_guardrails.py
--rw-r--r--  2.0 unx     4962 b- defN 24-Mar-15 10:01 examples/scripts/demo_streaming.py
--rw-r--r--  2.0 unx     5707 b- defN 24-Mar-15 10:01 examples/scripts/langchain/experiments.py
--rw-r--r--  2.0 unx      962 b- defN 24-Mar-15 10:01 examples/server_configs/atomic/input_checking/config.yml
--rw-r--r--  2.0 unx       76 b- defN 24-Mar-15 10:01 examples/server_configs/atomic/main/config.yml
--rw-r--r--  2.0 unx      870 b- defN 24-Mar-15 10:01 examples/server_configs/atomic/output_checking/config.yml
--rw-r--r--  2.0 unx      983 b- defN 24-Mar-15 10:01 nemoguardrails/__init__.py
--rw-r--r--  2.0 unx      754 b- defN 24-Mar-15 10:01 nemoguardrails/__main__.py
--rw-r--r--  2.0 unx     1263 b- defN 24-Mar-15 10:01 nemoguardrails/context.py
--rw-r--r--  2.0 unx     1537 b- defN 24-Mar-15 10:01 nemoguardrails/patch_asyncio.py
--rw-r--r--  2.0 unx    11834 b- defN 24-Mar-15 10:01 nemoguardrails/streaming.py
--rw-r--r--  2.0 unx     7821 b- defN 24-Mar-15 10:01 nemoguardrails/utils.py
--rw-r--r--  2.0 unx      708 b- defN 24-Mar-15 10:01 nemoguardrails/actions/__init__.py
--rw-r--r--  2.0 unx    12173 b- defN 24-Mar-15 10:01 nemoguardrails/actions/action_dispatcher.py
--rw-r--r--  2.0 unx     2312 b- defN 24-Mar-15 10:01 nemoguardrails/actions/actions.py
--rw-r--r--  2.0 unx     1627 b- defN 24-Mar-15 10:01 nemoguardrails/actions/core.py
--rw-r--r--  2.0 unx     3524 b- defN 24-Mar-15 10:01 nemoguardrails/actions/math.py
--rw-r--r--  2.0 unx     3106 b- defN 24-Mar-15 10:01 nemoguardrails/actions/retrieve_relevant_chunks.py
--rw-r--r--  2.0 unx     2127 b- defN 24-Mar-15 10:01 nemoguardrails/actions/summarize_document.py
--rw-r--r--  2.0 unx      679 b- defN 24-Mar-15 10:01 nemoguardrails/actions/langchain/__init__.py
--rw-r--r--  2.0 unx     1830 b- defN 24-Mar-15 10:01 nemoguardrails/actions/langchain/actions.py
--rw-r--r--  2.0 unx     3678 b- defN 24-Mar-15 10:01 nemoguardrails/actions/langchain/safetools.py
--rw-r--r--  2.0 unx      679 b- defN 24-Mar-15 10:01 nemoguardrails/actions/llm/__init__.py
--rw-r--r--  2.0 unx    51162 b- defN 24-Mar-15 10:01 nemoguardrails/actions/llm/generation.py
--rw-r--r--  2.0 unx    17327 b- defN 24-Mar-15 10:01 nemoguardrails/actions/llm/utils.py
--rw-r--r--  2.0 unx      679 b- defN 24-Mar-15 10:01 nemoguardrails/actions/v2_x/__init__.py
--rw-r--r--  2.0 unx    19117 b- defN 24-Mar-15 10:01 nemoguardrails/actions/v2_x/generation.py
--rw-r--r--  2.0 unx      700 b- defN 24-Mar-15 10:01 nemoguardrails/actions/validation/__init__.py
--rw-r--r--  2.0 unx     4518 b- defN 24-Mar-15 10:01 nemoguardrails/actions/validation/base.py
--rw-r--r--  2.0 unx     1375 b- defN 24-Mar-15 10:01 nemoguardrails/actions/validation/filter_secrets.py
--rw-r--r--  2.0 unx      679 b- defN 24-Mar-15 10:01 nemoguardrails/actions_server/__init__.py
--rw-r--r--  2.0 unx     2492 b- defN 24-Mar-15 10:01 nemoguardrails/actions_server/actions_server.py
--rw-r--r--  2.0 unx     5960 b- defN 24-Mar-15 10:01 nemoguardrails/cli/__init__.py
--rw-r--r--  2.0 unx    25338 b- defN 24-Mar-15 10:01 nemoguardrails/cli/chat.py
--rw-r--r--  2.0 unx     2474 b- defN 24-Mar-15 10:01 nemoguardrails/cli/simplify_formatter.py
--rw-r--r--  2.0 unx     1681 b- defN 24-Mar-15 10:01 nemoguardrails/colang/__init__.py
--rw-r--r--  2.0 unx     3924 b- defN 24-Mar-15 10:01 nemoguardrails/colang/runtime.py
--rw-r--r--  2.0 unx      679 b- defN 24-Mar-15 10:01 nemoguardrails/colang/v1_0/__init__.py
--rw-r--r--  2.0 unx      679 b- defN 24-Mar-15 10:01 nemoguardrails/colang/v1_0/lang/__init__.py
--rw-r--r--  2.0 unx    73671 b- defN 24-Mar-15 10:01 nemoguardrails/colang/v1_0/lang/colang_parser.py
--rw-r--r--  2.0 unx    17098 b- defN 24-Mar-15 10:01 nemoguardrails/colang/v1_0/lang/comd_parser.py
--rw-r--r--  2.0 unx    22118 b- defN 24-Mar-15 10:01 nemoguardrails/colang/v1_0/lang/coyml_parser.py
--rw-r--r--  2.0 unx     4314 b- defN 24-Mar-15 10:01 nemoguardrails/colang/v1_0/lang/parser.py
--rw-r--r--  2.0 unx    11644 b- defN 24-Mar-15 10:01 nemoguardrails/colang/v1_0/lang/utils.py
--rw-r--r--  2.0 unx      679 b- defN 24-Mar-15 10:01 nemoguardrails/colang/v1_0/runtime/__init__.py
--rw-r--r--  2.0 unx     2417 b- defN 24-Mar-15 10:01 nemoguardrails/colang/v1_0/runtime/eval.py
--rw-r--r--  2.0 unx    24260 b- defN 24-Mar-15 10:01 nemoguardrails/colang/v1_0/runtime/flows.py
--rw-r--r--  2.0 unx    18621 b- defN 24-Mar-15 10:01 nemoguardrails/colang/v1_0/runtime/runtime.py
--rw-r--r--  2.0 unx     4942 b- defN 24-Mar-15 10:01 nemoguardrails/colang/v1_0/runtime/sliding.py
--rw-r--r--  2.0 unx     1881 b- defN 24-Mar-15 10:01 nemoguardrails/colang/v1_0/runtime/utils.py
--rw-r--r--  2.0 unx      679 b- defN 24-Mar-15 10:01 nemoguardrails/colang/v2_x/__init__.py
--rw-r--r--  2.0 unx      679 b- defN 24-Mar-15 10:01 nemoguardrails/colang/v2_x/lang/__init__.py
--rw-r--r--  2.0 unx    10686 b- defN 24-Mar-15 10:01 nemoguardrails/colang/v2_x/lang/colang_ast.py
--rw-r--r--  2.0 unx    38410 b- defN 24-Mar-15 10:01 nemoguardrails/colang/v2_x/lang/expansion.py
--rw-r--r--  2.0 unx     5784 b- defN 24-Mar-15 10:01 nemoguardrails/colang/v2_x/lang/parser.py
--rw-r--r--  2.0 unx    20879 b- defN 24-Mar-15 10:01 nemoguardrails/colang/v2_x/lang/transformer.py
--rw-r--r--  2.0 unx     1296 b- defN 24-Mar-15 10:01 nemoguardrails/colang/v2_x/lang/utils.py
--rw-r--r--  2.0 unx      679 b- defN 24-Mar-15 10:01 nemoguardrails/colang/v2_x/lang/grammar/__init__.py
--rw-r--r--  2.0 unx     7383 b- defN 24-Mar-15 10:01 nemoguardrails/colang/v2_x/lang/grammar/colang.lark
--rw-r--r--  2.0 unx     1314 b- defN 24-Mar-15 10:01 nemoguardrails/colang/v2_x/lang/grammar/load.py
--rw-r--r--  2.0 unx    20350 b- defN 24-Mar-15 10:01 nemoguardrails/colang/v2_x/library/avatars.co
--rw-r--r--  2.0 unx      814 b- defN 24-Mar-15 10:01 nemoguardrails/colang/v2_x/library/core.co
--rw-r--r--  2.0 unx      300 b- defN 24-Mar-15 10:01 nemoguardrails/colang/v2_x/library/utils.co
--rw-r--r--  2.0 unx      679 b- defN 24-Mar-15 10:01 nemoguardrails/colang/v2_x/runtime/__init__.py
--rw-r--r--  2.0 unx     6294 b- defN 24-Mar-15 10:01 nemoguardrails/colang/v2_x/runtime/eval.py
--rw-r--r--  2.0 unx    26208 b- defN 24-Mar-15 10:01 nemoguardrails/colang/v2_x/runtime/flows.py
--rw-r--r--  2.0 unx    24744 b- defN 24-Mar-15 10:01 nemoguardrails/colang/v2_x/runtime/runtime.py
--rw-r--r--  2.0 unx    88576 b- defN 24-Mar-15 10:01 nemoguardrails/colang/v2_x/runtime/statemachine.py
--rw-r--r--  2.0 unx     1050 b- defN 24-Mar-15 10:01 nemoguardrails/colang/v2_x/runtime/system_functions.py
--rw-r--r--  2.0 unx     1459 b- defN 24-Mar-15 10:01 nemoguardrails/colang/v2_x/runtime/utils.py
--rw-r--r--  2.0 unx      679 b- defN 24-Mar-15 10:01 nemoguardrails/embeddings/__init__.py
--rw-r--r--  2.0 unx     9994 b- defN 24-Mar-15 10:01 nemoguardrails/embeddings/basic.py
--rw-r--r--  2.0 unx     9773 b- defN 24-Mar-15 10:01 nemoguardrails/embeddings/cache.py
--rw-r--r--  2.0 unx     2438 b- defN 24-Mar-15 10:01 nemoguardrails/embeddings/index.py
--rw-r--r--  2.0 unx     3268 b- defN 24-Mar-15 10:01 nemoguardrails/embeddings/embedding_providers/__init__.py
--rw-r--r--  2.0 unx     3285 b- defN 24-Mar-15 10:01 nemoguardrails/embeddings/embedding_providers/fastembed.py
--rw-r--r--  2.0 unx     3983 b- defN 24-Mar-15 10:01 nemoguardrails/embeddings/embedding_providers/openai.py
--rw-r--r--  2.0 unx     2776 b- defN 24-Mar-15 10:01 nemoguardrails/embeddings/embedding_providers/sentence_transformers.py
--rw-r--r--  2.0 unx      679 b- defN 24-Mar-15 10:01 nemoguardrails/eval/__init__.py
--rw-r--r--  2.0 unx     9013 b- defN 24-Mar-15 10:01 nemoguardrails/eval/evaluate_factcheck.py
--rw-r--r--  2.0 unx     6953 b- defN 24-Mar-15 10:01 nemoguardrails/eval/evaluate_hallucination.py
--rw-r--r--  2.0 unx     8580 b- defN 24-Mar-15 10:01 nemoguardrails/eval/evaluate_moderation.py
--rw-r--r--  2.0 unx    17162 b- defN 24-Mar-15 10:01 nemoguardrails/eval/evaluate_topical.py
--rw-r--r--  2.0 unx     1685 b- defN 24-Mar-15 10:01 nemoguardrails/eval/utils.py
--rw-r--r--  2.0 unx      679 b- defN 24-Mar-15 10:01 nemoguardrails/eval/cli/__init__.py
--rw-r--r--  2.0 unx    10465 b- defN 24-Mar-15 10:01 nemoguardrails/eval/cli/evaluate.py
--rw-r--r--  2.0 unx     4208 b- defN 24-Mar-15 10:01 nemoguardrails/eval/data/factchecking/README.md
--rw-r--r--  2.0 unx      679 b- defN 24-Mar-15 10:01 nemoguardrails/eval/data/factchecking/__init__.py
--rw-r--r--  2.0 unx     1539 b- defN 24-Mar-15 10:01 nemoguardrails/eval/data/factchecking/process_msmarco_data.py
--rw-r--r--  2.0 unx      283 b- defN 24-Mar-15 10:01 nemoguardrails/eval/data/factchecking/sample.json
--rw-r--r--  2.0 unx      679 b- defN 24-Mar-15 10:01 nemoguardrails/eval/data/hallucination/__init__.py
--rw-r--r--  2.0 unx     1071 b- defN 24-Mar-15 10:01 nemoguardrails/eval/data/hallucination/sample.txt
--rw-r--r--  2.0 unx     3956 b- defN 24-Mar-15 10:01 nemoguardrails/eval/data/moderation/README.md
--rw-r--r--  2.0 unx      679 b- defN 24-Mar-15 10:01 nemoguardrails/eval/data/moderation/__init__.py
--rw-r--r--  2.0 unx       24 b- defN 24-Mar-15 10:01 nemoguardrails/eval/data/moderation/harmful.txt
--rw-r--r--  2.0 unx       21 b- defN 24-Mar-15 10:01 nemoguardrails/eval/data/moderation/helpful.txt
--rw-r--r--  2.0 unx     4451 b- defN 24-Mar-15 10:01 nemoguardrails/eval/data/moderation/process_anthropic_dataset.py
--rw-r--r--  2.0 unx     4838 b- defN 24-Mar-15 10:01 nemoguardrails/eval/data/topical/README.md
--rw-r--r--  2.0 unx      679 b- defN 24-Mar-15 10:01 nemoguardrails/eval/data/topical/__init__.py
--rw-r--r--  2.0 unx     2465 b- defN 24-Mar-15 10:01 nemoguardrails/eval/data/topical/create_colang_intent_file.py
--rw-r--r--  2.0 unx    12970 b- defN 24-Mar-15 10:01 nemoguardrails/eval/data/topical/dataset_tools.py
--rw-r--r--  2.0 unx     5190 b- defN 24-Mar-15 10:01 nemoguardrails/eval/data/topical/banking/categories_canonical_forms.json
--rw-r--r--  2.0 unx      889 b- defN 24-Mar-15 10:01 nemoguardrails/eval/data/topical/banking/config.yml
--rw-r--r--  2.0 unx     9186 b- defN 24-Mar-15 10:01 nemoguardrails/eval/data/topical/banking/flows.co
--rw-r--r--  2.0 unx     6118 b- defN 24-Mar-15 10:01 nemoguardrails/eval/data/topical/chitchat/bot.co
--rw-r--r--  2.0 unx      863 b- defN 24-Mar-15 10:01 nemoguardrails/eval/data/topical/chitchat/config.yml
--rw-r--r--  2.0 unx     6844 b- defN 24-Mar-15 10:01 nemoguardrails/eval/data/topical/chitchat/flows.co
--rw-r--r--  2.0 unx     3260 b- defN 24-Mar-15 10:01 nemoguardrails/eval/data/topical/chitchat/intent_canonical_forms.json
--rw-r--r--  2.0 unx      199 b- defN 24-Mar-15 10:01 nemoguardrails/eval/data/topical/chitchat/user-other.co
--rw-r--r--  2.0 unx      679 b- defN 24-Mar-15 10:01 nemoguardrails/integrations/__init__.py
--rw-r--r--  2.0 unx      679 b- defN 24-Mar-15 10:01 nemoguardrails/integrations/langchain/__init__.py
--rw-r--r--  2.0 unx     9158 b- defN 24-Mar-15 10:01 nemoguardrails/integrations/langchain/runnable_rails.py
--rw-r--r--  2.0 unx      679 b- defN 24-Mar-15 10:01 nemoguardrails/kb/__init__.py
--rw-r--r--  2.0 unx     7342 b- defN 24-Mar-15 10:01 nemoguardrails/kb/kb.py
--rw-r--r--  2.0 unx     3878 b- defN 24-Mar-15 10:01 nemoguardrails/kb/utils.py
--rw-r--r--  2.0 unx      679 b- defN 24-Mar-15 10:01 nemoguardrails/library/__init__.py
--rw-r--r--  2.0 unx      679 b- defN 24-Mar-15 10:01 nemoguardrails/library/activefence/__init__.py
--rw-r--r--  2.0 unx     2430 b- defN 24-Mar-15 10:01 nemoguardrails/library/activefence/actions.py
--rw-r--r--  2.0 unx     2249 b- defN 24-Mar-15 10:01 nemoguardrails/library/activefence/flows.co
--rw-r--r--  2.0 unx      679 b- defN 24-Mar-15 10:01 nemoguardrails/library/factchecking/__init__.py
--rw-r--r--  2.0 unx      679 b- defN 24-Mar-15 10:01 nemoguardrails/library/factchecking/align_score/__init__.py
--rw-r--r--  2.0 unx     2222 b- defN 24-Mar-15 10:01 nemoguardrails/library/factchecking/align_score/actions.py
--rw-r--r--  2.0 unx      405 b- defN 24-Mar-15 10:01 nemoguardrails/library/factchecking/align_score/flows.co
--rw-r--r--  2.0 unx     1714 b- defN 24-Mar-15 10:01 nemoguardrails/library/factchecking/align_score/request.py
--rw-r--r--  2.0 unx      147 b- defN 24-Mar-15 10:01 nemoguardrails/library/factchecking/align_score/requirements.txt
--rw-r--r--  2.0 unx     3373 b- defN 24-Mar-15 10:01 nemoguardrails/library/factchecking/align_score/server.py
--rw-r--r--  2.0 unx      679 b- defN 24-Mar-15 10:01 nemoguardrails/library/hallucination/__init__.py
--rw-r--r--  2.0 unx     5280 b- defN 24-Mar-15 10:01 nemoguardrails/library/hallucination/actions.py
--rw-r--r--  2.0 unx      892 b- defN 24-Mar-15 10:01 nemoguardrails/library/hallucination/flows.co
--rw-r--r--  2.0 unx      679 b- defN 24-Mar-15 10:01 nemoguardrails/library/jailbreak_detection/__init__.py
--rw-r--r--  2.0 unx     2492 b- defN 24-Mar-15 10:01 nemoguardrails/library/jailbreak_detection/actions.py
--rw-r--r--  2.0 unx      322 b- defN 24-Mar-15 10:01 nemoguardrails/library/jailbreak_detection/flows.co
--rw-r--r--  2.0 unx     1739 b- defN 24-Mar-15 10:01 nemoguardrails/library/jailbreak_detection/request.py
--rw-r--r--  2.0 unx      214 b- defN 24-Mar-15 10:01 nemoguardrails/library/jailbreak_detection/requirements.txt
--rw-r--r--  2.0 unx     3063 b- defN 24-Mar-15 10:01 nemoguardrails/library/jailbreak_detection/server.py
--rw-r--r--  2.0 unx      679 b- defN 24-Mar-15 10:01 nemoguardrails/library/jailbreak_detection/heuristics/__init__.py
--rw-r--r--  2.0 unx     3500 b- defN 24-Mar-15 10:01 nemoguardrails/library/jailbreak_detection/heuristics/checks.py
--rw-r--r--  2.0 unx      679 b- defN 24-Mar-15 10:01 nemoguardrails/library/llama_guard/__init__.py
--rw-r--r--  2.0 unx     4200 b- defN 24-Mar-15 10:01 nemoguardrails/library/llama_guard/actions.py
--rw-r--r--  2.0 unx      709 b- defN 24-Mar-15 10:01 nemoguardrails/library/llama_guard/flows.co
--rw-r--r--  2.0 unx       81 b- defN 24-Mar-15 10:01 nemoguardrails/library/llama_guard/requirements.txt
--rw-r--r--  2.0 unx      679 b- defN 24-Mar-15 10:01 nemoguardrails/library/self_check/__init__.py
--rw-r--r--  2.0 unx      679 b- defN 24-Mar-15 10:01 nemoguardrails/library/self_check/facts/__init__.py
--rw-r--r--  2.0 unx     2160 b- defN 24-Mar-15 10:01 nemoguardrails/library/self_check/facts/actions.py
--rw-r--r--  2.0 unx      459 b- defN 24-Mar-15 10:01 nemoguardrails/library/self_check/facts/flows.co
--rw-r--r--  2.0 unx      679 b- defN 24-Mar-15 10:01 nemoguardrails/library/self_check/input_check/__init__.py
--rw-r--r--  2.0 unx     2613 b- defN 24-Mar-15 10:01 nemoguardrails/library/self_check/input_check/actions.py
--rw-r--r--  2.0 unx      191 b- defN 24-Mar-15 10:01 nemoguardrails/library/self_check/input_check/flows.co
--rw-r--r--  2.0 unx      679 b- defN 24-Mar-15 10:01 nemoguardrails/library/self_check/output_check/__init__.py
--rw-r--r--  2.0 unx     2562 b- defN 24-Mar-15 10:01 nemoguardrails/library/self_check/output_check/actions.py
--rw-r--r--  2.0 unx      194 b- defN 24-Mar-15 10:01 nemoguardrails/library/self_check/output_check/flows.co
--rw-r--r--  2.0 unx      679 b- defN 24-Mar-15 10:01 nemoguardrails/library/sensitive_data_detection/__init__.py
--rw-r--r--  2.0 unx     4926 b- defN 24-Mar-15 10:01 nemoguardrails/library/sensitive_data_detection/actions.py
--rw-r--r--  2.0 unx     1466 b- defN 24-Mar-15 10:01 nemoguardrails/library/sensitive_data_detection/flows.co
--rw-r--r--  2.0 unx      679 b- defN 24-Mar-15 10:01 nemoguardrails/llm/__init__.py
--rw-r--r--  2.0 unx     9899 b- defN 24-Mar-15 10:01 nemoguardrails/llm/filters.py
--rw-r--r--  2.0 unx     3263 b- defN 24-Mar-15 10:01 nemoguardrails/llm/helpers.py
--rw-r--r--  2.0 unx     2237 b- defN 24-Mar-15 10:01 nemoguardrails/llm/output_parsers.py
--rw-r--r--  2.0 unx     3179 b- defN 24-Mar-15 10:01 nemoguardrails/llm/params.py
--rw-r--r--  2.0 unx     5154 b- defN 24-Mar-15 10:01 nemoguardrails/llm/prompts.py
--rw-r--r--  2.0 unx    10800 b- defN 24-Mar-15 10:01 nemoguardrails/llm/taskmanager.py
--rw-r--r--  2.0 unx     1750 b- defN 24-Mar-15 10:01 nemoguardrails/llm/types.py
--rw-r--r--  2.0 unx     2608 b- defN 24-Mar-15 10:01 nemoguardrails/llm/prompts/cohere.yml
--rw-r--r--  2.0 unx     2887 b- defN 24-Mar-15 10:01 nemoguardrails/llm/prompts/dolly.yml
--rw-r--r--  2.0 unx     5700 b- defN 24-Mar-15 10:01 nemoguardrails/llm/prompts/general.yml
--rw-r--r--  2.0 unx     3235 b- defN 24-Mar-15 10:01 nemoguardrails/llm/prompts/mosaic.yml
--rw-r--r--  2.0 unx     6149 b- defN 24-Mar-15 10:01 nemoguardrails/llm/prompts/nemollm.yml
--rw-r--r--  2.0 unx     3547 b- defN 24-Mar-15 10:01 nemoguardrails/llm/prompts/openai-chatgpt.yml
--rw-r--r--  2.0 unx       57 b- defN 24-Mar-15 10:01 nemoguardrails/llm/prompts/openai-gpt35-instruct.yml
--rw-r--r--  2.0 unx      819 b- defN 24-Mar-15 10:01 nemoguardrails/llm/providers/__init__.py
--rw-r--r--  2.0 unx    11487 b- defN 24-Mar-15 10:01 nemoguardrails/llm/providers/nemollm.py
--rw-r--r--  2.0 unx     8308 b- defN 24-Mar-15 10:01 nemoguardrails/llm/providers/providers.py
--rw-r--r--  2.0 unx      729 b- defN 24-Mar-15 10:01 nemoguardrails/llm/providers/huggingface/__init__.py
--rw-r--r--  2.0 unx     1939 b- defN 24-Mar-15 10:01 nemoguardrails/llm/providers/huggingface/streamers.py
--rw-r--r--  2.0 unx      679 b- defN 24-Mar-15 10:01 nemoguardrails/llm/providers/trtllm/__init__.py
--rw-r--r--  2.0 unx     8353 b- defN 24-Mar-15 10:01 nemoguardrails/llm/providers/trtllm/client.py
--rw-r--r--  2.0 unx     5827 b- defN 24-Mar-15 10:01 nemoguardrails/llm/providers/trtllm/llm.py
--rw-r--r--  2.0 unx      679 b- defN 24-Mar-15 10:01 nemoguardrails/logging/__init__.py
--rw-r--r--  2.0 unx     9675 b- defN 24-Mar-15 10:01 nemoguardrails/logging/callbacks.py
--rw-r--r--  2.0 unx     3710 b- defN 24-Mar-15 10:01 nemoguardrails/logging/explain.py
--rw-r--r--  2.0 unx    10887 b- defN 24-Mar-15 10:01 nemoguardrails/logging/processing_log.py
--rw-r--r--  2.0 unx     2062 b- defN 24-Mar-15 10:01 nemoguardrails/logging/stats.py
--rw-r--r--  2.0 unx     6887 b- defN 24-Mar-15 10:01 nemoguardrails/logging/verbose.py
--rw-r--r--  2.0 unx      751 b- defN 24-Mar-15 10:01 nemoguardrails/rails/__init__.py
--rw-r--r--  2.0 unx      679 b- defN 24-Mar-15 10:01 nemoguardrails/rails/llm/__init__.py
--rw-r--r--  2.0 unx    34588 b- defN 24-Mar-15 10:01 nemoguardrails/rails/llm/config.py
--rw-r--r--  2.0 unx     1906 b- defN 24-Mar-15 10:01 nemoguardrails/rails/llm/default_config.yml
--rw-r--r--  2.0 unx     4667 b- defN 24-Mar-15 10:01 nemoguardrails/rails/llm/llm_flows.co
--rw-r--r--  2.0 unx    36117 b- defN 24-Mar-15 10:01 nemoguardrails/rails/llm/llmrails.py
--rw-r--r--  2.0 unx    15237 b- defN 24-Mar-15 10:01 nemoguardrails/rails/llm/options.py
--rw-r--r--  2.0 unx     1511 b- defN 24-Mar-15 10:01 nemoguardrails/rails/llm/utils.py
--rw-r--r--  2.0 unx      679 b- defN 24-Mar-15 10:01 nemoguardrails/server/__init__.py
--rw-r--r--  2.0 unx    18413 b- defN 24-Mar-15 10:01 nemoguardrails/server/api.py
--rw-r--r--  2.0 unx      679 b- defN 24-Mar-15 10:01 nemoguardrails/server/datastore/__init__.py
--rw-r--r--  2.0 unx     1305 b- defN 24-Mar-15 10:01 nemoguardrails/server/datastore/datastore.py
--rw-r--r--  2.0 unx     1469 b- defN 24-Mar-15 10:01 nemoguardrails/server/datastore/memory_store.py
--rw-r--r--  2.0 unx     2090 b- defN 24-Mar-15 10:01 nemoguardrails/server/datastore/redis_store.py
--rw-r--r--  2.0 unx     9861 b- defN 24-Mar-15 10:01 nemoguardrails-0.8.1.dist-info/LICENSE-Apache-2.0.txt
--rw-r--r--  2.0 unx      654 b- defN 24-Mar-15 10:01 nemoguardrails-0.8.1.dist-info/LICENSE.md
--rw-r--r--  2.0 unx    20573 b- defN 24-Mar-15 10:01 nemoguardrails-0.8.1.dist-info/METADATA
--rw-r--r--  2.0 unx       92 b- defN 24-Mar-15 10:01 nemoguardrails-0.8.1.dist-info/WHEEL
--rw-r--r--  2.0 unx       63 b- defN 24-Mar-15 10:01 nemoguardrails-0.8.1.dist-info/entry_points.txt
--rw-r--r--  2.0 unx       15 b- defN 24-Mar-15 10:01 nemoguardrails-0.8.1.dist-info/top_level.txt
--rw-rw-r--  2.0 unx    33699 b- defN 24-Mar-15 10:02 nemoguardrails-0.8.1.dist-info/RECORD
-334 files, 6626625 bytes uncompressed, 2450402 bytes compressed:  63.0%
+Zip file size: 2564916 bytes, number of entries: 447
+-rw-r--r--  2.0 unx      240 b- defN 24-Apr-01 20:07 chat-ui/README.md
+-rw-r--r--  2.0 unx     2512 b- defN 24-Apr-01 20:07 chat-ui/frontend/404.html
+-rw-r--r--  2.0 unx    15406 b- defN 24-Apr-01 20:07 chat-ui/frontend/favicon.ico
+-rw-r--r--  2.0 unx     2098 b- defN 24-Apr-01 20:07 chat-ui/frontend/index.html
+-rw-r--r--  2.0 unx       88 b- defN 24-Apr-01 20:07 chat-ui/frontend/_next/data/c_3yGnIXISqv9z1fDmxxC/index.json
+-rw-r--r--  2.0 unx      368 b- defN 24-Apr-01 20:07 chat-ui/frontend/_next/static/c_3yGnIXISqv9z1fDmxxC/_buildManifest.js
+-rw-r--r--  2.0 unx       89 b- defN 24-Apr-01 20:07 chat-ui/frontend/_next/static/c_3yGnIXISqv9z1fDmxxC/_ssgManifest.js
+-rw-r--r--  2.0 unx  2129821 b- defN 24-Apr-01 20:07 chat-ui/frontend/_next/static/chunks/105-4bc220fcbf235939.js
+-rw-r--r--  2.0 unx   611276 b- defN 24-Apr-01 20:07 chat-ui/frontend/_next/static/chunks/542b50fd-07ebedc579cef971.js
+-rw-r--r--  2.0 unx   141075 b- defN 24-Apr-01 20:07 chat-ui/frontend/_next/static/chunks/framework-73b8966a3c579ab0.js
+-rw-r--r--  2.0 unx    90429 b- defN 24-Apr-01 20:07 chat-ui/frontend/_next/static/chunks/main-6260d066cf2cd7b1.js
+-rw-r--r--  2.0 unx    91460 b- defN 24-Apr-01 20:07 chat-ui/frontend/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js
+-rw-r--r--  2.0 unx     1607 b- defN 24-Apr-01 20:07 chat-ui/frontend/_next/static/chunks/webpack-59c5c889f52620d6.js
+-rw-r--r--  2.0 unx    75294 b- defN 24-Apr-01 20:07 chat-ui/frontend/_next/static/chunks/pages/_app-fe4220f7110f9c0d.js
+-rw-r--r--  2.0 unx      251 b- defN 24-Apr-01 20:07 chat-ui/frontend/_next/static/chunks/pages/_error-3f6d1c55bb8051ab.js
+-rw-r--r--  2.0 unx    55866 b- defN 24-Apr-01 20:07 chat-ui/frontend/_next/static/chunks/pages/index-a45704f8dbae891c.js
+-rw-r--r--  2.0 unx    39792 b- defN 24-Apr-01 20:07 chat-ui/frontend/_next/static/css/c15848365b5406ac.css
+-rw-r--r--  2.0 unx    37780 b- defN 24-Apr-01 20:07 chat-ui/frontend/_next/static/media/2aaf0723e720e8b9-s.p.woff2
+-rw-r--r--  2.0 unx    57244 b- defN 24-Apr-01 20:07 chat-ui/frontend/_next/static/media/9c4f34569c9b36ca-s.woff2
+-rw-r--r--  2.0 unx    11924 b- defN 24-Apr-01 20:07 chat-ui/frontend/_next/static/media/ae9ae6716d4f8bf8-s.woff2
+-rw-r--r--  2.0 unx     8652 b- defN 24-Apr-01 20:07 chat-ui/frontend/_next/static/media/b1db3e28af9ef94a-s.woff2
+-rw-r--r--  2.0 unx    17040 b- defN 24-Apr-01 20:07 chat-ui/frontend/_next/static/media/b967158bc7d7a9fb-s.woff2
+-rw-r--r--  2.0 unx    21960 b- defN 24-Apr-01 20:07 chat-ui/frontend/_next/static/media/c0f5ec5bbf5913b7-s.woff2
+-rw-r--r--  2.0 unx    26728 b- defN 24-Apr-01 20:07 chat-ui/frontend/_next/static/media/d1d9458b69004127-s.woff2
+-rw-r--r--  2.0 unx    15134 b- defN 24-Apr-01 20:07 chat-ui/frontend/static/bot-avatar.png
+-rw-r--r--  2.0 unx    12124 b- defN 24-Apr-01 20:07 chat-ui/frontend/static/user-avatar.png
+-rw-r--r--  2.0 unx      679 b- defN 24-Apr-01 20:07 examples/__init__.py
+-rw-r--r--  2.0 unx     1159 b- defN 24-Apr-01 20:07 examples/sample_config.yml
+-rw-r--r--  2.0 unx      398 b- defN 24-Apr-01 20:07 examples/bots/README.md
+-rw-r--r--  2.0 unx     1641 b- defN 24-Apr-01 20:07 examples/bots/abc/README.md
+-rw-r--r--  2.0 unx     1256 b- defN 24-Apr-01 20:07 examples/bots/abc/config.yml
+-rw-r--r--  2.0 unx     1715 b- defN 24-Apr-01 20:07 examples/bots/abc/prompts.yml
+-rw-r--r--  2.0 unx    14650 b- defN 24-Apr-01 20:07 examples/bots/abc/kb/employee-handbook.md
+-rw-r--r--  2.0 unx     6993 b- defN 24-Apr-01 20:07 examples/bots/abc/rails/disallowed.co
+-rw-r--r--  2.0 unx      771 b- defN 24-Apr-01 20:07 examples/bots/hello_world/README.md
+-rw-r--r--  2.0 unx       76 b- defN 24-Apr-01 20:07 examples/bots/hello_world/config.yml
+-rw-r--r--  2.0 unx      939 b- defN 24-Apr-01 20:07 examples/bots/hello_world/rails.co
+-rw-r--r--  2.0 unx      238 b- defN 24-Apr-01 20:07 examples/configs/README.md
+-rw-r--r--  2.0 unx      679 b- defN 24-Apr-01 20:07 examples/configs/__init__.py
+-rw-r--r--  2.0 unx     2135 b- defN 24-Apr-01 20:07 examples/configs/guardrails_only/README.md
+-rw-r--r--  2.0 unx     2532 b- defN 24-Apr-01 20:07 examples/configs/guardrails_only/demo.py
+-rw-r--r--  2.0 unx      270 b- defN 24-Apr-01 20:07 examples/configs/guardrails_only/input/config.co
+-rw-r--r--  2.0 unx      117 b- defN 24-Apr-01 20:07 examples/configs/guardrails_only/input/config.yml
+-rw-r--r--  2.0 unx      342 b- defN 24-Apr-01 20:07 examples/configs/guardrails_only/output/config.co
+-rw-r--r--  2.0 unx      521 b- defN 24-Apr-01 20:07 examples/configs/guardrails_only/output/config.yml
+-rw-r--r--  2.0 unx      323 b- defN 24-Apr-01 20:07 examples/configs/jailbreak_detection/README.md
+-rw-r--r--  2.0 unx      295 b- defN 24-Apr-01 20:07 examples/configs/jailbreak_detection/config.yml
+-rw-r--r--  2.0 unx     1057 b- defN 24-Apr-01 20:07 examples/configs/jailbreak_detection/flows.co
+-rw-r--r--  2.0 unx      916 b- defN 24-Apr-01 20:07 examples/configs/llama_guard/README.md
+-rw-r--r--  2.0 unx      349 b- defN 24-Apr-01 20:07 examples/configs/llama_guard/config.yml
+-rw-r--r--  2.0 unx    10157 b- defN 24-Apr-01 20:07 examples/configs/llama_guard/prompts.yml
+-rw-r--r--  2.0 unx      776 b- defN 24-Apr-01 20:07 examples/configs/llm/README.md
+-rw-r--r--  2.0 unx      679 b- defN 24-Apr-01 20:07 examples/configs/llm/__init__.py
+-rw-r--r--  2.0 unx      626 b- defN 24-Apr-01 20:07 examples/configs/llm/hf_endpoint/README.md
+-rw-r--r--  2.0 unx      433 b- defN 24-Apr-01 20:07 examples/configs/llm/hf_endpoint/config.yml
+-rw-r--r--  2.0 unx      380 b- defN 24-Apr-01 20:07 examples/configs/llm/hf_endpoint/rails.co
+-rw-r--r--  2.0 unx      844 b- defN 24-Apr-01 20:07 examples/configs/llm/hf_pipeline_dolly/README.md
+-rw-r--r--  2.0 unx     2190 b- defN 24-Apr-01 20:07 examples/configs/llm/hf_pipeline_dolly/config.py
+-rw-r--r--  2.0 unx     3625 b- defN 24-Apr-01 20:07 examples/configs/llm/hf_pipeline_dolly/config.yml
+-rw-r--r--  2.0 unx      625 b- defN 24-Apr-01 20:07 examples/configs/llm/hf_pipeline_dolly/rails.co
+-rw-r--r--  2.0 unx      785 b- defN 24-Apr-01 20:07 examples/configs/llm/hf_pipeline_falcon/README.md
+-rw-r--r--  2.0 unx      679 b- defN 24-Apr-01 20:07 examples/configs/llm/hf_pipeline_falcon/__init__.py
+-rw-r--r--  2.0 unx     1587 b- defN 24-Apr-01 20:07 examples/configs/llm/hf_pipeline_falcon/config.py
+-rw-r--r--  2.0 unx     3885 b- defN 24-Apr-01 20:07 examples/configs/llm/hf_pipeline_falcon/config.yml
+-rw-r--r--  2.0 unx      380 b- defN 24-Apr-01 20:07 examples/configs/llm/hf_pipeline_falcon/rails.co
+-rw-r--r--  2.0 unx     1795 b- defN 24-Apr-01 20:07 examples/configs/llm/hf_pipeline_llama2/README.md
+-rw-r--r--  2.0 unx      679 b- defN 24-Apr-01 20:07 examples/configs/llm/hf_pipeline_llama2/__init__.py
+-rw-r--r--  2.0 unx     4494 b- defN 24-Apr-01 20:07 examples/configs/llm/hf_pipeline_llama2/config.py
+-rw-r--r--  2.0 unx     4261 b- defN 24-Apr-01 20:07 examples/configs/llm/hf_pipeline_llama2/config.yml
+-rw-r--r--  2.0 unx     7499 b- defN 24-Apr-01 20:07 examples/configs/llm/hf_pipeline_llama2/kb/report.md
+-rw-r--r--  2.0 unx      228 b- defN 24-Apr-01 20:07 examples/configs/llm/hf_pipeline_llama2/rails/factcheck.co
+-rw-r--r--  2.0 unx     1776 b- defN 24-Apr-01 20:07 examples/configs/llm/hf_pipeline_llama2/rails/general.co
+-rw-r--r--  2.0 unx      763 b- defN 24-Apr-01 20:07 examples/configs/llm/hf_pipeline_mosaic/README.md
+-rw-r--r--  2.0 unx     2406 b- defN 24-Apr-01 20:07 examples/configs/llm/hf_pipeline_mosaic/config.py
+-rw-r--r--  2.0 unx     3885 b- defN 24-Apr-01 20:07 examples/configs/llm/hf_pipeline_mosaic/config.yml
+-rw-r--r--  2.0 unx      380 b- defN 24-Apr-01 20:07 examples/configs/llm/hf_pipeline_mosaic/rails.co
+-rw-r--r--  2.0 unx      899 b- defN 24-Apr-01 20:07 examples/configs/llm/hf_pipeline_vicuna/README.md
+-rw-r--r--  2.0 unx     3900 b- defN 24-Apr-01 20:07 examples/configs/llm/hf_pipeline_vicuna/config.py
+-rw-r--r--  2.0 unx     3885 b- defN 24-Apr-01 20:07 examples/configs/llm/hf_pipeline_vicuna/config.yml
+-rw-r--r--  2.0 unx      380 b- defN 24-Apr-01 20:07 examples/configs/llm/hf_pipeline_vicuna/rails.co
+-rw-r--r--  2.0 unx     1199 b- defN 24-Apr-01 20:07 examples/configs/llm/nemollm/README.md
+-rw-r--r--  2.0 unx     1535 b- defN 24-Apr-01 20:07 examples/configs/llm/nemollm/config.yml
+-rw-r--r--  2.0 unx      380 b- defN 24-Apr-01 20:07 examples/configs/llm/nemollm/rails.co
+-rw-r--r--  2.0 unx     1481 b- defN 24-Apr-01 20:07 examples/configs/llm/vertexai/README.md
+-rw-r--r--  2.0 unx      200 b- defN 24-Apr-01 20:07 examples/configs/llm/vertexai/config.yml
+-rw-r--r--  2.0 unx      826 b- defN 24-Apr-01 20:07 examples/configs/llm/vertexai/prompts.yml
+-rw-r--r--  2.0 unx      380 b- defN 24-Apr-01 20:07 examples/configs/llm/vertexai/rails.co
+-rw-r--r--  2.0 unx      138 b- defN 24-Apr-01 20:07 examples/configs/rag/README.md
+-rw-r--r--  2.0 unx      679 b- defN 24-Apr-01 20:07 examples/configs/rag/__init__.py
+-rw-r--r--  2.0 unx     1670 b- defN 24-Apr-01 20:07 examples/configs/rag/custom_rag_output_rails/README.md
+-rw-r--r--  2.0 unx     2495 b- defN 24-Apr-01 20:07 examples/configs/rag/custom_rag_output_rails/config.py
+-rw-r--r--  2.0 unx      817 b- defN 24-Apr-01 20:07 examples/configs/rag/custom_rag_output_rails/config.yml
+-rw-r--r--  2.0 unx     7499 b- defN 24-Apr-01 20:07 examples/configs/rag/custom_rag_output_rails/kb/report.md
+-rw-r--r--  2.0 unx      314 b- defN 24-Apr-01 20:07 examples/configs/rag/custom_rag_output_rails/rails/output.co
+-rw-r--r--  2.0 unx      608 b- defN 24-Apr-01 20:07 examples/configs/rag/fact_checking/README.md
+-rw-r--r--  2.0 unx      242 b- defN 24-Apr-01 20:07 examples/configs/rag/fact_checking/config.yml
+-rw-r--r--  2.0 unx     7499 b- defN 24-Apr-01 20:07 examples/configs/rag/fact_checking/kb/report.md
+-rw-r--r--  2.0 unx     1276 b- defN 24-Apr-01 20:07 examples/configs/rag/fact_checking/rails/factcheck.co
+-rw-r--r--  2.0 unx     1746 b- defN 24-Apr-01 20:07 examples/configs/rag/fact_checking/rails/general.co
+-rw-r--r--  2.0 unx     2494 b- defN 24-Apr-01 20:07 examples/configs/rag/multi_kb/README.md
+-rw-r--r--  2.0 unx      679 b- defN 24-Apr-01 20:07 examples/configs/rag/multi_kb/__init__.py
+-rw-r--r--  2.0 unx    10507 b- defN 24-Apr-01 20:07 examples/configs/rag/multi_kb/config.py
+-rw-r--r--  2.0 unx     1332 b- defN 24-Apr-01 20:07 examples/configs/rag/multi_kb/config.yml
+-rw-r--r--  2.0 unx       66 b- defN 24-Apr-01 20:07 examples/configs/rag/multi_kb/requirements.txt
+-rw-r--r--  2.0 unx     3348 b- defN 24-Apr-01 20:07 examples/configs/rag/multi_kb/tabular_llm.py
+-rw-r--r--  2.0 unx       26 b- defN 24-Apr-01 20:07 examples/configs/rag/multi_kb/kb/README.md
+-rw-r--r--  2.0 unx      261 b- defN 24-Apr-01 20:07 examples/configs/rag/multi_kb/rails/factcheck.co
+-rw-r--r--  2.0 unx      759 b- defN 24-Apr-01 20:07 examples/configs/rag/multi_kb/rails/general.co
+-rw-r--r--  2.0 unx     6256 b- defN 24-Apr-01 20:07 examples/configs/rag/pinecone/README.md
+-rw-r--r--  2.0 unx      679 b- defN 24-Apr-01 20:07 examples/configs/rag/pinecone/__init__.py
+-rw-r--r--  2.0 unx     3631 b- defN 24-Apr-01 20:07 examples/configs/rag/pinecone/config.py
+-rw-r--r--  2.0 unx      905 b- defN 24-Apr-01 20:07 examples/configs/rag/pinecone/config.yml
+-rw-r--r--  2.0 unx      657 b- defN 24-Apr-01 20:07 examples/configs/rag/pinecone/rails.co
+-rw-r--r--  2.0 unx   124624 b- defN 24-Apr-01 20:07 examples/configs/rag/pinecone/kb/data-00000-of-00001.arrow
+-rw-r--r--  2.0 unx      300 b- defN 24-Apr-01 20:07 examples/configs/rag/pinecone/kb/dataset_info.json
+-rw-r--r--  2.0 unx  1641329 b- defN 24-Apr-01 20:07 examples/configs/rag/pinecone/kb/nvidia.pdf
+-rw-r--r--  2.0 unx      248 b- defN 24-Apr-01 20:07 examples/configs/rag/pinecone/kb/state.json
+-rw-r--r--  2.0 unx      537 b- defN 24-Apr-01 20:07 examples/configs/red-teaming/README.md
+-rw-r--r--  2.0 unx      679 b- defN 24-Apr-01 20:07 examples/configs/red-teaming/__init__.py
+-rw-r--r--  2.0 unx      664 b- defN 24-Apr-01 20:07 examples/configs/red-teaming/challenges.json
+-rw-r--r--  2.0 unx     1927 b- defN 24-Apr-01 20:07 examples/configs/red-teaming/config.py
+-rw-r--r--  2.0 unx       76 b- defN 24-Apr-01 20:07 examples/configs/red-teaming/config_1/config.yml
+-rw-r--r--  2.0 unx       76 b- defN 24-Apr-01 20:07 examples/configs/red-teaming/config_2/config.yml
+-rw-r--r--  2.0 unx      566 b- defN 24-Apr-01 20:07 examples/configs/sample/config.co
+-rw-r--r--  2.0 unx      678 b- defN 24-Apr-01 20:07 examples/configs/sample/config.yml
+-rw-r--r--  2.0 unx      570 b- defN 24-Apr-01 20:07 examples/configs/streaming/README.md
+-rw-r--r--  2.0 unx      438 b- defN 24-Apr-01 20:07 examples/configs/streaming/config.co
+-rw-r--r--  2.0 unx      500 b- defN 24-Apr-01 20:07 examples/configs/streaming/config.yml
+-rw-r--r--  2.0 unx     2357 b- defN 24-Apr-01 20:07 examples/configs/threads/README.md
+-rw-r--r--  2.0 unx      973 b- defN 24-Apr-01 20:07 examples/configs/threads/config.py
+-rw-r--r--  2.0 unx      205 b- defN 24-Apr-01 20:07 examples/configs/threads/config_1/config.yml
+-rw-r--r--  2.0 unx      241 b- defN 24-Apr-01 20:07 examples/configs/threads/config_1/rails.co
+-rw-r--r--  2.0 unx     5305 b- defN 24-Apr-01 20:07 examples/notebooks/generate_events_and_streaming.ipynb
+-rw-r--r--  2.0 unx      679 b- defN 24-Apr-01 20:07 examples/scripts/__init__.py
+-rw-r--r--  2.0 unx     3281 b- defN 24-Apr-01 20:07 examples/scripts/demo_llama_index_guardrails.py
+-rw-r--r--  2.0 unx     4962 b- defN 24-Apr-01 20:07 examples/scripts/demo_streaming.py
+-rw-r--r--  2.0 unx     5707 b- defN 24-Apr-01 20:07 examples/scripts/langchain/experiments.py
+-rw-r--r--  2.0 unx      962 b- defN 24-Apr-01 20:07 examples/server_configs/atomic/input_checking/config.yml
+-rw-r--r--  2.0 unx       76 b- defN 24-Apr-01 20:07 examples/server_configs/atomic/main/config.yml
+-rw-r--r--  2.0 unx      870 b- defN 24-Apr-01 20:07 examples/server_configs/atomic/output_checking/config.yml
+-rw-r--r--  2.0 unx       22 b- defN 24-Apr-01 20:07 examples/v2_x/language_reference/control_flow_tools/catch_failing_flow/config.yml
+-rw-r--r--  2.0 unx      461 b- defN 24-Apr-01 20:07 examples/v2_x/language_reference/control_flow_tools/catch_failing_flow/main.co
+-rw-r--r--  2.0 unx       85 b- defN 24-Apr-01 20:07 examples/v2_x/language_reference/development_and_debugging/print_statement/config.yml
+-rw-r--r--  2.0 unx       67 b- defN 24-Apr-01 20:07 examples/v2_x/language_reference/development_and_debugging/print_statement/main.co
+-rw-r--r--  2.0 unx       22 b- defN 24-Apr-01 20:07 examples/v2_x/language_reference/event_generation/event_groups/config.yml
+-rw-r--r--  2.0 unx      205 b- defN 24-Apr-01 20:07 examples/v2_x/language_reference/event_generation/event_groups/main.co
+-rw-r--r--  2.0 unx       22 b- defN 24-Apr-01 20:07 examples/v2_x/language_reference/event_generation/event_references/config.yml
+-rw-r--r--  2.0 unx      160 b- defN 24-Apr-01 20:07 examples/v2_x/language_reference/event_generation/event_references/main.co
+-rw-r--r--  2.0 unx       22 b- defN 24-Apr-01 20:07 examples/v2_x/language_reference/event_pattern_matching/dictionary_parameters/config.yml
+-rw-r--r--  2.0 unx       93 b- defN 24-Apr-01 20:07 examples/v2_x/language_reference/event_pattern_matching/dictionary_parameters/main.co
+-rw-r--r--  2.0 unx       22 b- defN 24-Apr-01 20:07 examples/v2_x/language_reference/event_pattern_matching/event_grouping/config.yml
+-rw-r--r--  2.0 unx      343 b- defN 24-Apr-01 20:07 examples/v2_x/language_reference/event_pattern_matching/event_grouping/main.co
+-rw-r--r--  2.0 unx       22 b- defN 24-Apr-01 20:07 examples/v2_x/language_reference/event_pattern_matching/event_grouping_advanced/config.yml
+-rw-r--r--  2.0 unx      435 b- defN 24-Apr-01 20:07 examples/v2_x/language_reference/event_pattern_matching/event_grouping_advanced/main.co
+-rw-r--r--  2.0 unx       22 b- defN 24-Apr-01 20:07 examples/v2_x/language_reference/event_pattern_matching/event_matching/config.yml
+-rw-r--r--  2.0 unx       97 b- defN 24-Apr-01 20:07 examples/v2_x/language_reference/event_pattern_matching/event_matching/main.co
+-rw-r--r--  2.0 unx       22 b- defN 24-Apr-01 20:07 examples/v2_x/language_reference/event_pattern_matching/event_reference/config.yml
+-rw-r--r--  2.0 unx      143 b- defN 24-Apr-01 20:07 examples/v2_x/language_reference/event_pattern_matching/event_reference/main.co
+-rw-r--r--  2.0 unx       22 b- defN 24-Apr-01 20:07 examples/v2_x/language_reference/event_pattern_matching/list_parameters/config.yml
+-rw-r--r--  2.0 unx       94 b- defN 24-Apr-01 20:07 examples/v2_x/language_reference/event_pattern_matching/list_parameters/main.co
+-rw-r--r--  2.0 unx       22 b- defN 24-Apr-01 20:07 examples/v2_x/language_reference/event_pattern_matching/regular_expression_parameters/config.yml
+-rw-r--r--  2.0 unx      279 b- defN 24-Apr-01 20:07 examples/v2_x/language_reference/event_pattern_matching/regular_expression_parameters/main.co
+-rw-r--r--  2.0 unx       22 b- defN 24-Apr-01 20:07 examples/v2_x/language_reference/event_pattern_matching/set_parameters/config.yml
+-rw-r--r--  2.0 unx       94 b- defN 24-Apr-01 20:07 examples/v2_x/language_reference/event_pattern_matching/set_parameters/main.co
+-rw-r--r--  2.0 unx       22 b- defN 24-Apr-01 20:07 examples/v2_x/language_reference/flows/action_conflict_resolution/config.yml
+-rw-r--r--  2.0 unx      639 b- defN 24-Apr-01 20:07 examples/v2_x/language_reference/flows/action_conflict_resolution/main.co
+-rw-r--r--  2.0 unx       22 b- defN 24-Apr-01 20:07 examples/v2_x/language_reference/flows/call_a_flow/config.yml
+-rw-r--r--  2.0 unx      365 b- defN 24-Apr-01 20:07 examples/v2_x/language_reference/flows/call_a_flow/main.co
+-rw-r--r--  2.0 unx       22 b- defN 24-Apr-01 20:07 examples/v2_x/language_reference/flows/concurrent_flows/config.yml
+-rw-r--r--  2.0 unx      476 b- defN 24-Apr-01 20:07 examples/v2_x/language_reference/flows/concurrent_flows/main.co
+-rw-r--r--  2.0 unx       22 b- defN 24-Apr-01 20:07 examples/v2_x/language_reference/flows/concurrent_flows_hierarchy/config.yml
+-rw-r--r--  2.0 unx      496 b- defN 24-Apr-01 20:07 examples/v2_x/language_reference/flows/concurrent_flows_hierarchy/main.co
+-rw-r--r--  2.0 unx       22 b- defN 24-Apr-01 20:07 examples/v2_x/language_reference/flows/flow_hierarchy/config.yml
+-rw-r--r--  2.0 unx      464 b- defN 24-Apr-01 20:07 examples/v2_x/language_reference/flows/flow_hierarchy/main.co
+-rw-r--r--  2.0 unx       22 b- defN 24-Apr-01 20:07 examples/v2_x/language_reference/flows/flow_parameters/config.yml
+-rw-r--r--  2.0 unx      166 b- defN 24-Apr-01 20:07 examples/v2_x/language_reference/flows/flow_parameters/main.co
+-rw-r--r--  2.0 unx       22 b- defN 24-Apr-01 20:07 examples/v2_x/language_reference/flows/flows_failing/config.yml
+-rw-r--r--  2.0 unx      497 b- defN 24-Apr-01 20:07 examples/v2_x/language_reference/flows/flows_failing/main.co
+-rw-r--r--  2.0 unx       22 b- defN 24-Apr-01 20:07 examples/v2_x/language_reference/flows/parallel_flows/config.yml
+-rw-r--r--  2.0 unx      476 b- defN 24-Apr-01 20:07 examples/v2_x/language_reference/flows/parallel_flows/main.co
+-rw-r--r--  2.0 unx       22 b- defN 24-Apr-01 20:07 examples/v2_x/language_reference/flows/parallel_flows_hierarchy/config.yml
+-rw-r--r--  2.0 unx      496 b- defN 24-Apr-01 20:07 examples/v2_x/language_reference/flows/parallel_flows_hierarchy/main.co
+-rw-r--r--  2.0 unx       22 b- defN 24-Apr-01 20:07 examples/v2_x/language_reference/flows/start_a_flow/config.yml
+-rw-r--r--  2.0 unx      381 b- defN 24-Apr-01 20:07 examples/v2_x/language_reference/flows/start_a_flow/main.co
+-rw-r--r--  2.0 unx       22 b- defN 24-Apr-01 20:07 examples/v2_x/language_reference/internal_events/undefined_flow/hello_world/config.yml
+-rw-r--r--  2.0 unx      509 b- defN 24-Apr-01 20:07 examples/v2_x/language_reference/internal_events/undefined_flow/hello_world/main.co
+-rw-r--r--  2.0 unx       22 b- defN 24-Apr-01 20:07 examples/v2_x/language_reference/introduction/hello_world/config.yml
+-rw-r--r--  2.0 unx       46 b- defN 24-Apr-01 20:07 examples/v2_x/language_reference/introduction/hello_world/main.co
+-rw-r--r--  2.0 unx       22 b- defN 24-Apr-01 20:07 examples/v2_x/language_reference/introduction/hello_world_umim/config.yml
+-rw-r--r--  2.0 unx      105 b- defN 24-Apr-01 20:07 examples/v2_x/language_reference/introduction/hello_world_umim/main.co
+-rw-r--r--  2.0 unx       22 b- defN 24-Apr-01 20:07 examples/v2_x/language_reference/introduction/interaction_sequence/config.yml
+-rw-r--r--  2.0 unx      317 b- defN 24-Apr-01 20:07 examples/v2_x/language_reference/introduction/interaction_sequence/main.co
+-rw-r--r--  2.0 unx     6775 b- defN 24-Apr-01 20:07 examples/v2_x/language_reference/llm/bot_intent_generation_example/config.yml
+-rw-r--r--  2.0 unx      148 b- defN 24-Apr-01 20:07 examples/v2_x/language_reference/llm/bot_intent_generation_example/main.co
+-rw-r--r--  2.0 unx      105 b- defN 24-Apr-01 20:07 examples/v2_x/language_reference/llm/nld_example/config.yml
+-rw-r--r--  2.0 unx      949 b- defN 24-Apr-01 20:07 examples/v2_x/language_reference/llm/nld_example/main.co
+-rw-r--r--  2.0 unx     6761 b- defN 24-Apr-01 20:07 examples/v2_x/language_reference/llm/user_intent_match_example/config.yml
+-rw-r--r--  2.0 unx      497 b- defN 24-Apr-01 20:07 examples/v2_x/language_reference/llm/user_intent_match_example/main.co
+-rw-r--r--  2.0 unx       22 b- defN 24-Apr-01 20:07 examples/v2_x/language_reference/more_on_flows/activate_flow/config.yml
+-rw-r--r--  2.0 unx      348 b- defN 24-Apr-01 20:07 examples/v2_x/language_reference/more_on_flows/activate_flow/main.co
+-rw-r--r--  2.0 unx       22 b- defN 24-Apr-01 20:07 examples/v2_x/language_reference/more_on_flows/interaction_loops/config.yml
+-rw-r--r--  2.0 unx      951 b- defN 24-Apr-01 20:07 examples/v2_x/language_reference/more_on_flows/interaction_loops/main.co
+-rw-r--r--  2.0 unx       22 b- defN 24-Apr-01 20:07 examples/v2_x/language_reference/more_on_flows/restart_flow_instance/config.yml
+-rw-r--r--  2.0 unx      348 b- defN 24-Apr-01 20:07 examples/v2_x/language_reference/more_on_flows/restart_flow_instance/main.co
+-rw-r--r--  2.0 unx       22 b- defN 24-Apr-01 20:07 examples/v2_x/language_reference/more_on_flows/start_new_flow_instance/config.yml
+-rw-r--r--  2.0 unx      441 b- defN 24-Apr-01 20:07 examples/v2_x/language_reference/more_on_flows/start_new_flow_instance/main.co
+-rw-r--r--  2.0 unx       22 b- defN 24-Apr-01 20:07 examples/v2_x/language_reference/working_with_actions/action_events/config.yml
+-rw-r--r--  2.0 unx      213 b- defN 24-Apr-01 20:07 examples/v2_x/language_reference/working_with_actions/action_events/main.co
+-rw-r--r--  2.0 unx       22 b- defN 24-Apr-01 20:07 examples/v2_x/language_reference/working_with_actions/action_grouping/config.yml
+-rw-r--r--  2.0 unx      309 b- defN 24-Apr-01 20:07 examples/v2_x/language_reference/working_with_actions/action_grouping/main.co
+-rw-r--r--  2.0 unx       22 b- defN 24-Apr-01 20:07 examples/v2_x/language_reference/working_with_actions/await_keyword/config.yml
+-rw-r--r--  2.0 unx      355 b- defN 24-Apr-01 20:07 examples/v2_x/language_reference/working_with_actions/await_keyword/main.co
+-rw-r--r--  2.0 unx       22 b- defN 24-Apr-01 20:07 examples/v2_x/language_reference/working_with_actions/dialog_pattern/config.yml
+-rw-r--r--  2.0 unx      457 b- defN 24-Apr-01 20:07 examples/v2_x/language_reference/working_with_actions/dialog_pattern/main.co
+-rw-r--r--  2.0 unx       22 b- defN 24-Apr-01 20:07 examples/v2_x/language_reference/working_with_actions/omit_wait_keyword/config.yml
+-rw-r--r--  2.0 unx      289 b- defN 24-Apr-01 20:07 examples/v2_x/language_reference/working_with_actions/omit_wait_keyword/main.co
+-rw-r--r--  2.0 unx       22 b- defN 24-Apr-01 20:07 examples/v2_x/language_reference/working_with_actions/start_keyword/config.yml
+-rw-r--r--  2.0 unx      193 b- defN 24-Apr-01 20:07 examples/v2_x/language_reference/working_with_actions/start_keyword/main.co
+-rw-r--r--  2.0 unx       22 b- defN 24-Apr-01 20:07 examples/v2_x/language_reference/working_with_actions/stop_action/config.yml
+-rw-r--r--  2.0 unx      128 b- defN 24-Apr-01 20:07 examples/v2_x/language_reference/working_with_actions/stop_action/main.co
+-rw-r--r--  2.0 unx       22 b- defN 24-Apr-01 20:07 examples/v2_x/language_reference/working_with_actions/wait_for_first_action_only/config.yml
+-rw-r--r--  2.0 unx      232 b- defN 24-Apr-01 20:07 examples/v2_x/language_reference/working_with_actions/wait_for_first_action_only/main.co
+-rw-r--r--  2.0 unx      144 b- defN 24-Apr-01 20:07 examples/v2_x/other/llm_flow/README.md
+-rw-r--r--  2.0 unx       99 b- defN 24-Apr-01 20:07 examples/v2_x/other/llm_flow/config.yml
+-rw-r--r--  2.0 unx      311 b- defN 24-Apr-01 20:07 examples/v2_x/other/llm_flow/rails.co
+-rw-r--r--  2.0 unx      412 b- defN 24-Apr-01 20:07 examples/v2_x/tutorial/guardrails_1/README.md
+-rw-r--r--  2.0 unx       99 b- defN 24-Apr-01 20:07 examples/v2_x/tutorial/guardrails_1/config.yml
+-rw-r--r--  2.0 unx      869 b- defN 24-Apr-01 20:07 examples/v2_x/tutorial/guardrails_1/rails.co
+-rw-r--r--  2.0 unx      492 b- defN 24-Apr-01 20:07 examples/v2_x/tutorial/hello_world_1/README.md
+-rw-r--r--  2.0 unx       99 b- defN 24-Apr-01 20:07 examples/v2_x/tutorial/hello_world_1/config.yml
+-rw-r--r--  2.0 unx       65 b- defN 24-Apr-01 20:07 examples/v2_x/tutorial/hello_world_1/rails.co
+-rw-r--r--  2.0 unx      534 b- defN 24-Apr-01 20:07 examples/v2_x/tutorial/hello_world_2/README.md
+-rw-r--r--  2.0 unx       99 b- defN 24-Apr-01 20:07 examples/v2_x/tutorial/hello_world_2/config.yml
+-rw-r--r--  2.0 unx      193 b- defN 24-Apr-01 20:07 examples/v2_x/tutorial/hello_world_2/rails.co
+-rw-r--r--  2.0 unx      676 b- defN 24-Apr-01 20:07 examples/v2_x/tutorial/hello_world_3/README.md
+-rw-r--r--  2.0 unx       99 b- defN 24-Apr-01 20:07 examples/v2_x/tutorial/hello_world_3/config.yml
+-rw-r--r--  2.0 unx      267 b- defN 24-Apr-01 20:07 examples/v2_x/tutorial/hello_world_3/rails.co
+-rw-r--r--  2.0 unx      729 b- defN 24-Apr-01 20:07 examples/v2_x/tutorial/interaction_loop/README.md
+-rw-r--r--  2.0 unx     2294 b- defN 24-Apr-01 20:07 examples/v2_x/tutorial/interaction_loop/config.yml
+-rw-r--r--  2.0 unx      603 b- defN 24-Apr-01 20:07 examples/v2_x/tutorial/interaction_loop/main.co
+-rw-r--r--  2.0 unx      232 b- defN 24-Apr-01 20:07 examples/v2_x/tutorial/multi_modal/README.md
+-rw-r--r--  2.0 unx       99 b- defN 24-Apr-01 20:07 examples/v2_x/tutorial/multi_modal/config.yml
+-rw-r--r--  2.0 unx      472 b- defN 24-Apr-01 20:07 examples/v2_x/tutorial/multi_modal/main.co
+-rw-r--r--  2.0 unx      983 b- defN 24-Apr-01 20:07 nemoguardrails/__init__.py
+-rw-r--r--  2.0 unx      754 b- defN 24-Apr-01 20:07 nemoguardrails/__main__.py
+-rw-r--r--  2.0 unx     1263 b- defN 24-Apr-01 20:07 nemoguardrails/context.py
+-rw-r--r--  2.0 unx     1537 b- defN 24-Apr-01 20:07 nemoguardrails/patch_asyncio.py
+-rw-r--r--  2.0 unx    11834 b- defN 24-Apr-01 20:07 nemoguardrails/streaming.py
+-rw-r--r--  2.0 unx     7936 b- defN 24-Apr-01 20:07 nemoguardrails/utils.py
+-rw-r--r--  2.0 unx      708 b- defN 24-Apr-01 20:07 nemoguardrails/actions/__init__.py
+-rw-r--r--  2.0 unx    12173 b- defN 24-Apr-01 20:07 nemoguardrails/actions/action_dispatcher.py
+-rw-r--r--  2.0 unx     2312 b- defN 24-Apr-01 20:07 nemoguardrails/actions/actions.py
+-rw-r--r--  2.0 unx     1627 b- defN 24-Apr-01 20:07 nemoguardrails/actions/core.py
+-rw-r--r--  2.0 unx     3524 b- defN 24-Apr-01 20:07 nemoguardrails/actions/math.py
+-rw-r--r--  2.0 unx     3106 b- defN 24-Apr-01 20:07 nemoguardrails/actions/retrieve_relevant_chunks.py
+-rw-r--r--  2.0 unx     2127 b- defN 24-Apr-01 20:07 nemoguardrails/actions/summarize_document.py
+-rw-r--r--  2.0 unx      679 b- defN 24-Apr-01 20:07 nemoguardrails/actions/langchain/__init__.py
+-rw-r--r--  2.0 unx     1830 b- defN 24-Apr-01 20:07 nemoguardrails/actions/langchain/actions.py
+-rw-r--r--  2.0 unx     3678 b- defN 24-Apr-01 20:07 nemoguardrails/actions/langchain/safetools.py
+-rw-r--r--  2.0 unx      679 b- defN 24-Apr-01 20:07 nemoguardrails/actions/llm/__init__.py
+-rw-r--r--  2.0 unx    51308 b- defN 24-Apr-01 20:07 nemoguardrails/actions/llm/generation.py
+-rw-r--r--  2.0 unx    17866 b- defN 24-Apr-01 20:07 nemoguardrails/actions/llm/utils.py
+-rw-r--r--  2.0 unx      679 b- defN 24-Apr-01 20:07 nemoguardrails/actions/v2_x/__init__.py
+-rw-r--r--  2.0 unx    21742 b- defN 24-Apr-01 20:07 nemoguardrails/actions/v2_x/generation.py
+-rw-r--r--  2.0 unx      700 b- defN 24-Apr-01 20:07 nemoguardrails/actions/validation/__init__.py
+-rw-r--r--  2.0 unx     4518 b- defN 24-Apr-01 20:07 nemoguardrails/actions/validation/base.py
+-rw-r--r--  2.0 unx     1375 b- defN 24-Apr-01 20:07 nemoguardrails/actions/validation/filter_secrets.py
+-rw-r--r--  2.0 unx      679 b- defN 24-Apr-01 20:07 nemoguardrails/actions_server/__init__.py
+-rw-r--r--  2.0 unx     2492 b- defN 24-Apr-01 20:07 nemoguardrails/actions_server/actions_server.py
+-rw-r--r--  2.0 unx     6086 b- defN 24-Apr-01 20:07 nemoguardrails/cli/__init__.py
+-rw-r--r--  2.0 unx    23763 b- defN 24-Apr-01 20:07 nemoguardrails/cli/chat.py
+-rw-r--r--  2.0 unx     1681 b- defN 24-Apr-01 20:07 nemoguardrails/colang/__init__.py
+-rw-r--r--  2.0 unx     4293 b- defN 24-Apr-01 20:07 nemoguardrails/colang/runtime.py
+-rw-r--r--  2.0 unx      679 b- defN 24-Apr-01 20:07 nemoguardrails/colang/v1_0/__init__.py
+-rw-r--r--  2.0 unx      679 b- defN 24-Apr-01 20:07 nemoguardrails/colang/v1_0/lang/__init__.py
+-rw-r--r--  2.0 unx    73671 b- defN 24-Apr-01 20:07 nemoguardrails/colang/v1_0/lang/colang_parser.py
+-rw-r--r--  2.0 unx    17098 b- defN 24-Apr-01 20:07 nemoguardrails/colang/v1_0/lang/comd_parser.py
+-rw-r--r--  2.0 unx    22118 b- defN 24-Apr-01 20:07 nemoguardrails/colang/v1_0/lang/coyml_parser.py
+-rw-r--r--  2.0 unx     4314 b- defN 24-Apr-01 20:07 nemoguardrails/colang/v1_0/lang/parser.py
+-rw-r--r--  2.0 unx    11644 b- defN 24-Apr-01 20:07 nemoguardrails/colang/v1_0/lang/utils.py
+-rw-r--r--  2.0 unx      679 b- defN 24-Apr-01 20:07 nemoguardrails/colang/v1_0/runtime/__init__.py
+-rw-r--r--  2.0 unx     2417 b- defN 24-Apr-01 20:07 nemoguardrails/colang/v1_0/runtime/eval.py
+-rw-r--r--  2.0 unx    24260 b- defN 24-Apr-01 20:07 nemoguardrails/colang/v1_0/runtime/flows.py
+-rw-r--r--  2.0 unx    18621 b- defN 24-Apr-01 20:07 nemoguardrails/colang/v1_0/runtime/runtime.py
+-rw-r--r--  2.0 unx     4942 b- defN 24-Apr-01 20:07 nemoguardrails/colang/v1_0/runtime/sliding.py
+-rw-r--r--  2.0 unx     1881 b- defN 24-Apr-01 20:07 nemoguardrails/colang/v1_0/runtime/utils.py
+-rw-r--r--  2.0 unx      679 b- defN 24-Apr-01 20:07 nemoguardrails/colang/v2_x/__init__.py
+-rw-r--r--  2.0 unx      679 b- defN 24-Apr-01 20:07 nemoguardrails/colang/v2_x/lang/__init__.py
+-rw-r--r--  2.0 unx    10686 b- defN 24-Apr-01 20:07 nemoguardrails/colang/v2_x/lang/colang_ast.py
+-rw-r--r--  2.0 unx    39684 b- defN 24-Apr-01 20:07 nemoguardrails/colang/v2_x/lang/expansion.py
+-rw-r--r--  2.0 unx     6849 b- defN 24-Apr-01 20:07 nemoguardrails/colang/v2_x/lang/parser.py
+-rw-r--r--  2.0 unx    20902 b- defN 24-Apr-01 20:07 nemoguardrails/colang/v2_x/lang/transformer.py
+-rw-r--r--  2.0 unx     1296 b- defN 24-Apr-01 20:07 nemoguardrails/colang/v2_x/lang/utils.py
+-rw-r--r--  2.0 unx      679 b- defN 24-Apr-01 20:07 nemoguardrails/colang/v2_x/lang/grammar/__init__.py
+-rw-r--r--  2.0 unx     7421 b- defN 24-Apr-01 20:07 nemoguardrails/colang/v2_x/lang/grammar/colang.lark
+-rw-r--r--  2.0 unx     1314 b- defN 24-Apr-01 20:07 nemoguardrails/colang/v2_x/lang/grammar/load.py
+-rw-r--r--  2.0 unx    20852 b- defN 24-Apr-01 20:07 nemoguardrails/colang/v2_x/library/avatars.co
+-rw-r--r--  2.0 unx      713 b- defN 24-Apr-01 20:07 nemoguardrails/colang/v2_x/library/core.co
+-rw-r--r--  2.0 unx     1184 b- defN 24-Apr-01 20:07 nemoguardrails/colang/v2_x/library/guardrails.co
+-rw-r--r--  2.0 unx     3931 b- defN 24-Apr-01 20:07 nemoguardrails/colang/v2_x/library/llm.co
+-rw-r--r--  2.0 unx      683 b- defN 24-Apr-01 20:07 nemoguardrails/colang/v2_x/library/utils.co
+-rw-r--r--  2.0 unx      679 b- defN 24-Apr-01 20:07 nemoguardrails/colang/v2_x/runtime/__init__.py
+-rw-r--r--  2.0 unx      553 b- defN 24-Apr-01 20:07 nemoguardrails/colang/v2_x/runtime/errors.py
+-rw-r--r--  2.0 unx    11075 b- defN 24-Apr-01 20:07 nemoguardrails/colang/v2_x/runtime/eval.py
+-rw-r--r--  2.0 unx    29322 b- defN 24-Apr-01 20:07 nemoguardrails/colang/v2_x/runtime/flows.py
+-rw-r--r--  2.0 unx    29461 b- defN 24-Apr-01 20:07 nemoguardrails/colang/v2_x/runtime/runtime.py
+-rw-r--r--  2.0 unx     7804 b- defN 24-Apr-01 20:07 nemoguardrails/colang/v2_x/runtime/serialization.py
+-rw-r--r--  2.0 unx    90268 b- defN 24-Apr-01 20:07 nemoguardrails/colang/v2_x/runtime/statemachine.py
+-rw-r--r--  2.0 unx     1050 b- defN 24-Apr-01 20:07 nemoguardrails/colang/v2_x/runtime/system_functions.py
+-rw-r--r--  2.0 unx     1459 b- defN 24-Apr-01 20:07 nemoguardrails/colang/v2_x/runtime/utils.py
+-rw-r--r--  2.0 unx      679 b- defN 24-Apr-01 20:07 nemoguardrails/embeddings/__init__.py
+-rw-r--r--  2.0 unx     9994 b- defN 24-Apr-01 20:07 nemoguardrails/embeddings/basic.py
+-rw-r--r--  2.0 unx     9773 b- defN 24-Apr-01 20:07 nemoguardrails/embeddings/cache.py
+-rw-r--r--  2.0 unx     2438 b- defN 24-Apr-01 20:07 nemoguardrails/embeddings/index.py
+-rw-r--r--  2.0 unx     3268 b- defN 24-Apr-01 20:07 nemoguardrails/embeddings/embedding_providers/__init__.py
+-rw-r--r--  2.0 unx     3285 b- defN 24-Apr-01 20:07 nemoguardrails/embeddings/embedding_providers/fastembed.py
+-rw-r--r--  2.0 unx     3983 b- defN 24-Apr-01 20:07 nemoguardrails/embeddings/embedding_providers/openai.py
+-rw-r--r--  2.0 unx     2776 b- defN 24-Apr-01 20:07 nemoguardrails/embeddings/embedding_providers/sentence_transformers.py
+-rw-r--r--  2.0 unx      679 b- defN 24-Apr-01 20:07 nemoguardrails/eval/__init__.py
+-rw-r--r--  2.0 unx     9013 b- defN 24-Apr-01 20:07 nemoguardrails/eval/evaluate_factcheck.py
+-rw-r--r--  2.0 unx     8606 b- defN 24-Apr-01 20:07 nemoguardrails/eval/evaluate_hallucination.py
+-rw-r--r--  2.0 unx     9657 b- defN 24-Apr-01 20:07 nemoguardrails/eval/evaluate_moderation.py
+-rw-r--r--  2.0 unx    17715 b- defN 24-Apr-01 20:07 nemoguardrails/eval/evaluate_topical.py
+-rw-r--r--  2.0 unx     1685 b- defN 24-Apr-01 20:07 nemoguardrails/eval/utils.py
+-rw-r--r--  2.0 unx      679 b- defN 24-Apr-01 20:07 nemoguardrails/eval/cli/__init__.py
+-rw-r--r--  2.0 unx    10465 b- defN 24-Apr-01 20:07 nemoguardrails/eval/cli/evaluate.py
+-rw-r--r--  2.0 unx     2572 b- defN 24-Apr-01 20:07 nemoguardrails/eval/cli/simplify_formatter.py
+-rw-r--r--  2.0 unx     4208 b- defN 24-Apr-01 20:07 nemoguardrails/eval/data/factchecking/README.md
+-rw-r--r--  2.0 unx      679 b- defN 24-Apr-01 20:07 nemoguardrails/eval/data/factchecking/__init__.py
+-rw-r--r--  2.0 unx     1539 b- defN 24-Apr-01 20:07 nemoguardrails/eval/data/factchecking/process_msmarco_data.py
+-rw-r--r--  2.0 unx      283 b- defN 24-Apr-01 20:07 nemoguardrails/eval/data/factchecking/sample.json
+-rw-r--r--  2.0 unx      679 b- defN 24-Apr-01 20:07 nemoguardrails/eval/data/hallucination/__init__.py
+-rw-r--r--  2.0 unx     1071 b- defN 24-Apr-01 20:07 nemoguardrails/eval/data/hallucination/sample.txt
+-rw-r--r--  2.0 unx     3956 b- defN 24-Apr-01 20:07 nemoguardrails/eval/data/moderation/README.md
+-rw-r--r--  2.0 unx      679 b- defN 24-Apr-01 20:07 nemoguardrails/eval/data/moderation/__init__.py
+-rw-r--r--  2.0 unx       24 b- defN 24-Apr-01 20:07 nemoguardrails/eval/data/moderation/harmful.txt
+-rw-r--r--  2.0 unx       21 b- defN 24-Apr-01 20:07 nemoguardrails/eval/data/moderation/helpful.txt
+-rw-r--r--  2.0 unx     4451 b- defN 24-Apr-01 20:07 nemoguardrails/eval/data/moderation/process_anthropic_dataset.py
+-rw-r--r--  2.0 unx     4838 b- defN 24-Apr-01 20:07 nemoguardrails/eval/data/topical/README.md
+-rw-r--r--  2.0 unx      679 b- defN 24-Apr-01 20:07 nemoguardrails/eval/data/topical/__init__.py
+-rw-r--r--  2.0 unx     2465 b- defN 24-Apr-01 20:07 nemoguardrails/eval/data/topical/create_colang_intent_file.py
+-rw-r--r--  2.0 unx    12970 b- defN 24-Apr-01 20:07 nemoguardrails/eval/data/topical/dataset_tools.py
+-rw-r--r--  2.0 unx     5190 b- defN 24-Apr-01 20:07 nemoguardrails/eval/data/topical/banking/categories_canonical_forms.json
+-rw-r--r--  2.0 unx      889 b- defN 24-Apr-01 20:07 nemoguardrails/eval/data/topical/banking/config.yml
+-rw-r--r--  2.0 unx     9186 b- defN 24-Apr-01 20:07 nemoguardrails/eval/data/topical/banking/flows.co
+-rw-r--r--  2.0 unx     6118 b- defN 24-Apr-01 20:07 nemoguardrails/eval/data/topical/chitchat/bot.co
+-rw-r--r--  2.0 unx      863 b- defN 24-Apr-01 20:07 nemoguardrails/eval/data/topical/chitchat/config.yml
+-rw-r--r--  2.0 unx     6844 b- defN 24-Apr-01 20:07 nemoguardrails/eval/data/topical/chitchat/flows.co
+-rw-r--r--  2.0 unx     3260 b- defN 24-Apr-01 20:07 nemoguardrails/eval/data/topical/chitchat/intent_canonical_forms.json
+-rw-r--r--  2.0 unx      199 b- defN 24-Apr-01 20:07 nemoguardrails/eval/data/topical/chitchat/user-other.co
+-rw-r--r--  2.0 unx      679 b- defN 24-Apr-01 20:07 nemoguardrails/integrations/__init__.py
+-rw-r--r--  2.0 unx      679 b- defN 24-Apr-01 20:07 nemoguardrails/integrations/langchain/__init__.py
+-rw-r--r--  2.0 unx     9237 b- defN 24-Apr-01 20:07 nemoguardrails/integrations/langchain/runnable_rails.py
+-rw-r--r--  2.0 unx      679 b- defN 24-Apr-01 20:07 nemoguardrails/kb/__init__.py
+-rw-r--r--  2.0 unx     7342 b- defN 24-Apr-01 20:07 nemoguardrails/kb/kb.py
+-rw-r--r--  2.0 unx     3878 b- defN 24-Apr-01 20:07 nemoguardrails/kb/utils.py
+-rw-r--r--  2.0 unx      679 b- defN 24-Apr-01 20:07 nemoguardrails/library/__init__.py
+-rw-r--r--  2.0 unx      679 b- defN 24-Apr-01 20:07 nemoguardrails/library/activefence/__init__.py
+-rw-r--r--  2.0 unx     2430 b- defN 24-Apr-01 20:07 nemoguardrails/library/activefence/actions.py
+-rw-r--r--  2.0 unx     2249 b- defN 24-Apr-01 20:07 nemoguardrails/library/activefence/flows.co
+-rw-r--r--  2.0 unx      679 b- defN 24-Apr-01 20:07 nemoguardrails/library/factchecking/__init__.py
+-rw-r--r--  2.0 unx      679 b- defN 24-Apr-01 20:07 nemoguardrails/library/factchecking/align_score/__init__.py
+-rw-r--r--  2.0 unx     2222 b- defN 24-Apr-01 20:07 nemoguardrails/library/factchecking/align_score/actions.py
+-rw-r--r--  2.0 unx      405 b- defN 24-Apr-01 20:07 nemoguardrails/library/factchecking/align_score/flows.co
+-rw-r--r--  2.0 unx     1714 b- defN 24-Apr-01 20:07 nemoguardrails/library/factchecking/align_score/request.py
+-rw-r--r--  2.0 unx      147 b- defN 24-Apr-01 20:07 nemoguardrails/library/factchecking/align_score/requirements.txt
+-rw-r--r--  2.0 unx     3373 b- defN 24-Apr-01 20:07 nemoguardrails/library/factchecking/align_score/server.py
+-rw-r--r--  2.0 unx      679 b- defN 24-Apr-01 20:07 nemoguardrails/library/hallucination/__init__.py
+-rw-r--r--  2.0 unx     5280 b- defN 24-Apr-01 20:07 nemoguardrails/library/hallucination/actions.py
+-rw-r--r--  2.0 unx      895 b- defN 24-Apr-01 20:07 nemoguardrails/library/hallucination/flows.co
+-rw-r--r--  2.0 unx      679 b- defN 24-Apr-01 20:07 nemoguardrails/library/jailbreak_detection/__init__.py
+-rw-r--r--  2.0 unx     2492 b- defN 24-Apr-01 20:07 nemoguardrails/library/jailbreak_detection/actions.py
+-rw-r--r--  2.0 unx      322 b- defN 24-Apr-01 20:07 nemoguardrails/library/jailbreak_detection/flows.co
+-rw-r--r--  2.0 unx     1739 b- defN 24-Apr-01 20:07 nemoguardrails/library/jailbreak_detection/request.py
+-rw-r--r--  2.0 unx      214 b- defN 24-Apr-01 20:07 nemoguardrails/library/jailbreak_detection/requirements.txt
+-rw-r--r--  2.0 unx     3063 b- defN 24-Apr-01 20:07 nemoguardrails/library/jailbreak_detection/server.py
+-rw-r--r--  2.0 unx      679 b- defN 24-Apr-01 20:07 nemoguardrails/library/jailbreak_detection/heuristics/__init__.py
+-rw-r--r--  2.0 unx     3500 b- defN 24-Apr-01 20:07 nemoguardrails/library/jailbreak_detection/heuristics/checks.py
+-rw-r--r--  2.0 unx      679 b- defN 24-Apr-01 20:07 nemoguardrails/library/llama_guard/__init__.py
+-rw-r--r--  2.0 unx     4200 b- defN 24-Apr-01 20:07 nemoguardrails/library/llama_guard/actions.py
+-rw-r--r--  2.0 unx      709 b- defN 24-Apr-01 20:07 nemoguardrails/library/llama_guard/flows.co
+-rw-r--r--  2.0 unx       81 b- defN 24-Apr-01 20:07 nemoguardrails/library/llama_guard/requirements.txt
+-rw-r--r--  2.0 unx      679 b- defN 24-Apr-01 20:07 nemoguardrails/library/self_check/__init__.py
+-rw-r--r--  2.0 unx      679 b- defN 24-Apr-01 20:07 nemoguardrails/library/self_check/facts/__init__.py
+-rw-r--r--  2.0 unx     2160 b- defN 24-Apr-01 20:07 nemoguardrails/library/self_check/facts/actions.py
+-rw-r--r--  2.0 unx      459 b- defN 24-Apr-01 20:07 nemoguardrails/library/self_check/facts/flows.co
+-rw-r--r--  2.0 unx      679 b- defN 24-Apr-01 20:07 nemoguardrails/library/self_check/input_check/__init__.py
+-rw-r--r--  2.0 unx     2613 b- defN 24-Apr-01 20:07 nemoguardrails/library/self_check/input_check/actions.py
+-rw-r--r--  2.0 unx      191 b- defN 24-Apr-01 20:07 nemoguardrails/library/self_check/input_check/flows.co
+-rw-r--r--  2.0 unx      679 b- defN 24-Apr-01 20:07 nemoguardrails/library/self_check/output_check/__init__.py
+-rw-r--r--  2.0 unx     2562 b- defN 24-Apr-01 20:07 nemoguardrails/library/self_check/output_check/actions.py
+-rw-r--r--  2.0 unx      194 b- defN 24-Apr-01 20:07 nemoguardrails/library/self_check/output_check/flows.co
+-rw-r--r--  2.0 unx      679 b- defN 24-Apr-01 20:07 nemoguardrails/library/sensitive_data_detection/__init__.py
+-rw-r--r--  2.0 unx     4926 b- defN 24-Apr-01 20:07 nemoguardrails/library/sensitive_data_detection/actions.py
+-rw-r--r--  2.0 unx     1466 b- defN 24-Apr-01 20:07 nemoguardrails/library/sensitive_data_detection/flows.co
+-rw-r--r--  2.0 unx      679 b- defN 24-Apr-01 20:07 nemoguardrails/llm/__init__.py
+-rw-r--r--  2.0 unx     9899 b- defN 24-Apr-01 20:07 nemoguardrails/llm/filters.py
+-rw-r--r--  2.0 unx     3263 b- defN 24-Apr-01 20:07 nemoguardrails/llm/helpers.py
+-rw-r--r--  2.0 unx     2237 b- defN 24-Apr-01 20:07 nemoguardrails/llm/output_parsers.py
+-rw-r--r--  2.0 unx     3179 b- defN 24-Apr-01 20:07 nemoguardrails/llm/params.py
+-rw-r--r--  2.0 unx     5154 b- defN 24-Apr-01 20:07 nemoguardrails/llm/prompts.py
+-rw-r--r--  2.0 unx    10800 b- defN 24-Apr-01 20:07 nemoguardrails/llm/taskmanager.py
+-rw-r--r--  2.0 unx     1750 b- defN 24-Apr-01 20:07 nemoguardrails/llm/types.py
+-rw-r--r--  2.0 unx     2608 b- defN 24-Apr-01 20:07 nemoguardrails/llm/prompts/cohere.yml
+-rw-r--r--  2.0 unx     2887 b- defN 24-Apr-01 20:07 nemoguardrails/llm/prompts/dolly.yml
+-rw-r--r--  2.0 unx     5739 b- defN 24-Apr-01 20:07 nemoguardrails/llm/prompts/general.yml
+-rw-r--r--  2.0 unx     3235 b- defN 24-Apr-01 20:07 nemoguardrails/llm/prompts/mosaic.yml
+-rw-r--r--  2.0 unx     6149 b- defN 24-Apr-01 20:07 nemoguardrails/llm/prompts/nemollm.yml
+-rw-r--r--  2.0 unx     3547 b- defN 24-Apr-01 20:07 nemoguardrails/llm/prompts/openai-chatgpt.yml
+-rw-r--r--  2.0 unx       57 b- defN 24-Apr-01 20:07 nemoguardrails/llm/prompts/openai-gpt35-instruct.yml
+-rw-r--r--  2.0 unx      819 b- defN 24-Apr-01 20:07 nemoguardrails/llm/providers/__init__.py
+-rw-r--r--  2.0 unx    11487 b- defN 24-Apr-01 20:07 nemoguardrails/llm/providers/nemollm.py
+-rw-r--r--  2.0 unx     9495 b- defN 24-Apr-01 20:07 nemoguardrails/llm/providers/providers.py
+-rw-r--r--  2.0 unx      729 b- defN 24-Apr-01 20:07 nemoguardrails/llm/providers/huggingface/__init__.py
+-rw-r--r--  2.0 unx     1939 b- defN 24-Apr-01 20:07 nemoguardrails/llm/providers/huggingface/streamers.py
+-rw-r--r--  2.0 unx      679 b- defN 24-Apr-01 20:07 nemoguardrails/llm/providers/trtllm/__init__.py
+-rw-r--r--  2.0 unx     8353 b- defN 24-Apr-01 20:07 nemoguardrails/llm/providers/trtllm/client.py
+-rw-r--r--  2.0 unx     5827 b- defN 24-Apr-01 20:07 nemoguardrails/llm/providers/trtllm/llm.py
+-rw-r--r--  2.0 unx      679 b- defN 24-Apr-01 20:07 nemoguardrails/logging/__init__.py
+-rw-r--r--  2.0 unx     9703 b- defN 24-Apr-01 20:07 nemoguardrails/logging/callbacks.py
+-rw-r--r--  2.0 unx     3710 b- defN 24-Apr-01 20:07 nemoguardrails/logging/explain.py
+-rw-r--r--  2.0 unx    10887 b- defN 24-Apr-01 20:07 nemoguardrails/logging/processing_log.py
+-rw-r--r--  2.0 unx     2041 b- defN 24-Apr-01 20:07 nemoguardrails/logging/simplify_formatter.py
+-rw-r--r--  2.0 unx     2062 b- defN 24-Apr-01 20:07 nemoguardrails/logging/stats.py
+-rw-r--r--  2.0 unx     9095 b- defN 24-Apr-01 20:07 nemoguardrails/logging/verbose.py
+-rw-r--r--  2.0 unx      751 b- defN 24-Apr-01 20:07 nemoguardrails/rails/__init__.py
+-rw-r--r--  2.0 unx      679 b- defN 24-Apr-01 20:07 nemoguardrails/rails/llm/__init__.py
+-rw-r--r--  2.0 unx    35933 b- defN 24-Apr-01 20:07 nemoguardrails/rails/llm/config.py
+-rw-r--r--  2.0 unx     1906 b- defN 24-Apr-01 20:07 nemoguardrails/rails/llm/default_config.yml
+-rw-r--r--  2.0 unx     3123 b- defN 24-Apr-01 20:07 nemoguardrails/rails/llm/default_config_v2.yml
+-rw-r--r--  2.0 unx     4668 b- defN 24-Apr-01 20:07 nemoguardrails/rails/llm/llm_flows.co
+-rw-r--r--  2.0 unx    42747 b- defN 24-Apr-01 20:07 nemoguardrails/rails/llm/llmrails.py
+-rw-r--r--  2.0 unx    15405 b- defN 24-Apr-01 20:07 nemoguardrails/rails/llm/options.py
+-rw-r--r--  2.0 unx     1511 b- defN 24-Apr-01 20:07 nemoguardrails/rails/llm/utils.py
+-rw-r--r--  2.0 unx      679 b- defN 24-Apr-01 20:07 nemoguardrails/server/__init__.py
+-rw-r--r--  2.0 unx    18827 b- defN 24-Apr-01 20:07 nemoguardrails/server/api.py
+-rw-r--r--  2.0 unx      679 b- defN 24-Apr-01 20:07 nemoguardrails/server/datastore/__init__.py
+-rw-r--r--  2.0 unx     1305 b- defN 24-Apr-01 20:07 nemoguardrails/server/datastore/datastore.py
+-rw-r--r--  2.0 unx     1469 b- defN 24-Apr-01 20:07 nemoguardrails/server/datastore/memory_store.py
+-rw-r--r--  2.0 unx     2090 b- defN 24-Apr-01 20:07 nemoguardrails/server/datastore/redis_store.py
+-rw-r--r--  2.0 unx     9861 b- defN 24-Apr-01 20:07 nemoguardrails-0.8.2.dist-info/LICENSE-Apache-2.0.txt
+-rw-r--r--  2.0 unx      654 b- defN 24-Apr-01 20:07 nemoguardrails-0.8.2.dist-info/LICENSE.md
+-rw-r--r--  2.0 unx    20573 b- defN 24-Apr-01 20:07 nemoguardrails-0.8.2.dist-info/METADATA
+-rw-r--r--  2.0 unx       92 b- defN 24-Apr-01 20:07 nemoguardrails-0.8.2.dist-info/WHEEL
+-rw-r--r--  2.0 unx       63 b- defN 24-Apr-01 20:07 nemoguardrails-0.8.2.dist-info/entry_points.txt
+-rw-r--r--  2.0 unx       15 b- defN 24-Apr-01 20:07 nemoguardrails-0.8.2.dist-info/top_level.txt
+-rw-rw-r--  2.0 unx    47498 b- defN 24-Apr-01 20:07 nemoguardrails-0.8.2.dist-info/RECORD
+447 files, 6734452 bytes uncompressed, 2486276 bytes compressed:  63.1%
```

## zipnote {}

```diff
@@ -243,14 +243,26 @@
 
 Filename: examples/configs/llm/nemollm/config.yml
 Comment: 
 
 Filename: examples/configs/llm/nemollm/rails.co
 Comment: 
 
+Filename: examples/configs/llm/vertexai/README.md
+Comment: 
+
+Filename: examples/configs/llm/vertexai/config.yml
+Comment: 
+
+Filename: examples/configs/llm/vertexai/prompts.yml
+Comment: 
+
+Filename: examples/configs/llm/vertexai/rails.co
+Comment: 
+
 Filename: examples/configs/rag/README.md
 Comment: 
 
 Filename: examples/configs/rag/__init__.py
 Comment: 
 
 Filename: examples/configs/rag/custom_rag_output_rails/README.md
@@ -402,14 +414,323 @@
 
 Filename: examples/server_configs/atomic/main/config.yml
 Comment: 
 
 Filename: examples/server_configs/atomic/output_checking/config.yml
 Comment: 
 
+Filename: examples/v2_x/language_reference/control_flow_tools/catch_failing_flow/config.yml
+Comment: 
+
+Filename: examples/v2_x/language_reference/control_flow_tools/catch_failing_flow/main.co
+Comment: 
+
+Filename: examples/v2_x/language_reference/development_and_debugging/print_statement/config.yml
+Comment: 
+
+Filename: examples/v2_x/language_reference/development_and_debugging/print_statement/main.co
+Comment: 
+
+Filename: examples/v2_x/language_reference/event_generation/event_groups/config.yml
+Comment: 
+
+Filename: examples/v2_x/language_reference/event_generation/event_groups/main.co
+Comment: 
+
+Filename: examples/v2_x/language_reference/event_generation/event_references/config.yml
+Comment: 
+
+Filename: examples/v2_x/language_reference/event_generation/event_references/main.co
+Comment: 
+
+Filename: examples/v2_x/language_reference/event_pattern_matching/dictionary_parameters/config.yml
+Comment: 
+
+Filename: examples/v2_x/language_reference/event_pattern_matching/dictionary_parameters/main.co
+Comment: 
+
+Filename: examples/v2_x/language_reference/event_pattern_matching/event_grouping/config.yml
+Comment: 
+
+Filename: examples/v2_x/language_reference/event_pattern_matching/event_grouping/main.co
+Comment: 
+
+Filename: examples/v2_x/language_reference/event_pattern_matching/event_grouping_advanced/config.yml
+Comment: 
+
+Filename: examples/v2_x/language_reference/event_pattern_matching/event_grouping_advanced/main.co
+Comment: 
+
+Filename: examples/v2_x/language_reference/event_pattern_matching/event_matching/config.yml
+Comment: 
+
+Filename: examples/v2_x/language_reference/event_pattern_matching/event_matching/main.co
+Comment: 
+
+Filename: examples/v2_x/language_reference/event_pattern_matching/event_reference/config.yml
+Comment: 
+
+Filename: examples/v2_x/language_reference/event_pattern_matching/event_reference/main.co
+Comment: 
+
+Filename: examples/v2_x/language_reference/event_pattern_matching/list_parameters/config.yml
+Comment: 
+
+Filename: examples/v2_x/language_reference/event_pattern_matching/list_parameters/main.co
+Comment: 
+
+Filename: examples/v2_x/language_reference/event_pattern_matching/regular_expression_parameters/config.yml
+Comment: 
+
+Filename: examples/v2_x/language_reference/event_pattern_matching/regular_expression_parameters/main.co
+Comment: 
+
+Filename: examples/v2_x/language_reference/event_pattern_matching/set_parameters/config.yml
+Comment: 
+
+Filename: examples/v2_x/language_reference/event_pattern_matching/set_parameters/main.co
+Comment: 
+
+Filename: examples/v2_x/language_reference/flows/action_conflict_resolution/config.yml
+Comment: 
+
+Filename: examples/v2_x/language_reference/flows/action_conflict_resolution/main.co
+Comment: 
+
+Filename: examples/v2_x/language_reference/flows/call_a_flow/config.yml
+Comment: 
+
+Filename: examples/v2_x/language_reference/flows/call_a_flow/main.co
+Comment: 
+
+Filename: examples/v2_x/language_reference/flows/concurrent_flows/config.yml
+Comment: 
+
+Filename: examples/v2_x/language_reference/flows/concurrent_flows/main.co
+Comment: 
+
+Filename: examples/v2_x/language_reference/flows/concurrent_flows_hierarchy/config.yml
+Comment: 
+
+Filename: examples/v2_x/language_reference/flows/concurrent_flows_hierarchy/main.co
+Comment: 
+
+Filename: examples/v2_x/language_reference/flows/flow_hierarchy/config.yml
+Comment: 
+
+Filename: examples/v2_x/language_reference/flows/flow_hierarchy/main.co
+Comment: 
+
+Filename: examples/v2_x/language_reference/flows/flow_parameters/config.yml
+Comment: 
+
+Filename: examples/v2_x/language_reference/flows/flow_parameters/main.co
+Comment: 
+
+Filename: examples/v2_x/language_reference/flows/flows_failing/config.yml
+Comment: 
+
+Filename: examples/v2_x/language_reference/flows/flows_failing/main.co
+Comment: 
+
+Filename: examples/v2_x/language_reference/flows/parallel_flows/config.yml
+Comment: 
+
+Filename: examples/v2_x/language_reference/flows/parallel_flows/main.co
+Comment: 
+
+Filename: examples/v2_x/language_reference/flows/parallel_flows_hierarchy/config.yml
+Comment: 
+
+Filename: examples/v2_x/language_reference/flows/parallel_flows_hierarchy/main.co
+Comment: 
+
+Filename: examples/v2_x/language_reference/flows/start_a_flow/config.yml
+Comment: 
+
+Filename: examples/v2_x/language_reference/flows/start_a_flow/main.co
+Comment: 
+
+Filename: examples/v2_x/language_reference/internal_events/undefined_flow/hello_world/config.yml
+Comment: 
+
+Filename: examples/v2_x/language_reference/internal_events/undefined_flow/hello_world/main.co
+Comment: 
+
+Filename: examples/v2_x/language_reference/introduction/hello_world/config.yml
+Comment: 
+
+Filename: examples/v2_x/language_reference/introduction/hello_world/main.co
+Comment: 
+
+Filename: examples/v2_x/language_reference/introduction/hello_world_umim/config.yml
+Comment: 
+
+Filename: examples/v2_x/language_reference/introduction/hello_world_umim/main.co
+Comment: 
+
+Filename: examples/v2_x/language_reference/introduction/interaction_sequence/config.yml
+Comment: 
+
+Filename: examples/v2_x/language_reference/introduction/interaction_sequence/main.co
+Comment: 
+
+Filename: examples/v2_x/language_reference/llm/bot_intent_generation_example/config.yml
+Comment: 
+
+Filename: examples/v2_x/language_reference/llm/bot_intent_generation_example/main.co
+Comment: 
+
+Filename: examples/v2_x/language_reference/llm/nld_example/config.yml
+Comment: 
+
+Filename: examples/v2_x/language_reference/llm/nld_example/main.co
+Comment: 
+
+Filename: examples/v2_x/language_reference/llm/user_intent_match_example/config.yml
+Comment: 
+
+Filename: examples/v2_x/language_reference/llm/user_intent_match_example/main.co
+Comment: 
+
+Filename: examples/v2_x/language_reference/more_on_flows/activate_flow/config.yml
+Comment: 
+
+Filename: examples/v2_x/language_reference/more_on_flows/activate_flow/main.co
+Comment: 
+
+Filename: examples/v2_x/language_reference/more_on_flows/interaction_loops/config.yml
+Comment: 
+
+Filename: examples/v2_x/language_reference/more_on_flows/interaction_loops/main.co
+Comment: 
+
+Filename: examples/v2_x/language_reference/more_on_flows/restart_flow_instance/config.yml
+Comment: 
+
+Filename: examples/v2_x/language_reference/more_on_flows/restart_flow_instance/main.co
+Comment: 
+
+Filename: examples/v2_x/language_reference/more_on_flows/start_new_flow_instance/config.yml
+Comment: 
+
+Filename: examples/v2_x/language_reference/more_on_flows/start_new_flow_instance/main.co
+Comment: 
+
+Filename: examples/v2_x/language_reference/working_with_actions/action_events/config.yml
+Comment: 
+
+Filename: examples/v2_x/language_reference/working_with_actions/action_events/main.co
+Comment: 
+
+Filename: examples/v2_x/language_reference/working_with_actions/action_grouping/config.yml
+Comment: 
+
+Filename: examples/v2_x/language_reference/working_with_actions/action_grouping/main.co
+Comment: 
+
+Filename: examples/v2_x/language_reference/working_with_actions/await_keyword/config.yml
+Comment: 
+
+Filename: examples/v2_x/language_reference/working_with_actions/await_keyword/main.co
+Comment: 
+
+Filename: examples/v2_x/language_reference/working_with_actions/dialog_pattern/config.yml
+Comment: 
+
+Filename: examples/v2_x/language_reference/working_with_actions/dialog_pattern/main.co
+Comment: 
+
+Filename: examples/v2_x/language_reference/working_with_actions/omit_wait_keyword/config.yml
+Comment: 
+
+Filename: examples/v2_x/language_reference/working_with_actions/omit_wait_keyword/main.co
+Comment: 
+
+Filename: examples/v2_x/language_reference/working_with_actions/start_keyword/config.yml
+Comment: 
+
+Filename: examples/v2_x/language_reference/working_with_actions/start_keyword/main.co
+Comment: 
+
+Filename: examples/v2_x/language_reference/working_with_actions/stop_action/config.yml
+Comment: 
+
+Filename: examples/v2_x/language_reference/working_with_actions/stop_action/main.co
+Comment: 
+
+Filename: examples/v2_x/language_reference/working_with_actions/wait_for_first_action_only/config.yml
+Comment: 
+
+Filename: examples/v2_x/language_reference/working_with_actions/wait_for_first_action_only/main.co
+Comment: 
+
+Filename: examples/v2_x/other/llm_flow/README.md
+Comment: 
+
+Filename: examples/v2_x/other/llm_flow/config.yml
+Comment: 
+
+Filename: examples/v2_x/other/llm_flow/rails.co
+Comment: 
+
+Filename: examples/v2_x/tutorial/guardrails_1/README.md
+Comment: 
+
+Filename: examples/v2_x/tutorial/guardrails_1/config.yml
+Comment: 
+
+Filename: examples/v2_x/tutorial/guardrails_1/rails.co
+Comment: 
+
+Filename: examples/v2_x/tutorial/hello_world_1/README.md
+Comment: 
+
+Filename: examples/v2_x/tutorial/hello_world_1/config.yml
+Comment: 
+
+Filename: examples/v2_x/tutorial/hello_world_1/rails.co
+Comment: 
+
+Filename: examples/v2_x/tutorial/hello_world_2/README.md
+Comment: 
+
+Filename: examples/v2_x/tutorial/hello_world_2/config.yml
+Comment: 
+
+Filename: examples/v2_x/tutorial/hello_world_2/rails.co
+Comment: 
+
+Filename: examples/v2_x/tutorial/hello_world_3/README.md
+Comment: 
+
+Filename: examples/v2_x/tutorial/hello_world_3/config.yml
+Comment: 
+
+Filename: examples/v2_x/tutorial/hello_world_3/rails.co
+Comment: 
+
+Filename: examples/v2_x/tutorial/interaction_loop/README.md
+Comment: 
+
+Filename: examples/v2_x/tutorial/interaction_loop/config.yml
+Comment: 
+
+Filename: examples/v2_x/tutorial/interaction_loop/main.co
+Comment: 
+
+Filename: examples/v2_x/tutorial/multi_modal/README.md
+Comment: 
+
+Filename: examples/v2_x/tutorial/multi_modal/config.yml
+Comment: 
+
+Filename: examples/v2_x/tutorial/multi_modal/main.co
+Comment: 
+
 Filename: nemoguardrails/__init__.py
 Comment: 
 
 Filename: nemoguardrails/__main__.py
 Comment: 
 
 Filename: nemoguardrails/context.py
@@ -486,17 +807,14 @@
 
 Filename: nemoguardrails/cli/__init__.py
 Comment: 
 
 Filename: nemoguardrails/cli/chat.py
 Comment: 
 
-Filename: nemoguardrails/cli/simplify_formatter.py
-Comment: 
-
 Filename: nemoguardrails/colang/__init__.py
 Comment: 
 
 Filename: nemoguardrails/colang/runtime.py
 Comment: 
 
 Filename: nemoguardrails/colang/v1_0/__init__.py
@@ -570,29 +888,41 @@
 
 Filename: nemoguardrails/colang/v2_x/library/avatars.co
 Comment: 
 
 Filename: nemoguardrails/colang/v2_x/library/core.co
 Comment: 
 
+Filename: nemoguardrails/colang/v2_x/library/guardrails.co
+Comment: 
+
+Filename: nemoguardrails/colang/v2_x/library/llm.co
+Comment: 
+
 Filename: nemoguardrails/colang/v2_x/library/utils.co
 Comment: 
 
 Filename: nemoguardrails/colang/v2_x/runtime/__init__.py
 Comment: 
 
+Filename: nemoguardrails/colang/v2_x/runtime/errors.py
+Comment: 
+
 Filename: nemoguardrails/colang/v2_x/runtime/eval.py
 Comment: 
 
 Filename: nemoguardrails/colang/v2_x/runtime/flows.py
 Comment: 
 
 Filename: nemoguardrails/colang/v2_x/runtime/runtime.py
 Comment: 
 
+Filename: nemoguardrails/colang/v2_x/runtime/serialization.py
+Comment: 
+
 Filename: nemoguardrails/colang/v2_x/runtime/statemachine.py
 Comment: 
 
 Filename: nemoguardrails/colang/v2_x/runtime/system_functions.py
 Comment: 
 
 Filename: nemoguardrails/colang/v2_x/runtime/utils.py
@@ -642,14 +972,17 @@
 
 Filename: nemoguardrails/eval/cli/__init__.py
 Comment: 
 
 Filename: nemoguardrails/eval/cli/evaluate.py
 Comment: 
 
+Filename: nemoguardrails/eval/cli/simplify_formatter.py
+Comment: 
+
 Filename: nemoguardrails/eval/data/factchecking/README.md
 Comment: 
 
 Filename: nemoguardrails/eval/data/factchecking/__init__.py
 Comment: 
 
 Filename: nemoguardrails/eval/data/factchecking/process_msmarco_data.py
@@ -927,14 +1260,17 @@
 
 Filename: nemoguardrails/logging/explain.py
 Comment: 
 
 Filename: nemoguardrails/logging/processing_log.py
 Comment: 
 
+Filename: nemoguardrails/logging/simplify_formatter.py
+Comment: 
+
 Filename: nemoguardrails/logging/stats.py
 Comment: 
 
 Filename: nemoguardrails/logging/verbose.py
 Comment: 
 
 Filename: nemoguardrails/rails/__init__.py
@@ -945,14 +1281,17 @@
 
 Filename: nemoguardrails/rails/llm/config.py
 Comment: 
 
 Filename: nemoguardrails/rails/llm/default_config.yml
 Comment: 
 
+Filename: nemoguardrails/rails/llm/default_config_v2.yml
+Comment: 
+
 Filename: nemoguardrails/rails/llm/llm_flows.co
 Comment: 
 
 Filename: nemoguardrails/rails/llm/llmrails.py
 Comment: 
 
 Filename: nemoguardrails/rails/llm/options.py
@@ -975,29 +1314,29 @@
 
 Filename: nemoguardrails/server/datastore/memory_store.py
 Comment: 
 
 Filename: nemoguardrails/server/datastore/redis_store.py
 Comment: 
 
-Filename: nemoguardrails-0.8.1.dist-info/LICENSE-Apache-2.0.txt
+Filename: nemoguardrails-0.8.2.dist-info/LICENSE-Apache-2.0.txt
 Comment: 
 
-Filename: nemoguardrails-0.8.1.dist-info/LICENSE.md
+Filename: nemoguardrails-0.8.2.dist-info/LICENSE.md
 Comment: 
 
-Filename: nemoguardrails-0.8.1.dist-info/METADATA
+Filename: nemoguardrails-0.8.2.dist-info/METADATA
 Comment: 
 
-Filename: nemoguardrails-0.8.1.dist-info/WHEEL
+Filename: nemoguardrails-0.8.2.dist-info/WHEEL
 Comment: 
 
-Filename: nemoguardrails-0.8.1.dist-info/entry_points.txt
+Filename: nemoguardrails-0.8.2.dist-info/entry_points.txt
 Comment: 
 
-Filename: nemoguardrails-0.8.1.dist-info/top_level.txt
+Filename: nemoguardrails-0.8.2.dist-info/top_level.txt
 Comment: 
 
-Filename: nemoguardrails-0.8.1.dist-info/RECORD
+Filename: nemoguardrails-0.8.2.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## nemoguardrails/__init__.py

```diff
@@ -22,8 +22,8 @@
 patch_asyncio.apply()
 
 # Ignore a warning message from torch.
 warnings.filterwarnings(
     "ignore", category=UserWarning, message="TypedStorage is deprecated"
 )
 
-__version__ = "0.8.1"
+__version__ = "0.8.2"
```

## nemoguardrails/utils.py

```diff
@@ -18,14 +18,18 @@
 import uuid
 from collections import namedtuple
 from datetime import datetime, timezone
 from enum import Enum
 from typing import Any, Dict, List, Tuple, Union
 
 import yaml
+from rich.console import Console
+
+# Global console object to be used throughout the code base.
+console = Console()
 
 
 def new_uid() -> str:
     """Helper to create a new UID."""
 
     return str(uuid.uuid4())
```

## nemoguardrails/actions/llm/generation.py

```diff
@@ -157,15 +157,19 @@
                 if isinstance(spec, dict) and spec.get("_type") == "spec_or":
                     specs = spec.get("elements")
                 else:
                     assert isinstance(spec, Spec)
                     specs = [spec]
 
                 for spec in specs:
-                    if not spec.name.startswith("user "):
+                    if (
+                        not spec.name.startswith("user ")
+                        or not spec.arguments
+                        or not spec.arguments["$0"]
+                    ):
                         continue
 
                     message = spec.arguments["$0"][1:-1]
                     if flow.name not in self.user_messages:
                         self.user_messages[flow.name] = []
 
                     self.user_messages[flow.name].append(message)
```

## nemoguardrails/actions/llm/utils.py

```diff
@@ -169,16 +169,27 @@
     elif colang_version == "2.x":
         new_history: List[str] = []
 
         # Structure the user/bot intent/action events
         action_group: List[InternalEvent] = []
         current_intent: Optional[str] = None
 
+        previous_event = None
         for event in events:
             if not isinstance(event, InternalEvent):
+                # Skip non-internal events
+                continue
+
+            if (
+                event.name == InternalEvents.USER_ACTION_LOG
+                and previous_event
+                and events_to_dialog_history([previous_event])
+                == events_to_dialog_history([event])
+            ):
+                # Remove duplicated user action log events that stem from the same user event as the previous event
                 continue
 
             if (
                 event.name == InternalEvents.BOT_ACTION_LOG
                 or event.name == InternalEvents.USER_ACTION_LOG
             ):
                 if len(action_group) > 0 and (
@@ -187,14 +198,16 @@
                 ):
                     new_history.append(events_to_dialog_history(action_group))
                     new_history.append("")
                     action_group.clear()
 
                 action_group.append(event)
                 current_intent = event.arguments["intent_flow_id"]
+
+                previous_event = event
             elif (
                 event.name == InternalEvents.BOT_INTENT_LOG
                 or event.name == InternalEvents.USER_INTENT_LOG
             ):
                 if event.arguments["flow_id"] == current_intent:
                     # Found parent of current group
                     if event.name == InternalEvents.BOT_INTENT_LOG:
@@ -211,14 +224,16 @@
                         new_history.append("")
                     new_history.append(events_to_dialog_history([event]))
                     new_history.append("")
                 # Start a new group
                 action_group.clear()
                 current_intent = None
 
+                previous_event = event
+
         if action_group:
             new_history.append(events_to_dialog_history(action_group))
 
         history = "\n".join(new_history).rstrip("\n")
 
     return history
```

## nemoguardrails/actions/v2_x/generation.py

```diff
@@ -29,19 +29,16 @@
     get_initial_actions,
     get_last_user_utterance_event_v2_x,
     llm_call,
     remove_action_intent_identifiers,
 )
 from nemoguardrails.colang.v2_x.lang.colang_ast import Flow
 from nemoguardrails.colang.v2_x.lang.utils import new_uuid
-from nemoguardrails.colang.v2_x.runtime.flows import (
-    ActionEvent,
-    InternalEvent,
-    LlmResponseError,
-)
+from nemoguardrails.colang.v2_x.runtime.errors import LlmResponseError
+from nemoguardrails.colang.v2_x.runtime.flows import ActionEvent, InternalEvent
 from nemoguardrails.colang.v2_x.runtime.statemachine import (
     Event,
     InternalEvents,
     State,
     find_all_active_event_matchers,
     get_element_from_head,
     get_event_from_element,
@@ -113,23 +110,17 @@
         instruction_flows = []
 
         for flow in self.config.flows:
             colang_flow = flow.get("source_code")
             if colang_flow:
                 assert isinstance(flow, Flow)
                 # Check if we need to exclude this flow.
-                # TODO: deprecate the use of the "# meta: " comment in favor of
-                #   @meta(llm_exclude=True)
-                if "# meta: exclude from llm" in colang_flow or (
-                    "exclude_from_llm" not in flow.file_info
-                    or flow.file_info["exclude_from_llm"]
-                    or (
-                        "meta" in flow.decorators
-                        and flow.decorators["meta"].parameters.get("llm_exclude")
-                    )
+                if flow.file_info.get("exclude_from_llm") or (
+                    "meta" in flow.decorators
+                    and flow.decorators["meta"].parameters.get("llm_exclude")
                 ):
                     continue
 
                 all_flows.append(colang_flow)
 
                 # If the first line is a comment, we consider it to be an instruction
                 lines = colang_flow.split("\n")
@@ -139,14 +130,18 @@
                         instruction_flows.append(colang_flow)
 
         self.flows_index = await self._init_colang_flows_index(all_flows)
         self.instruction_flows_index = await self._init_colang_flows_index(
             instruction_flows
         )
 
+        # If we don't have an instruction_flows_index, we fall back to using the main one
+        if self.instruction_flows_index is None:
+            self.instruction_flows_index = self.flows_index
+
     @action(name="GetLastUserMessageAction", is_system_action=True)
     async def get_last_user_message(
         self, events: List[dict], llm: Optional[BaseLLM] = None
     ) -> str:
         event = get_last_user_utterance_event_v2_x(events)
         assert event and event["type"] == "UtteranceUserActionFinished"
         return event["final_transcript"]
@@ -193,15 +188,15 @@
                 and "flow_id" in event.arguments
             ):
                 flow_id = event.arguments["flow_id"]
                 flow_config = state.flow_configs.get(flow_id, None)
                 if isinstance(flow_id, str) and (
                     flow_config is None
                     or (
-                        "# meta: user intent" in flow_config.source_code
+                        flow_config.has_meta_tag("user_intent")
                         and flow_id not in potential_user_intents
                     )
                 ):
                     examples += f"user intent: {flow_id}\n\n"
                     potential_user_intents.append(flow_id)
         examples = examples.strip("\n")
 
@@ -210,18 +205,21 @@
             events=events,
             context={
                 "examples": examples,
                 "potential_user_intents": ", ".join(potential_user_intents),
                 "user_action": user_action,
             },
         )
+        stop = self.llm_task_manager.get_stop_tokens(
+            Task.GENERATE_USER_INTENT_FROM_USER_ACTION
+        )
 
         # We make this call with temperature 0 to have it as deterministic as possible.
         with llm_params(llm, temperature=self.config.lowest_temperature):
-            result = await llm_call(llm, prompt)
+            result = await llm_call(llm, prompt, stop=stop)
 
         # Parse the output using the associated parser
         result = self.llm_task_manager.parse_task_output(
             Task.GENERATE_USER_INTENT_FROM_USER_ACTION, output=result
         )
 
         user_intent = get_first_nonempty_line(result)
@@ -237,14 +235,19 @@
         return f"{user_intent}" or "user unknown intent"
 
     @action(name="CheckValidFlowExistsAction", is_system_action=True)
     async def check_if_flow_exists(self, state: "State", flow_id: str) -> bool:
         """Return True if a flow with the provided flow_id exists."""
         return flow_id in state.flow_id_states
 
+    @action(name="CheckFlowDefinedAction", is_system_action=True)
+    async def check_if_flow_defined(self, state: "State", flow_id: str) -> bool:
+        """Return True if a flow with the provided flow_id is defined."""
+        return flow_id in state.flow_configs
+
     @action(name="CheckForActiveEventMatchAction", is_system_action=True)
     async def check_for_active_flow_finished_match(
         self, state: "State", event_name: str, **arguments: Any
     ) -> bool:
         """Return True if there is a flow waiting for the provided event name and parameters."""
         event: Event
         if event_name in InternalEvents.ALL:
@@ -452,16 +455,16 @@
 
         lines = remove_action_intent_identifiers(lines)
         lines = get_initial_actions(lines)
 
         return {
             "name": flow_name,
             "parameters": flow_parameters,
-            "body": f"flow {flow_name}\n"
-            + f'  # meta: bot intent = "{intent}"\n'
+            "body": f'@meta(bot_intent="{intent}")\n'
+            + f"flow {flow_name}\n"
             + "\n".join(["  " + l.strip(" ") for l in lines]),
         }
 
     @action(name="GenerateValueAction", is_system_action=True, execute_async=True)
     async def generate_value(
         self,
         instructions: str,
@@ -520,7 +523,71 @@
         # a ";" at the end of the line. We remove that
         if value.endswith(";"):
             value = value[:-1]
 
         log.info("Generated value for $%s: %s", var_name, value)
 
         return literal_eval(value)
+
+    @action(name="GenerateFlowAction", is_system_action=True, execute_async=True)
+    async def generate_flow(
+        self,
+        state: State,
+        events: List[dict],
+        llm: Optional[BaseLLM] = None,
+    ) -> dict:
+        """Generate the body for a flow."""
+        # Use action specific llm if registered else fallback to main llm
+        llm = llm or self.llm
+
+        event = events[-1]
+        assert event["type"] == "StartGenerateFlowAction"
+        action_uid = event["action_uid"]
+
+        # We need to search for the flow that is waiting on this action
+        triggering_flow_id = None
+        for _, flow_state in state.flow_states.items():
+            if action_uid in flow_state.action_uids:
+                triggering_flow_id = flow_state.flow_id
+                break
+
+        assert triggering_flow_id is not None
+
+        flow_config = state.flow_configs[triggering_flow_id]
+        docstrings = re.findall(r'"""(.*?)"""', flow_config.source_code, re.DOTALL)
+        assert len(docstrings) > 0
+
+        render_context = {}
+        render_context.update(state.context)
+        render_context.update(state.flow_id_states[triggering_flow_id][0].context)
+
+        # TODO: add the context of the flow
+        prompt = self.llm_task_manager._render_string(
+            docstrings[0], context=render_context, events=events
+        )
+
+        # We make this call with temperature 0 to have it as deterministic as possible.
+        with llm_params(llm, temperature=self.config.lowest_temperature):
+            result = await llm_call(llm, prompt)
+
+        lines = _remove_leading_empty_lines(result).split("\n")
+
+        if len(lines) == 0 or (len(lines) == 1 and lines[0] == ""):
+            return {
+                "name": "bot inform LLM issue",
+                "body": 'flow bot inform LLM issue\n  bot say "Sorry! There was an issue in the LLM result form GenerateFlowContinuationAction!"',
+            }
+
+        uuid = new_uuid()[0:8]
+
+        flow_name = f"_dynamic_{uuid}"
+        for i in range(len(lines)):
+            if not lines[i].startswith("  "):
+                lines[i] = "  " + lines[i]
+
+        body = "\n".join(lines)
+
+        return {
+            "name": flow_name,
+            "parameters": [],
+            "body": f"""flow {flow_name}\n{body}""",
+        }
```

## nemoguardrails/cli/__init__.py

```diff
@@ -16,21 +16,20 @@
 import logging
 import os
 from typing import List, Optional
 
 import typer
 import uvicorn
 from fastapi import FastAPI
-from rich.logging import RichHandler
 
 from nemoguardrails import __version__
 from nemoguardrails.actions_server import actions_server
 from nemoguardrails.cli.chat import run_chat
-from nemoguardrails.cli.simplify_formatter import SimplifyFormatter
 from nemoguardrails.eval.cli import evaluate
+from nemoguardrails.eval.cli.simplify_formatter import SimplifyFormatter
 from nemoguardrails.logging.verbose import set_verbose
 from nemoguardrails.server import api
 
 app = typer.Typer()
 app.add_typer(evaluate.app, name="evaluate", short_help="Run an evaluation task.")
 app.pretty_exceptions_enable = False
 
@@ -44,17 +43,17 @@
         exists=True,
         help="Path to a directory containing configuration files to use. Can also point to a single configuration file.",
     ),
     verbose: bool = typer.Option(
         default=False,
         help="If the chat should be verbose and output detailed logging information.",
     ),
-    verbose_llm_calls: bool = typer.Option(
+    verbose_no_llm: bool = typer.Option(
         default=False,
-        help="If the chat should be verbose and include the prompts and responses for the LLM calls.",
+        help="If the chat should be verbose and exclude the prompts and responses for the LLM calls.",
     ),
     verbose_simplify: bool = typer.Option(
         default=False,
         help="Simplify further the verbose output.",
     ),
     debug_level: List[str] = typer.Option(
         default=[],
@@ -70,37 +69,37 @@
         "In this case, the --config-id must also be specified.",
     ),
     config_id: Optional[str] = typer.Option(
         default=None, help="The config_id to be used when interacting with the server."
     ),
 ):
     """Start an interactive chat session."""
-    if verbose:
-        set_verbose(True)
-
     if len(config) > 1:
         typer.secho(f"Multiple configurations are not supported.", fg=typer.colors.RED)
         typer.echo("Please provide a single folder.")
         raise typer.Exit(1)
 
-    if len(debug_level) == 1:
-        root_logger = logging.getLogger()
-        rich_handler = RichHandler(markup=True)
-
-        # If needed, simplify further the verbose output
-        if verbose_simplify:
-            rich_handler.setFormatter(SimplifyFormatter())
+    # We enable verbose mode automatically when a debug level is specified.
+    # If the `--verbose-no-llm` mode is used, we activate the verbose mode as well.
+    # This means that the user doesn't have to use both options at the same time.
+    verbose = verbose or verbose_no_llm or len(debug_level) > 0
 
-        root_logger.addHandler(rich_handler)
-        root_logger.setLevel(debug_level[0])
+    if verbose:
+        set_verbose(
+            True,
+            llm_calls=not verbose_no_llm,
+            debug=len(debug_level) > 0,
+            debug_level=debug_level[0] if debug_level else "INFO",
+            simplify=verbose_simplify,
+        )
 
     run_chat(
         config_path=config[0],
         verbose=verbose,
-        verbose_llm_calls=verbose_llm_calls,
+        verbose_llm_calls=not verbose_no_llm,
         streaming=streaming,
         server_url=server_url,
         config_id=config_id,
     )
 
 
 @app.command()
```

## nemoguardrails/cli/chat.py

```diff
@@ -13,26 +13,30 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 import asyncio
 import os
 from typing import Dict, List, Optional
 
 import aiohttp
-from prompt_toolkit import PromptSession
+from prompt_toolkit import HTML, PromptSession
 from prompt_toolkit.patch_stdout import patch_stdout
+from prompt_toolkit.styles import Style
 
 from nemoguardrails import LLMRails, RailsConfig
 from nemoguardrails.colang.v2_x.runtime.eval import eval_expression
 from nemoguardrails.logging import verbose
-from nemoguardrails.logging.verbose import Styles, set_verbose_llm_calls
+from nemoguardrails.logging.verbose import console
 from nemoguardrails.streaming import StreamingHandler
 from nemoguardrails.utils import new_event_dict, new_uid
 
 os.environ["TOKENIZERS_PARALLELISM"] = "false"
 
+enable_input = asyncio.Event()
+enable_input.set()
+
 
 async def _run_chat_v1_0(
     config_path: Optional[str] = None,
     verbose: bool = False,
     streaming: bool = False,
     server_url: Optional[str] = None,
     config_id: Optional[str] = None,
@@ -51,26 +55,26 @@
             "At least one of `config_path` or `server-url` must be provided."
         )
 
     if not server_url:
         rails_config = RailsConfig.from_path(config_path)
         rails_app = LLMRails(rails_config, verbose=verbose)
         if streaming and not rails_config.streaming_supported:
-            print(
+            console.print(
                 f"WARNING: The config `{config_path}` does not support streaming. "
                 "Falling back to normal mode."
             )
             streaming = False
     else:
         rails_app = None
 
     history = []
     # And go into the default listening loop.
     while True:
-        print("")
+        console.print("")
         user_message = input("> ")
 
         history.append({"role": "user", "content": user_message})
 
         if not server_url:
             # If we have streaming from a locally loaded config, we initialize the handler.
             if streaming and not server_url and rails_app.main_llm_supports_streaming:
@@ -80,15 +84,15 @@
 
             bot_message = await rails_app.generate_async(
                 messages=history, streaming_handler=streaming_handler
             )
 
             if not streaming or not rails_app.main_llm_supports_streaming:
                 # We print bot messages in green.
-                print(Styles.GREEN + f"{bot_message['content']}" + Styles.RESET_ALL)
+                console.print("[green]" + f"{bot_message['content']}" + "[/]")
         else:
             data = {
                 "config_id": config_id,
                 "messages": history,
                 "stream": streaming,
             }
             async with aiohttp.ClientSession() as session:
@@ -97,56 +101,45 @@
                     json=data,
                 ) as response:
                     # If the response is streaming, we show each chunk as it comes
                     if response.headers.get("Transfer-Encoding") == "chunked":
                         bot_message_text = ""
                         async for chunk in response.content.iter_any():
                             chunk = chunk.decode("utf-8")
-                            print(Styles.GREEN + f"{chunk}" + Styles.RESET_ALL, end="")
+                            console.print("[green]" + f"{chunk}" + "[/]", end="")
                             bot_message_text += chunk
-                        print("")
+                        console.print("")
 
                         bot_message = {"role": "assistant", "content": bot_message_text}
                     else:
                         result = await response.json()
                         bot_message = result["messages"][0]
 
                         # We print bot messages in green.
-                        print(
-                            Styles.GREEN
-                            + f"{bot_message['content']}"
-                            + Styles.RESET_ALL
-                        )
+                        console.print("[green]" + f"{bot_message['content']}" + "[/]")
 
         history.append(bot_message)
 
 
 async def _run_chat_v2_x(rails_app: LLMRails):
     """Simple chat loop for v2.x using the stateful events API."""
     state = None
     waiting_user_input = False
     running_timer_tasks: Dict[str, asyncio.Task] = {}
     input_events: List[dict] = []
     output_events: List[dict] = []
     output_state = None
 
     session: PromptSession = PromptSession()
+    status = console.status("[bold green]Working ...[/]")
 
     # Start an asynchronous timer
     async def _start_timer(timer_name: str, delay_seconds: float, action_uid: str):
         nonlocal input_events
-        # print(
-        #     Styles.GREY + f"timer (start): {timer_name}/{action_uid}" + Styles.RESET_ALL
-        # )
         await asyncio.sleep(delay_seconds)
-        # print(
-        #     Styles.GREY
-        #     + f"timer (finished): {timer_name}/{action_uid}"
-        #     + Styles.RESET_ALL
-        # )
         input_events.append(
             new_event_dict(
                 "TimerBotActionFinished",
                 action_uid=action_uid,
                 is_success=True,
                 timer_name=timer_name,
             )
@@ -164,22 +157,24 @@
         for event in output_events:
             # Add all output events also to input events
             input_events.append(event)
 
             if event["type"] == "StartUtteranceBotAction":
                 # We print bot messages in green.
                 if not verbose.verbose_mode_enabled:
-                    print(Styles.GREEN + f"\n{event['script']}\n" + Styles.RESET_ALL)
+                    console.print(f"\n[green]{event['script']}[/]\n")
                 else:
-                    print(
-                        Styles.BLACK
-                        + Styles.GREEN_BACKGROUND
-                        + f"bot utterance: {event['script']}"
-                        + Styles.RESET_ALL
-                    )
+                    if not verbose.debug_mode_enabled:
+                        console.print(f"\n[#f0f0f0 on #008800]{event['script']}[/]\n")
+                    else:
+                        console.print(
+                            "[black on #008800]"
+                            + f"bot utterance: {event['script']}"
+                            + "[/]"
+                        )
 
                 input_events.append(
                     new_event_dict(
                         "UtteranceBotActionStarted",
                         action_uid=event["action_uid"],
                     )
                 )
@@ -190,26 +185,20 @@
                         is_success=True,
                         final_script=event["script"],
                     )
                 )
             elif event["type"] == "StartGestureBotAction":
                 # We print gesture messages in green.
                 if not verbose.verbose_mode_enabled:
-                    print(
-                        Styles.BLACK
-                        + Styles.BLUE_BACKGROUND
-                        + f"Gesture: {event['gesture']}"
-                        + Styles.RESET_ALL
+                    console.print(
+                        "[black on blue]" + f"Gesture: {event['gesture']}" + "[/]"
                     )
                 else:
-                    print(
-                        Styles.BLACK
-                        + Styles.BLUE_BACKGROUND
-                        + f"bot gesture: {event['gesture']}"
-                        + Styles.RESET_ALL
+                    console.print(
+                        "[black on blue]" + f"bot gesture: {event['gesture']}" + "[/]"
                     )
 
                 input_events.append(
                     new_event_dict(
                         "GestureBotActionStarted",
                         action_uid=event["action_uid"],
                     )
@@ -221,224 +210,189 @@
                         is_success=True,
                     )
                 )
 
             elif event["type"] == "StartPostureBotAction":
                 # We print posture messages in green.
                 if not verbose.verbose_mode_enabled:
-                    print(
-                        Styles.BLACK
-                        + Styles.BLUE_BACKGROUND
-                        + f"Posture: {event['posture']}."
-                        + Styles.RESET_ALL
+                    console.print(
+                        "[black on blue]" + f"Posture: {event['posture']}." + "[/]"
                     )
                 else:
-                    print(
-                        Styles.BLACK
-                        + Styles.BLUE_BACKGROUND
+                    console.print(
+                        "[black on blue]"
                         + f"bot posture (start): (posture={event['posture']}, action_uid={event['action_uid']}))"
-                        + Styles.RESET_ALL
+                        + "[/]"
                     )
                 input_events.append(
                     new_event_dict(
                         "PostureBotActionStarted",
                         action_uid=event["action_uid"],
                     )
                 )
 
             elif event["type"] == "StopPostureBotAction":
                 if verbose.verbose_mode_enabled:
-                    print(
-                        Styles.BLACK
-                        + Styles.BLUE_BACKGROUND
+                    console.print(
+                        "[black on blue]"
                         + f"bot posture (stop): (action_uid={event['action_uid']})"
-                        + Styles.RESET_ALL
+                        + "[/]"
                     )
 
                 input_events.append(
                     new_event_dict(
                         "PostureBotActionFinished",
                         action_uid=event["action_uid"],
                         is_success=True,
                     )
                 )
 
             elif event["type"] == "StartVisualInformationSceneAction":
                 # We print scene messages in green.
                 if not verbose.verbose_mode_enabled:
                     options = extract_scene_text_content(event["content"])
-                    print(
-                        Styles.BLACK
-                        + Styles.MAGENTA_BACKGROUND
+                    console.print(
+                        "[black on magenta]"
                         + f"Scene information: {event['title']}{options}"
-                        + Styles.RESET_ALL
+                        + "[/]"
                     )
                 else:
-                    print(
-                        Styles.BLACK
-                        + Styles.MAGENTA_BACKGROUND
+                    console.print(
+                        "[black on magenta]"
                         + f"scene information (start): (title={event['title']}, action_uid={event['action_uid']}, content={event['content']})"
-                        + Styles.RESET_ALL
+                        + "[/]"
                     )
 
                 input_events.append(
                     new_event_dict(
                         "VisualInformationSceneActionStarted",
                         action_uid=event["action_uid"],
                     )
                 )
 
             elif event["type"] == "StopVisualInformationSceneAction":
                 if verbose.verbose_mode_enabled:
-                    print(
-                        Styles.BLACK
-                        + Styles.MAGENTA_BACKGROUND
+                    console.print(
+                        "[black on magenta]"
                         + f"scene information (stop): (action_uid={event['action_uid']})"
-                        + Styles.RESET_ALL
+                        + "[/]"
                     )
 
                 input_events.append(
                     new_event_dict(
                         "VisualInformationSceneActionFinished",
                         action_uid=event["action_uid"],
                         is_success=True,
                     )
                 )
 
             elif event["type"] == "StartVisualFormSceneAction":
                 # We print scene messages in green.
                 if not verbose.verbose_mode_enabled:
-                    print(
-                        Styles.BLACK
-                        + Styles.MAGENTA_BACKGROUND
-                        + f"Scene form: {event['prompt']}"
-                        + Styles.RESET_ALL
+                    console.print(
+                        "[black on magenta]" + f"Scene form: {event['prompt']}" + "[/]"
                     )
                 else:
-                    print(
-                        Styles.BLACK
-                        + Styles.MAGENTA_BACKGROUND
+                    console.print(
+                        "[black on magenta]"
                         + f"scene form (start): (prompt={event['prompt']}, action_uid={event['action_uid']}, inputs={event['inputs']})"
-                        + Styles.RESET_ALL
+                        + "[/]"
                     )
                 input_events.append(
                     new_event_dict(
                         "VisualFormSceneActionStarted",
                         action_uid=event["action_uid"],
                     )
                 )
 
             elif event["type"] == "StopVisualFormSceneAction":
                 if verbose.verbose_mode_enabled:
-                    print(
-                        Styles.BLACK
-                        + Styles.MAGENTA_BACKGROUND
+                    console.print(
+                        "[black on magenta]"
                         + f"scene form (stop): (action_uid={event['action_uid']})"
-                        + Styles.RESET_ALL
+                        + "[/]"
                     )
                 input_events.append(
                     new_event_dict(
                         "VisualFormSceneActionFinished",
                         action_uid=event["action_uid"],
                         is_success=True,
                     )
                 )
 
             elif event["type"] == "StartVisualChoiceSceneAction":
                 # We print scene messages in green.
                 if not verbose.verbose_mode_enabled:
                     options = extract_scene_text_content(event["options"])
-                    print(
-                        Styles.BLACK
-                        + Styles.MAGENTA_BACKGROUND
+                    console.print(
+                        "[black on magenta]"
                         + f"Scene choice: {event['prompt']}{options}"
-                        + Styles.RESET_ALL
+                        + "[/]"
                     )
                 else:
-                    print(
-                        Styles.BLACK
-                        + Styles.MAGENTA_BACKGROUND
+                    console.print(
+                        "[black on magenta]"
                         + f"scene choice (start): (prompt={event['prompt']}, action_uid={event['action_uid']}, options={event['options']})"
-                        + Styles.RESET_ALL
+                        + "[/]"
                     )
                 input_events.append(
                     new_event_dict(
                         "VisualChoiceSceneActionStarted",
                         action_uid=event["action_uid"],
                     )
                 )
 
             elif event["type"] == "StopVisualChoiceSceneAction":
                 if verbose.verbose_mode_enabled:
-                    print(
-                        Styles.BLACK
-                        + Styles.MAGENTA_BACKGROUND
+                    console.print(
+                        "[black on magenta]"
                         + f"scene choice (stop): (action_uid={event['action_uid']})"
-                        + Styles.RESET_ALL
+                        + "[/]"
                     )
                 input_events.append(
                     new_event_dict(
                         "VisualChoiceSceneActionFinished",
                         action_uid=event["action_uid"],
                         is_success=True,
                     )
                 )
 
             elif event["type"] == "StartTimerBotAction":
-                if verbose.verbose_mode_enabled:
-                    print(
-                        Styles.BLACK
-                        + Styles.GREY
-                        + f"timer (start): {event['timer_name']} {event['duration']}"
-                        + Styles.RESET_ALL
-                    )
                 action_uid = event["action_uid"]
                 timer = _start_timer(event["timer_name"], event["duration"], action_uid)
                 # Manage timer tasks
                 if action_uid not in running_timer_tasks:
                     task = asyncio.create_task(timer)
                     running_timer_tasks.update({action_uid: task})
                 input_events.append(
                     new_event_dict(
                         "TimerBotActionStarted",
                         action_uid=event["action_uid"],
                     )
                 )
 
             elif event["type"] == "StopTimerBotAction":
-                if verbose.verbose_mode_enabled:
-                    print(
-                        Styles.GREY
-                        + f"timer (stop): {event['action_uid']}"
-                        + Styles.RESET_ALL
-                    )
                 action_uid = event["action_uid"]
                 if action_uid in running_timer_tasks:
                     running_timer_tasks[action_uid].cancel()
                     running_timer_tasks.pop(action_uid)
 
             elif event["type"] == "TimerBotActionFinished":
-                if verbose.verbose_mode_enabled:
-                    print(
-                        Styles.GREY
-                        + f"timer (finished): {event['action_uid']}"
-                        + Styles.RESET_ALL
-                    )
                 action_uid = event["action_uid"]
                 if action_uid in running_timer_tasks:
                     running_timer_tasks[action_uid].cancel()
                     running_timer_tasks.pop(action_uid)
             elif event["type"] == "LocalAsyncCounter":
-                if verbose.verbose_mode_enabled:
-                    print(Styles.GREY + f"Event: {event}" + Styles.RESET_ALL)
+                # if verbose.verbose_mode_enabled:
+                #     console.print(Styles.GREY + f"Event: {event}" + "[/]")
+                pass
             else:
-                if not verbose.verbose_mode_enabled:
-                    print(f"Event: {event['type']}")
-                else:
-                    print(f"Event: {event['type']}: {event}")
+                if event["type"] not in ["LocalAsyncCounter"]:
+                    if not verbose.verbose_mode_enabled:
+                        console.print(f"Event: {event['type']}")
 
         # TODO: deserialize the output state
         # state = State.from_dict(output_state)
         # Simulate serialization for testing
         # data = pickle.dumps(output_state)
         # output_state = pickle.loads(data)
         state = output_state
@@ -467,14 +421,16 @@
                 len(output_events) == 1
                 and output_events[0]["type"] == "LocalAsyncCounter"
                 and output_events[0]["counter"] == 0
             ):
                 # If there are no pending actions, we stop
                 check_task.cancel()
                 check_task = None
+                status.stop()
+                enable_input.set()
                 return
 
             output_events.clear()
 
             await asyncio.sleep(0.2)
 
     async def _process_input_events():
@@ -498,27 +454,37 @@
     first_time = True
     with patch_stdout(raw=True):
         while True:
             if first_time:
                 input_events = []
             else:
                 waiting_user_input = True
-                user_message: str = await session.prompt_async("> ")
+                await enable_input.wait()
+
+                user_message: str = await session.prompt_async(
+                    HTML("<prompt>\n> </prompt>"),
+                    style=Style.from_dict(
+                        {
+                            "prompt": "fg:#ffff00",
+                            "": "fg:#ffff00",
+                        }
+                    ),
+                )
+                enable_input.clear()
+                status.start()
                 waiting_user_input = False
                 if user_message == "":
                     input_events = [new_event_dict("CheckLocalAsync")]
                 elif user_message.startswith("/"):
                     # Non-UtteranceBotAction actions
                     event_input = user_message.lstrip("/")
                     event = parse_events_inputs(event_input)
                     if event is None:
-                        print(
-                            Styles.RED
-                            + f"Invalid event: {event_input}"
-                            + Styles.RESET_ALL
+                        console.print(
+                            "[white on red]" + f"Invalid event: {event_input}" + "[/]"
                         )
                     else:
                         input_events = [event]
                 else:
                     input_events = [
                         new_event_dict(
                             "UtteranceUserActionFinished",
@@ -615,30 +581,24 @@
         config_path (Optional[str]): The path to the configuration file. Defaults to None.
         verbose (bool): Whether to run in verbose mode. Defaults to False.
         verbose_llm_calls (bool): Whether to print the prompts and the completions. Defaults to False.
         streaming (bool): Whether to enable streaming mode. Defaults to False.
         server_url (Optional[str]): The URL of the chat server. Defaults to None.
         config_id (Optional[str]): The configuration ID. Defaults to None.
     """
-    # If the `--verbose-llm-calls` mode is used, we activate the verbose mode.
-    # This means that the user doesn't have to use both options at the same time.
-    verbose = verbose or verbose_llm_calls
-
     rails_config = RailsConfig.from_path(config_path)
     rails_app = LLMRails(rails_config, verbose=verbose)
 
-    set_verbose_llm_calls(verbose_llm_calls)
-
-    if verbose and not verbose_llm_calls:
-        print(
-            "NOTE: use the `--verbose-llm-calls` option to include the LLM prompts "
-            "and completions in the log.\n"
+    if verbose and verbose_llm_calls:
+        console.print(
+            "NOTE: use the `--verbose-no-llm` option to exclude the LLM prompts "
+            "and completions from the log.\n"
         )
 
-    print("Starting the chat (Press Ctrl + C twice to quit) ...")
+    console.print("Starting the chat (Press Ctrl + C twice to quit) ...")
 
     if rails_config.colang_version == "1.0":
         asyncio.run(
             _run_chat_v1_0(
                 config_path=config_path,
                 verbose=verbose,
                 streaming=streaming,
```

## nemoguardrails/colang/runtime.py

```diff
@@ -73,34 +73,42 @@
         """Registers an additional parameter that can be passed to the actions.
 
         :param name: The name of the parameter.
         :param value: The value of the parameter.
         """
         self.registered_action_params[name] = value
 
-    async def generate_events(self, events: List[dict]) -> List[dict]:
+    async def generate_events(
+        self, events: List[dict], processing_log: Optional[List[dict]] = None
+    ) -> List[dict]:
         """Generates the next events based on the provided history.
 
         This is a wrapper around the `process_events` method, that will keep
         processing the events until the `listen` event is produced.
 
+        Args:
+            events (List[dict]): The list of events.
+            processing_log (Optional[List[dict]]): The processing log so far. This will be mutated.
+
         :return: The list of events.
         """
         raise NotImplementedError()
 
     async def process_events(
-        self, events: List[dict], state: Optional[dict] = None
+        self, events: List[dict], state: Optional[Any] = None, blocking: bool = False
     ) -> Tuple[List[dict], Any]:
         """Process a sequence of events in a given state.
 
         The events will be processed one by one, in the input order.
 
         Args:
             events: A sequence of events that needs to be processed.
             state: The state that should be used as the starting point. If not provided,
               a clean state will be used.
+            blocking: In blocking mode, the event processing will also wait for all
+              local async actions.
 
         Returns:
             (output_events, output_state) Returns a sequence of output events and an output
               state.
         """
         raise NotImplementedError()
```

## nemoguardrails/colang/v2_x/lang/expansion.py

```diff
@@ -20,33 +20,31 @@
 from nemoguardrails.colang.v2_x.lang.colang_ast import (
     Abort,
     Assignment,
     BeginScope,
     Break,
     CatchPatternFailure,
     Continue,
+    Element,
     ElementType,
     EndScope,
     ForkHead,
     Goto,
     If,
     Label,
     MergeHeads,
     Spec,
     SpecOp,
     SpecType,
     WaitForHeads,
     When,
     While,
 )
-from nemoguardrails.colang.v2_x.runtime.flows import (
-    ColangSyntaxError,
-    FlowConfig,
-    InternalEvents,
-)
+from nemoguardrails.colang.v2_x.runtime.errors import ColangSyntaxError
+from nemoguardrails.colang.v2_x.runtime.flows import FlowConfig, InternalEvents
 from nemoguardrails.colang.v2_x.runtime.utils import new_var_uid
 
 
 def expand_elements(
     elements: List[ElementType],
     flow_configs: Dict[str, FlowConfig],
     continue_break_labels: Optional[Tuple[str, str]] = None,
@@ -85,14 +83,21 @@
                 if element.label is None and continue_break_labels is not None:
                     element.label = continue_break_labels[0]
             elif isinstance(element, Break):
                 if element.label is None and continue_break_labels is not None:
                     element.label = continue_break_labels[1]
 
             if len(expanded_elements) > 0:
+                # Map new elements to source
+                for expanded_element in expanded_elements:
+                    if isinstance(expanded_element, Element) and isinstance(
+                        element, Element
+                    ):
+                        expanded_element._source = element._source
+                # Add new elements
                 new_elements.extend(expanded_elements)
                 elements_changed = True
             else:
                 new_elements.extend([element])
 
         elements = new_elements
     return elements
@@ -157,19 +162,27 @@
     element: SpecOp,
 ) -> List[ElementType]:
     new_elements: List[ElementType] = []
     if isinstance(element.spec, Spec):
         # Single element
         if element.spec.spec_type == SpecType.FLOW and element.spec.members is None:
             # It's a flow
-            # send StartFlow(flow_id="FLOW_NAME")
+            # $_instance_<uid> = (<flow_id>)<uid>
+            instance_uid_variable_name = f"_instance_uid_{new_var_uid()}"
+            new_elements.append(
+                Assignment(
+                    key=instance_uid_variable_name,
+                    expression=f"'({element.spec.name}){{uid()}}'",
+                )
+            )
+            # send StartFlow(flow_id=<flow_id>, flow_instance_uid=$_instance_<uid>)
             element.spec.arguments.update(
                 {
                     "flow_id": f"'{element.spec.name}'",
-                    "flow_start_uid": f"'{new_var_uid()}'",
+                    "flow_instance_uid": f"'{{${instance_uid_variable_name}}}'",
                 }
             )
             new_elements.append(
                 SpecOp(
                     op="send",
                     spec=Spec(
                         name=InternalEvents.START_FLOW,
@@ -570,18 +583,26 @@
             if element.spec.ref is not None:
                 raise ColangSyntaxError(
                     "Activate statement cannot have a reference assignment!"
                 )
             element_copy = copy.deepcopy(element)
             # TODO: Remove assert once SpecOp type is refactored
             assert isinstance(element_copy.spec, Spec)
+            # $_instance_<uid> = (<flow_id>)<uid>
+            instance_uid_variable_name = f"_instance_uid_{new_var_uid()}"
+            new_elements.append(
+                Assignment(
+                    key=instance_uid_variable_name,
+                    expression=f"'({element.spec.name}){{uid()}}'",
+                )
+            )
             element_copy.spec.arguments.update(
                 {
                     "flow_id": f"'{element.spec.name}'",
-                    "flow_start_uid": f"'{new_var_uid()}'",
+                    "flow_instance_uid": f"'{{${instance_uid_variable_name}}}'",
                     "activated": "True",
                 }
             )
             new_elements.append(
                 SpecOp(
                     op="start",
                     spec=element_copy.spec,
@@ -607,17 +628,17 @@
 
     return new_elements
 
 
 def _expand_assignment_stmt_element(element: Assignment) -> List[ElementType]:
     new_elements: List[ElementType] = []
 
-    # Check if the expression is an NLD
-    nld_pattern = r"\"\"\"(.*?)\"\"\"|'''(.*?)'''"
-    match = re.search(nld_pattern, element.expression)
+    # Check if the expression is an NLD instruction
+    nld_instruction_pattern = r"^\s*i\"(.*)\"|^\s*i'(.*)'"
+    match = re.search(nld_instruction_pattern, element.expression)
 
     if match:
         # Replace the assignment with the GenerateValueAction system action
         new_elements.append(
             SpecOp(
                 op="await",
                 spec=Spec(
@@ -670,17 +691,19 @@
     if_else_body_label_name = f"if_else_body_label_{new_var_uid()}"
     if_end_label_name = f"if_end_label_{new_var_uid()}"
 
     # TODO: optimize for cases when the else section is missing
     elements.append(
         Goto(
             expression=f"not({element.expression})",
-            label=if_end_label_name
-            if not element.else_elements
-            else if_else_body_label_name,
+            label=(
+                if_end_label_name
+                if not element.else_elements
+                else if_else_body_label_name
+            ),
         )
     )
     elements.extend(expand_elements(element.then_elements, flow_configs))
 
     if element.else_elements:
         elements.append(Goto(label=if_end_label_name))
         elements.append(Label(name=if_else_body_label_name))
@@ -884,17 +907,19 @@
         group = {"_type": "spec_and", "elements": [group]}
 
     if group["_type"] == "spec_or":
         return flatten_or_group(
             {
                 "_type": "spec_or",
                 "elements": [
-                    normalize_element_groups(elem)
-                    if isinstance(elem, dict)
-                    else {"_type": "spec_and", "elements": [elem]}
+                    (
+                        normalize_element_groups(elem)
+                        if isinstance(elem, dict)
+                        else {"_type": "spec_and", "elements": [elem]}
+                    )
                     for elem in group["elements"]
                 ],
             }
         )
     elif group["_type"] == "spec_and":
         results = [{"_type": "spec_and", "elements": []}]
         for elem in group["elements"]:
```

## nemoguardrails/colang/v2_x/lang/parser.py

```diff
@@ -12,14 +12,15 @@
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 import logging
 import os
 import re
+import textwrap
 
 import yaml
 
 from nemoguardrails.colang.v2_x.lang.colang_ast import Flow, Import
 from nemoguardrails.colang.v2_x.lang.grammar.load import load_lark_parser
 from nemoguardrails.colang.v2_x.lang.transformer import ColangTransformer
 from nemoguardrails.utils import CustomDumper
@@ -48,32 +49,59 @@
         Returns:
             An instance of a parsing tree as returned by Lark.
         """
         # NOTE: dealing with EOF is a bit tricky in Lark; the easiest solution
         # to avoid some issues arising from that is to append a new line at the end
         return self._lark_parser.parse(content + "\n")
 
+    @staticmethod
+    def _apply_pre_parsing_expansions(content: str):
+        """Applies a set of expansions even before starting the parsing.
+
+        Currently, only the "..." is expanded.
+        """
+        # We make sure to capture the correct indentation level and use that.
+        content = re.sub(
+            r"\n( +)\.\.\.",
+            textwrap.dedent(
+                r"""
+                \1$flow_info = await GenerateFlowAction()
+                \1await AddFlowsAction(config=$flow_info['body'])
+                \1$instance_uid = uid()
+                \1send StartFlow(flow_id=$flow_info['name'], flow_instance_uid=$instance_uid, context=$self.context)
+                \1match FlowStarted(flow_instance_uid=$instance_uid)
+                \1match FlowFinished(flow_instance_uid=$instance_uid)
+                """
+            ),
+            content,
+        )
+
+        return content
+
     def parse_content(
         self, content: str, print_tokens: bool = False, print_parsing_tree: bool = False
     ) -> dict:
         """Parse the provided content and create element structure."""
         if print_tokens:
-            tokens = list(self._lark_parser.lex(content))
+            tokens = list(
+                self._lark_parser.lex(self._apply_pre_parsing_expansions(content))
+            )
             for token in tokens:
                 print(token.__repr__())
 
-        tree = self.get_parsing_tree(content)
+        tree = self.get_parsing_tree(self._apply_pre_parsing_expansions(content))
 
         if print_parsing_tree:
             print(tree.pretty())
 
         exclude_flows_from_llm = self._contains_exclude_from_llm_tag(content)
 
         transformer = ColangTransformer(
-            source=content, include_source_mapping=self.include_source_mapping
+            source=self._apply_pre_parsing_expansions(content),
+            include_source_mapping=self.include_source_mapping,
         )
         data = transformer.transform(tree)
 
         result: dict = {"flows": []}
 
         # For the special case when we only have one flow in the colang file
         if isinstance(data, Flow):
```

## nemoguardrails/colang/v2_x/lang/transformer.py

```diff
@@ -205,15 +205,15 @@
             parameters=param_defs,
             return_members=return_member_defs,
             _source=self.__source(meta),
             source_code=source if self.include_source_mapping else None,
         )
 
     def _remove_source_code_comments(self, source: str) -> str:
-        pattern = r"#(?! (llm:|meta:))[^\n]*"
+        pattern = r"#[^\n]*"
         return re.sub(pattern, "", source)
 
     def _spec_op(self, children: list, meta: Meta) -> SpecOp:
         """Processing for `spec_op` tree nodes.
 
         Rule:
             spec_op: [spec_operator] spec_expr
@@ -554,17 +554,19 @@
         a new copy of the tree node (i.e. ``return Tree(data, children: list, meta)``)
         """
         if isinstance(data, Token):
             data = data.value
 
         # Transform tokens to dicts
         children = [
-            child
-            if not isinstance(child, Token)
-            else {"_type": child.type, "elements": [child.value]}
+            (
+                child
+                if not isinstance(child, Token)
+                else {"_type": child.type, "elements": [child.value]}
+            )
             for child in children
         ]
 
         method_name = f"_{data}"
         if hasattr(self, method_name):
             return getattr(self, method_name)(children, meta)
         else:
```

## nemoguardrails/colang/v2_x/lang/grammar/colang.lark

```diff
@@ -34,16 +34,16 @@
       | "-=" -> decrement
 
 return_stmt: "return" [test] _NEWLINE
 abort_stmt: "abort" _NEWLINE
 break_stmt: "break" _NEWLINE
 continue_stmt: "continue" _NEWLINE
 pass_stmt: "pass" _NEWLINE
-log_stmt: "log" expr _NEWLINE
-print_stmt: "print" expr _NEWLINE
+log_stmt: "log" (expr | "(" expr ")") _NEWLINE
+print_stmt: "print" (expr | "(" expr ")") _NEWLINE
 priority_stmt: "priority" expr _NEWLINE
 label_stmt: NAME":" _NEWLINE
 global_stmt: "global" var_name _NEWLINE
 import_stmt: "import" (package_name | string) _NEWLINE
 expr_stmt: "(" test ")"
 
 // --------------------------------
@@ -200,15 +200,15 @@
 // High priority keywords
 
 // TODO: Create a positive lookbehind for whitespace characters: e.g. with /(?<=\s)
 _FLOW.1: /(?<!\.)flow(?!(\s*\(|[a-zA-Z0-9_]))/
 
 _IF.1: /if/
 _ELSE_IF.1: /(elif|else\s+if)/
-_OR_WHEN.1: /(orwhen)/
+_OR_WHEN.1: /(or ?when)/
 _AND.1: /(and[ \t]|(\r?\n[\t ]*)+and[ \t])/
 _OR.1: /(or[ \t]|(\r?\n[\t ]*)+or[ \t])/
 _IS.1: /is/
 _IN.1: /in/
 _NOT.1: /not/
 
 // Names
@@ -217,15 +217,15 @@
 VAR_NAME: /\$[^\W\d]\w*/
 
 COMMENT: /#[^\n]*/
 _NEWLINE: (/\r?\n[\t ]*/)+
 
 // Primitive values
 
-STRING: /r?("(?!"").*?(?<!\\)(\\\\)*?"|'(?!'').*?(?<!\\)(\\\\)*?')/i
+STRING: /i?("(?!"").*?(?<!\\)(\\\\)*?"|i?'(?!'').*?(?<!\\)(\\\\)*?')/i
 LONG_STRING: /(""".*?(?<!\\)(\\\\)*?"""|'''.*?(?<!\\)(\\\\)*?''')/is
 
 _SPECIAL_DEC: "0".."9"        ("_"?  "0".."9"                       )*
 DEC_NUMBER:   "1".."9"        ("_"?  "0".."9"                       )*
           |   "0"             ("_"?  "0"                            )* /(?![1-9])/
 _EXP: ("e"|"E") ["+" | "-"] _SPECIAL_DEC
 DECIMAL: "." _SPECIAL_DEC | _SPECIAL_DEC "." _SPECIAL_DEC?
```

## nemoguardrails/colang/v2_x/library/avatars.co

```diff
@@ -1,9 +1,7 @@
-# meta: exclude from llm
-
 ################################################################
 # COLANG 2.0 AVATARS CORE LIBRARY
 # VERSION 0.2.0
 #---------------------------------------------------------------
 # CHANGELOG
 #-------
 # 0.1.0 (2/14/2024)
@@ -26,120 +24,135 @@
 #  - Add missing flow out parameters
 #  - Adapt flow names to tracking/managing/handling/catching of ... convention
 #  - Adapt to parameter renaming:
 #    - UnhandledEvent: error_type -> type
 #    - Internal events: new_flow_start_uid -> new_flow_instance_uid
 #  - Cleanup and improvements of certain log and print statements
 #-------
+# 0.2.1 (3/20/2024)
+#  - Introduce $self variable to access flow attributes like loop_id
+#-------
 ################################################################
 
+# meta: exclude from llm
+
 # -----------------------------------
 # User UMIM event wrapper flows
 # DON'T CHANGE! Currently, hard-wired with LLM prompt generation
 # -----------------------------------
 
+@meta(user_action='user said "{$transcript}"')
 flow user said $text -> $transcript
-  # meta: user action
   match UtteranceUserAction.Finished(final_transcript=$text) as $event
   $transcript = $event.final_transcript
 
 flow user saying $text -> $transcript
-  match UtteranceUserAction.TranscriptUpdated(interim_transcript=r"(?i).*({{$text}})((\s*\w+\s*){0,2})\W*$") as $event
+  match UtteranceUserAction.TranscriptUpdated(interim_transcript=regex("(?i).*({$text})((\s*\w+\s*){0,2})\W*$")) as $event
   $transcript = $event.interim_transcript
 
 flow user started saying something
   match UtteranceUserAction.Started() as $event
 
 flow user typing $text -> $inputs
-  match VisualFormSceneAction.InputUpdated(interim_inputs=[{"value": r".*({{$text}})((\s*\w+\s*){0,2})\W*$"}]) as $event
+  match VisualFormSceneAction.InputUpdated(interim_inputs=[{"value": regex(".*({$text})((\s*\w+\s*){0,2})\W*$")}]) as $event
   $inputs = $event.interim_inputs
 
 flow user said something -> $transcript
   match UtteranceUserAction.Finished() as $event
   send UserActionLog(flow_id="user said", parameter=$event.final_transcript, intent_flow_id="user said something")
   $transcript = $event.final_transcript
 
 flow user said something unexpected -> $transcript
-  match UnhandledEvent(event="UtteranceUserActionFinished", loop_ids={$loop_id}) as $event
+  match UnhandledEvent(event="UtteranceUserActionFinished", loop_ids={$self.loop_id}) as $event
   send UserActionLog(flow_id="user said", parameter=$event.final_transcript, intent_flow_id=None)
   $transcript = $event.final_transcript
 
 flow user saying something -> $transcript
   match UtteranceUserAction.TranscriptUpdated() as $event
   #send UserActionLog(flow_id="user saying", parameter=$event.interim_transcript, intent_flow_id="user saying something")
   $transcript = $event.interim_transcript
 
+@meta(user_action='selected choice "{$choice}"')
 flow user selected choice $choice_id -> $choice
-  # meta: user action
   match VisualChoiceSceneAction.ChoiceUpdated(current_choice=[$choice_id]) as $event
   $choice = $event.current_choice
 
 flow user has selected choice $choice_id
   global $choice_selection_state
   if $choice_selection_state == None or $choice_selection_state != $choice_id
     match VisualChoiceSceneAction.ChoiceUpdated(current_choice=[$choice_id]) as $event
 
 flow unhandled user intent -> $intent
-  match UnhandledEvent(event="FinishFlow", flow_id=r"^user ", loop_ids={$loop_id}) as $event
+  match UnhandledEvent(event="FinishFlow", flow_id=regex("^user "), loop_ids={$self.loop_id}) as $event
   $intent = $event.flow_id
 
+@loop("user_was_silent")
+@meta(user_intent=True)
 flow user was silent $time_s
   """Triggers when user was silent for $time_s seconds."""
-  # meta: loop_id=user_was_silent
-  # meta: user intent
   while True
     start wait $time_s as $timer_ref
     when $timer_ref.Finished()
       break
     orwhen UtteranceUserAction.Started() or UtteranceUserAction.TranscriptUpdated()
       send $timer_ref.Stop()
       match UtteranceUserAction.Finished()
     orwhen UtteranceUserAction.Finished()
       send $timer_ref.Stop()
 
+@loop("user_did_not_respond")
+@meta(user_intent=True)
 flow user didnt respond $time_s
   """Triggers when user was silent for $time_s seconds while bot was silent."""
-  # meta: loop_id=user_did_not_respond
-  # meta: user intent
   while True
     start wait $time_s as $timer_ref
     when $timer_ref.Finished()
       break
     orwhen UtteranceUserAction.Started() or UtteranceUserAction.TranscriptUpdated()
       send $timer_ref.Stop()
       match UtteranceUserAction.Finished()
     orwhen UtteranceBotAction.Started()
       send $timer_ref.Stop()
       match UtteranceBotAction.Finished()
     orwhen UtteranceUserAction.Finished() or UtteranceBotAction.Finished()
       send $timer_ref.Stop()
 
+@meta(user_action=True)
+flow user gestured $gesture -> $final_gesture
+  match GestureUserAction.Finished(gesture=$gesture) as $event
+  $final_gesture = $event.gesture
+
+@meta(user_action=True)
+flow user became present -> $user_id
+  match PresenceUserAction.Finished() as $event
+  $user_id = $event.user_id
+
+@meta(user_intent=True)
 flow user interrupted bot talking $sentence_length=15
   """Triggers when the user talked while bot is speaking."""
-  # meta: user intent
   global $bot_talking_state
   while True
     if $bot_talking_state
       log "Bot is talking..."
     else
       bot started saying something
       log "Bot started talking..."
     while True
       when user saying something as $user_saying_ref
         $transcript = $user_saying_ref.context.event.interim_transcript
-        log "User saying something: {{$transcript}}"
+        log "User saying something: {$transcript}"
       orwhen user said something as $user_said_ref
         $transcript = $user_said_ref.context.transcript
-        log "User said something: {{$transcript}}"
+        log "User said something: {$transcript}"
       orwhen bot said something
         log "Bot finished talking without interruption"
         break
 
-      if "{{len($transcript) > $sentence_length}}" == "True"
-        log "Bot interrupted by user with: {{$transcript}}"
+      if len($transcript) > $sentence_length
+        log "Bot interrupted by user with: {$transcript}"
         return
 
 # ----------------------------------
 # Bot UMIM event wrapper flows
 # DON'T CHANGE! Currently, hard-wired with LLM prompt generation
 # ----------------------------------
 
@@ -190,23 +203,23 @@
 flow bot started posture $posture
   match FlowStarted(flow_id="bot posture", posture=$posture) as $event
 
 flow bot started a posture -> $posture
   match FlowStarted(flow_id="bot posture") as $event
   $posture = $event.posture
 
+@meta(bot_action=True)
 flow bot started an action -> $action
-  # meta: bot action
   match bot started saying something as $action
     or bot started a gesture as $action
     or bot started a posture as $action
 
+@loop("bot_was_silent")
+@meta(bot_intent=True)
 flow bot was silent $time_s
-  # meta: loop_id=bot_was_silent
-  # meta: bot intent
   while True
     start wait $time_s as $timer_ref
     when $timer_ref.Finished()
       break
     orwhen UtteranceBotAction.Started()
       send $timer_ref.Stop()
       match UtteranceBotAction.Finished()
@@ -218,128 +231,128 @@
 # DON'T CHANGE! Currently, hard-wired with LLM prompt generation
 # -----------------------------------
 
 flow _bot_say $text
   """It's an internal helper for higher semantic level flows"""
   await UtteranceBotAction(script=$text) as $action
 
+@meta(bot_action=True)
 flow bot gesture $gesture
-  # meta: bot action
   await GestureBotAction(gesture=$gesture) as $action
 
+@meta(bot_action=True)
 flow bot gesture with delay $gesture $delay
-  # meta: bot action
   wait $delay
   bot gesture $gesture
 
+@meta(bot_action=True)
 flow bot posture $posture
-  # meta: bot action
   await PostureBotAction(posture=$posture) as $action
 
+@meta(bot_action=True)
 flow scene show choice $prompt
-  # meta: bot action
   await VisualChoiceSceneAction(prompt=$prompt,choice_type="selection", allow_multiple_choices=False) as $action
 
+@meta(bot_action=True)
 flow scene show textual information $title $text $header_image
-  # meta: bot action
   await VisualInformationSceneAction(title=$title, support_prompts=[], content=[{"image":$header_image},{"text":$text}]) as $action
 
+@meta(bot_action=True)
 flow scene show short information $info
-  # meta: bot action
   await VisualInformationSceneAction(title=$info, support_prompts=[], content=[]) as $action
 
+@meta(bot_action=True)
 flow scene show form $prompt
-  # meta: bot action
   await VisualInformationSceneAction(prompt=$prompt) as $action
 
 # ----------------------------------
 # Bot action semantic wrapper flows
 # DON'T CHANGE! Currently, hard-wired with LLM prompt generation
 # -----------------------------------
 
+@meta(bot_action=True)
 flow bot say $text
-  # meta: bot action
   await _bot_say $text
 
 flow bot say something like $text
-  $variation = """Return a single string that is a new variation of: {{$text}}"""
+  $variation = i"Return a single string that is a new variation of: {$text}"
   await bot say $variation
 
+@meta(bot_action=True)
 flow bot inform $text
-  # meta: bot action
   await _bot_say $text
 
+@meta(bot_action=True)
 flow bot ask $text
-  # meta: bot action
   await _bot_say $text
 
+@meta(bot_action=True)
 flow bot express $text
-  # meta: bot action
   await _bot_say $text
 
+@meta(bot_action=True)
 flow bot respond $text
-  # meta: bot action
   await _bot_say $text
 
+@meta(bot_action=True)
 flow bot clarify $text
-  # meta: bot action
   await _bot_say $text
 
+@meta(bot_action=True)
 flow bot suggest $text
-  # meta: bot action
   await _bot_say $text
 
 # ----------------------------------
 # State tracking flows
 # DON'T CHANGE! Currently, hard-wired with LLM prompt generation
 # -----------------------------------
 
+@loop("state_tracking")
 flow tracking bot talking state
-  # meta: loop_id=state_tracking
   global $bot_talking_state
   if $user_talking_state == None
     $bot_talking_state = False
   await bot started saying something
   $bot_talking_state = True
   await bot said something
   $bot_talking_state = False
 
+@loop("state_tracking")
 flow tracking user talking state
-  # meta: loop_id=state_tracking
   global $user_talking_state
   if $user_talking_state == None
     $user_talking_state = False
   await user started saying something
   $user_talking_state = True
   await user said something
   $user_talking_state = False
 
+@loop("state_tracking")
 flow tracking unhandled user intent state
-  # meta: loop_id=state_tracking
   global $user_intent_state
   when unhandled user intent as $flow
     $unhandled_user_intent_state = $flow.context.flow_name
-  orwhen FlowFinished(flow_id=r"^user ") as $event
+  orwhen FlowFinished(flow_id=regex("^user ")) as $event
     $unhandled_user_intent_state = None
 
+@loop("state_tracking")
 flow tracking visual choice selection state
-  # meta: loop_id=state_tracking
   global $choice_selection_state
   when VisualChoiceSceneAction.Started()
     $choice_selection_state = None
   orwhen VisualChoiceSceneAction.ChoiceUpdated() as $event
-    if "{{len($event.current_choice) > 0}}" == "True"
+    if len($event.current_choice) > 0
       $choice_selection_state = $event.current_choice[0]
     else
       $choice_selection_state = None
   orwhen VisualChoiceSceneAction.Finished()
     $choice_selection_state = None
 
+@loop("state_tracking")
 flow tracking user utterance state
-  # meta: loop_id=state_tracking
   global $last_user_transcript
   global $last_user_message
 
   match UtteranceUserAction.Finished() as $event
   print "last user utterance = {$event.final_transcript}"
   $last_user_transcript = $event.final_transcript
   $last_user_message = $event.final_transcript
@@ -361,112 +374,116 @@
   send FinishFlow(flow_id="scene show short information")
   send FinishFlow(flow_id="scene show form")
 
 flow wait indefinitely
   """Little helper flow to wait indefinitely."""
   match NeverComingEvent()
 
-flow wait $time_s $timer_id="wait_timer_{{uid()}}"
+@loop("NEW")
+flow wait $time_s $timer_id="wait_timer_{uid()}"
   """Wait the specified number of seconds before continuing."""
-  # meta: loop_id=NEW
   await TimerBotAction(timer_name=$timer_id, duration=$time_s)
 
+@loop("catch_colang_errors")
 flow catching colang error
   """A flow to catching of any runtime Colang errors"""
-  # meta: loop_id=catch_colang_errors
   match ColangError() as $event
-  print "Colang error: {{$event.error_type}} - {{escape($event.error)}}"
+  print "Colang error: {$event.type} - {escape($event.error)}"
   bot say "Excuse me, what did you say?"
 
 flow catching start of undefined flows
   """A flow to catching of the start of undefined flows."""
   priority 0.1
   match UnhandledEvent(event="StartFlow") as $event
-  print "Undefined flow: '{{$event.flow_id}}' {{$event.loop_ids}}"
+  print "Undefined flow: '{$event.flow_id}' {$event.loop_ids}"
   bot say "Excuse me, what did you say?"
   # We need to abort the flow that sent the FlowStart event since it might be waiting for it
   send StopFlow(flow_instance_uid=$event.source_flow_instance_uid)
 
 flow catching unexpected user utterance
   """A flow to catching of unhandled user utterances."""
   priority 0.1
   match UnhandledEvent(event="UtteranceUserActionFinished") as $event
-  bot say "Warning: Unexpected user utterance '{{$event.final_transcript}}'"
+  bot say "Warning: Unexpected user utterance '{$event.final_transcript}'"
 
 flow repeating timer $timer_id $interval_s
   """Repeating timer."""
   while True
     await wait $interval_s $timer_id
     # await TimerBotAction(timer_name=$timer_id, duration=$interval_s)
 
 flow await_flow_by_name $flow_name
-  $new_flow_start_uid = "{{uid()}}"
-  send StartFlow(flow_id=$flow_name, flow_start_uid=$new_flow_start_uid)
-  match FlowStarted(flow_id=$flow_name, flow_start_uid=$new_flow_start_uid) as $event_ref
+  $new_flow_instance_uid = "($flow_name){uid()}"
+  send StartFlow(flow_id=$flow_name, flow_instance_uid=$new_flow_instance_uid)
+  match FlowStarted(flow_id=$flow_name, flow_instance_uid=$new_flow_instance_uid) as $event_ref
   match $event_ref.flow.Finished()
 
+@loop("bot_interruption")
 flow handling bot talking interruption $mode="inform"
   """Handling the bot talking interruption reaction."""
-  # meta: loop_id=bot_interruption
   user interrupted bot talking
   if $mode == "interrupt"
     finish all bot actions
   elif $mode == "inform"
     start VisualInformationSceneAction(title="Please wait with talking!", support_prompts=["You should only talk after the avatar."], content=[])
     wait 3.0
   elif $mode == "ignore"
     log "Bot ignored user interruption"
 
 # ----------------------------------
 # LLM mechanics
 # ----------------------------------
 
-flow polling llm request response $interval
-  # meta: loop_id=llm_response_polling
+@loop("llm_response_polling")
+flow polling llm request response $interval=1.0
   match StartGenerateUserIntentAction() as $event_ref
     or StartGenerateFlowContinuationAction() as $event_ref
     or StartGenerateFlowFromNameAction() as $event_ref
     or StartGenerateValueAction() as $event_ref
     or StartGenerateFlowFromInstructionsAction() as $event_ref
   start repeating timer "llm_response_polling" $interval as $polling_timer
   start bot posture "Thinking, idle" as $posture
   match $event_ref.action.Finished()
   send $polling_timer.Stop()
   send $posture.Stop()
 
 flow generating user intent for unhandled user utterance
   """This is the fallback flow that takes care of unhandled user utterances and will generate a user intent."""
+  activate tracking bot talking state
   global $bot_talking_state
-  match UnhandledEvent(event="UtteranceUserActionFinished", loop_ids={$loop_id}) as $event
+
+  match UnhandledEvent(event="UtteranceUserActionFinished", loop_ids={$self.loop_id}) as $event
   if $bot_talking_state == False
     $transcript = $event.final_transcript
-    log "generating user intent for unhandled user utterance: {{$transcript}}"
-    $action = 'user said "{{$transcript}}"'
+    log "generating user intent for unhandled user utterance: {$transcript}"
+    $action = 'user said "{$transcript}"'
     $intent = await derive user intent from user action $action 20
 
   # We need to log the user action
   send UserActionLog(flow_id="user said", parameter=$event.final_transcript, intent_flow_id=$intent)
   # We need to add the generated user intent to the intent log
   send UserIntentLog(flow_id=$intent, parameter=None)
 
   # Generate the 'user intent' by sending out the FinishFlow event
   send FinishFlow(flow_id=$intent)
 
 flow derive user intent from user action $user_action $max_example_flows -> $intent
+  activate polling llm request response
   $intent = await GenerateUserIntentAction(user_action=$user_action, max_example_flows=$max_example_flows)
   return $intent
 
 flow generate interaction continuation -> $flow_name
+  activate polling llm request response
   # Generate continuation based current interaction history
   $flow_info = await GenerateFlowContinuationAction(temperature=0.1)
 
   $exists = await CheckValidFlowExistsAction(flow_id=$flow_info.name)
   if $exists == False
     $flows = await AddFlowsAction(config=$flow_info.body)
-    if "{{len($flows)}}" == "0"
+    if len($flows) == 0
       print "LLM generated flow parsing failed!"
       bot say "Sorry, what did you say?"
       return None
 
   $flow_name = $flow_info.name
   return $flow_info.name
 
@@ -490,85 +507,85 @@
 flow handling start of undefined flow
   """We want to start an undefined flow."""
 
   match UnhandledEvent(event="StartFlow") as $event
 
   start_new_flow_instance:
 
-  if "{{search('^user ',$event.flow_id)}}" == "True"
+  if search('^user ',$event.flow_id)
 
     # We have an undefined user intent, so we just fake it to be started by this fallback flow
-    send FlowStarted(flow_id=$event.flow_id, flow_start_uid=$event.arguments.flow_start_uid)
+    send FlowStarted(flow_id=$event.flow_id, flow_instance_uid=$event.flow_instance_uid)
     # Once this fallback flow receives the user intent it will finish and therefore also trigger the original matcher
     match FlowFinished(flow_id=$event.flow_id)
 
   else
 
     # We have an undefined bot intent, let's generate a new flow for it
 
     $flow_source = await GenerateFlowFromNameAction(name=$event.flow_id)
 
     await AddFlowsAction(config=$flow_source)
-    $new_flow_start_uid = "{{uid()}}"
-    send StartFlow(flow_id=$event.flow_id, flow_start_uid=$new_flow_start_uid)
-    match FlowStarted(flow_id=$event.flow_id, flow_start_uid=$new_flow_start_uid) as $event_ref
+    $new_flow_instance_uid = "($event.flow_id){uid()}"
+    send StartFlow(flow_id=$event.flow_id, flow_instance_uid=$new_flow_instance_uid)
+    match FlowStarted(flow_id=$event.flow_id, flow_instance_uid=$new_flow_instance_uid) as $event_ref
     match $event_ref.flow.Finished()
     await RemoveFlowsAction(flow_ids=[$event.flow_id])
 
 flow execute llm instruction $instructions
   """This will create a new flow based on the provided instructions and start it."""
   $flow_info = await GenerateFlowFromInstructionsAction(instructions=$instructions)
 
   await AddFlowsAction(config=$flow_info.body)
 
-  $new_flow_start_uid = "{{uid()}}"
-  send StartFlow(flow_id=$flow_info.name, flow_start_uid=$new_flow_start_uid)
-  match FlowStarted(flow_id=$flow_info.name, flow_start_uid=$new_flow_start_uid) as $event_ref
+  $new_flow_instance_uid = "($flow_info.name){uid()}"
+  send StartFlow(flow_id=$flow_info.name, flow_instance_uid=$new_flow_instance_uid)
+  match FlowStarted(flow_id=$flow_info.name, flow_instance_uid=$new_flow_instance_uid) as $event_ref
   match $event_ref.flow.Finished()
   await RemoveFlowsAction(flow_ids=[$flow_info.name])
 
+@meta(user_intent=True)
 flow user requested a task
-  # meta: user intent
   user said "do something"
     or user said "can you do something"
     or user said "please do"
 
 flow custom instructions
   user requested a task
   $instructions = await GetLastUserMessageAction()
   execute llm instruction $instructions
 
 
 # --------------------------------------
 # Posture Management Flows
 # --------------------------------------
+@loop("managing_listening_posture")
 flow managing listening posture
-  # meta: loop_id=managing listening posture
   user started saying something
   start bot posture "listening"
   match UtteranceUserAction.Finished()
 
+@loop("managing_talking_posture")
 flow managing talking posture
-  # meta: loop_id=managing talking posture
   bot started saying something
   start bot posture "talking"
   bot said something
 
+@loop("managing_thinking_posture")
 flow managing thinking posture
-  # meta: loop_id=managing thinking posture
   global $bot_talking_state
 
   match UtteranceUserAction.Finished()
   if $bot_talking_state
-    log "Donot switch to thinking posture because bot is talking"
+    log "Do not switch to thinking posture because bot is talking"
   else
     start bot posture "thinking"
     bot started saying something
 
+@loop("managing_bot_postures")
 flow managing bot postures
-  # meta: loop_id=managing bot postures
   activate track bot talking state
   activate managing listening posture
   activate managing thinking posture
   activate managing talking posture
   start bot posture "attentive"
   wait indefinitely
```

## nemoguardrails/colang/v2_x/library/core.co

```diff
@@ -1,21 +1,28 @@
-flow user said $text -> $transcript
-  # meta: user action
-  match UtteranceUserAction.Finished(final_transcript=$text) as $event
-  $transcript = $event.arguments.final_transcript
+# meta: exclude from llm
 
-flow user said something -> $transcript
-  match UtteranceUserAction.Finished() as $event
-  send UserActionLog(flow_id="user said", parameter=$event.arguments.final_transcript, intent_flow_id="user said something")
-  $transcript = $event.arguments.final_transcript
-
-flow unhandled user intent -> $intent
-  match UnhandledEvent(event="FinishFlow", flow_id=r"^user ", loop_ids={$loop_id}) as $event
-  $intent = $event.arguments.flow_id
+# INTERNAL
+
+flow _user_said $text -> $event
+  """Core flow for when the user says something."""
+  if $text
+    match UtteranceUserAction.Finished(final_transcript=$text) as $event
+  else
+    match UtteranceUserAction.Finished() as $event
 
 flow _bot_say $text
   """It's an internal helper for higher semantic level flows"""
   await UtteranceBotAction(script=$text) as $action
 
+# PUBLIC
+
+flow user said $text -> $transcript
+  _user_said $text as $user_said
+  $transcript = $user_said.event.final_transcript
+  return $transcript
+
+flow user said something -> $transcript
+  _user_said as $user_said
+  $transcript = $user_said.event.final_transcript
+
 flow bot say $text
-  # meta: bot action
   await _bot_say $text
```

## nemoguardrails/colang/v2_x/library/utils.co

```diff
@@ -1,8 +1,17 @@
+# meta: exclude from llm
+
 flow wait indefinitely
   """Little helper flow to wait indefinitely."""
   match NeverComingEvent()
 
-flow wait $time_s $timer_id="wait_timer_{{uid()}}"
+@loop("NEW")
+flow wait $time_s $timer_id = "wait_timer_{uid()}"
   """Wait the specified number of seconds before continuing."""
-  # meta: loop_id=NEW
   await TimerBotAction(timer_name=$timer_id, duration=$time_s)
+
+flow await_flow_by_name $flow_name
+  """Start a flow with the provided name and wait for it to finish."""
+  $new_flow_instance_uid = "($flow_name){uid()}"
+  send StartFlow(flow_id=$flow_name, flow_instance_uid=$new_flow_instance_uid)
+  match FlowStarted(flow_id=$flow_name, flow_instance_uid=$new_flow_instance_uid) as $event_ref
+  match $event_ref.flow.Finished()
```

## nemoguardrails/colang/v2_x/runtime/eval.py

```diff
@@ -9,153 +9,193 @@
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
+import json
 import logging
 import re
 from functools import partial
-from typing import Any, List
+from typing import Any, Callable, Dict, List, Optional, Set
 
 from simpleeval import EvalWithCompoundTypes
 
+from nemoguardrails.colang.v2_x.lang.colang_ast import Element
 from nemoguardrails.colang.v2_x.runtime import system_functions
-from nemoguardrails.colang.v2_x.runtime.flows import ColangValueError
+from nemoguardrails.colang.v2_x.runtime.errors import ColangValueError
+from nemoguardrails.colang.v2_x.runtime.flows import FlowState, State
 from nemoguardrails.colang.v2_x.runtime.utils import AttributeDict
+from nemoguardrails.eval.cli.simplify_formatter import SimplifyFormatter
 from nemoguardrails.utils import new_uid
 
 log = logging.getLogger(__name__)
 
 
+class ComparisonExpression:
+    """An expression to compare to values."""
+
+    def __init__(self, operator: Callable[[Any], bool], value: Any) -> None:
+        if not isinstance(value, (int, float)):
+            raise ColangValueError(
+                f"Comparison operators don't support values of type '{type(value)}'"
+            )
+        self.value = value
+        self.operator = operator
+
+    def compare(self, value: Any) -> bool:
+        """Compare given value with the expression's value."""
+        if not isinstance(value, type(self.value)):
+            raise ColangValueError(
+                "Comparing variables of different types is not supported!"
+            )
+
+        return self.operator(value)
+
+
 def eval_expression(expr: str, context: dict) -> Any:
     """Evaluates the provided expression in the given."""
     # If it's not a string, we should return it as such
     if expr is None:
         return None
 
     if not isinstance(expr, str):
         assert isinstance(expr, bool) or isinstance(expr, int)
 
         return expr
 
-    # We search for all inner expressions marked by double curly brackets and evaluate them first
-    inner_expression_pattern = r"\{\{(.*?)\}\}"
-    inner_expressions = re.findall(inner_expression_pattern, expr)
-    if inner_expressions:
-        inner_expression_values = []
-        for inner_expression in inner_expressions:
-            try:
-                value = eval_expression(inner_expression, context)
-            except Exception as ex:
-                raise ColangValueError(
-                    f"Error evaluating inner expression: '{expr}'"
-                ) from ex
-            value = str(value).replace('"', '\\"').replace("'", "\\'")
-            inner_expression_values.append(value)
+    # We search for all expressions in strings within curly brackets and evaluate them first
+    # Find first all strings
+    string_pattern = r'("(?:\\"|[^"])*?")|(\'(?:\\\'|[^\'])*?\')'
+    string_expressions_matches = re.findall(string_pattern, expr)
+    string_expression_values = []
+    for string_expression_match in string_expressions_matches:
+        string_expression = (
+            string_expression_match[0]
+            if string_expression_match[0]
+            else string_expression_match[1]
+        )
+        if string_expression:
+            # Find expressions within curly brackets, ignoring double curly brackets
+            expression_pattern = r"{(?!\{)([^{}]+)\}(?!\})"
+            inner_expressions = re.findall(expression_pattern, string_expression)
+            if inner_expressions:
+                inner_expression_values = []
+                for inner_expression in inner_expressions:
+                    try:
+                        value = eval_expression(inner_expression, context)
+                    except Exception as ex:
+                        raise ColangValueError(
+                            f"Error evaluating inner expression: '{inner_expression}'"
+                        ) from ex
+                    value = str(value).replace('"', '\\"').replace("'", "\\'")
+                    inner_expression_values.append(value)
+                string_expression = re.sub(
+                    expression_pattern,
+                    lambda x: inner_expression_values.pop(0),
+                    string_expression,
+                )
+                string_expression = string_expression.replace("{{", "{").replace(
+                    "}}", "}"
+                )
+            string_expression_values.append(string_expression)
+    if string_expression_values:
         expr = re.sub(
-            inner_expression_pattern,
-            lambda x: inner_expression_values.pop(0),
+            string_pattern,
+            lambda x: string_expression_values.pop(0),
             expr,
         )
 
-    index_counter = 0
-
-    def replace_with_index(name, _match):
-        nonlocal index_counter
-        if _match.group(1) or _match.group(3):
-            replacement = f"{name}_{index_counter}"
-            index_counter += 1
-            return replacement
-        else:
-            return _match.group(0)
-
-    # If the expression contains the pattern r"(.*?)" it is considered a regular expression
-    expr_locals = {}
-    # This pattern first tries to match escaped quotes, then it matches any string enclosed by quotes
-    # finally it tries to match (and capture in a group) the r"" strings. At this point we know that we
-    # are not within a quote. We are doing this for both quoting styles ' and " separately
-    # Regular expressions are found if a match contains either non-empty group 1 or group 3
-    regex_pattern = (
-        r'\\"|"(?:\\"|[^"])*"|(r\"(.*?)\")|\\\'|\'(?:\\\'|[^\'])*\'|(r\'(.*?)\')'
-    )
-    regular_expressions = [
-        exp for exp in re.findall(regex_pattern, expr) if exp[1] or exp[3]
-    ]
-    updated_expr = re.sub(regex_pattern, partial(replace_with_index, "regex"), expr)
-
-    for idx, regular_expression in enumerate(regular_expressions):
-        try:
-            regex = (
-                regular_expression[1]
-                if regular_expression[1] != ""
-                else regular_expression[3]
-            )
-            compiled_regex = re.compile(regex)
-            expr_locals[f"regex_{idx}"] = compiled_regex
-        except Exception as ex:
-            raise ColangValueError(
-                f"Error in compiling regular expression '{expr}'"
-            ) from ex
-
     # We search for all variable names starting with $, remove the $ and add
     # the value in the dict for eval
+    expr_locals = {}
     regex_pattern = r"\$([a-zA-Z_][a-zA-Z0-9_]*)"
-    var_names = re.findall(regex_pattern, updated_expr)
-    updated_expr = re.sub(regex_pattern, r"var_\1", updated_expr)
+    var_names = re.findall(regex_pattern, expr)
+    updated_expr = re.sub(regex_pattern, r"var_\1", expr)
 
     for var_name in var_names:
         # if we've already computed the value, we skip
         if f"var_{var_name}" in expr_locals:
             continue
 
-        val = context.get(var_name, None)
+        # Check if it is a global variable
+        global_var_name = f"_global_{var_name}"
+        if global_var_name in context:
+            val = context.get(global_var_name, None)
+        else:
+            val = context.get(var_name, None)
 
         # We transform dicts to AttributeDict so we can access their keys as attributes
         # e.g. write things like $speaker.name
         if isinstance(val, dict):
             val = AttributeDict(val)
 
         expr_locals[f"var_{var_name}"] = val
 
     # Finally, just evaluate the expression
     try:
         # TODO: replace this with something even more restrictive.
+        functions: Dict[str, Callable] = {
+            "len": len,
+            "flow": system_functions.flow,  # TODO: Consider this to remove
+            "action": system_functions.action,  # TODO: Consider this to remove
+            "regex": _create_regex,
+            "search": _regex_search,
+            "find_all": _regex_findall,
+            "uid": new_uid,
+            "str": _to_str,
+            "pretty_str": _pretty_str,
+            "escape": _escape_string,
+            "is_int": _is_int,
+            "is_float": _is_float,
+            "is_bool": _is_bool,
+            "is_str": _is_str,
+            "is_regex": _is_regex,
+            "less_than": _less_than_operator,
+            "equal_less_than": _equal_or_less_than_operator,
+            "greater_than": _greater_than_operator,
+            "equal_greater_than": _equal_or_greater_than_operator,
+            "not_equal_to": _not_equal_to_operator,
+        }
+        if "_state" in context:
+            functions.update({"flows_info": partial(_flows_info, context["_state"])})
+
+        # TODO: replace this with something even more restrictive.
         s = EvalWithCompoundTypes(
-            functions={
-                "len": len,
-                "flow": system_functions.flow,
-                "action": system_functions.action,
-                "search": _regex_search,
-                "findall": _regex_findall,
-                "uid": new_uid,
-                "str": _to_str,
-                "escape": _escape_string,
-                "is_int": _is_int,
-                "is_float": _is_float,
-                "is_bool": _is_bool,
-                "is_str": _is_str,
-            },
+            functions=functions,
             names=expr_locals,
         )
         return s.eval(updated_expr)
-    except Exception as ex:
-        raise ColangValueError(f"Error evaluating '{expr}': {str(ex)}")
+    except Exception as e:
+        raise ColangValueError(f"Error evaluating '{expr}'") from e
+
+
+def _create_regex(pattern: str) -> re.Pattern:
+    return re.compile(pattern)
 
 
 def _regex_search(pattern: str, string: str) -> bool:
     return bool(re.search(pattern, string))
 
 
 def _regex_findall(pattern: str, string: str) -> List[str]:
     return re.findall(pattern, string)
 
 
 def _to_str(data: Any) -> str:
+    if isinstance(data, (dict, list, set)):
+        return json.dumps(data, indent=4)
+    return str(data)
+
+
+def _pretty_str(data: Any) -> str:
+    if isinstance(data, (dict, list, set)):
+        string = json.dumps(data, indent=4)
+        return SimplifyFormatter().format(string)
     return str(data)
 
 
 def _escape_string(string: str) -> str:
     """Escape a string and inner expressions."""
     return string.replace("\\", "\\\\").replace("{{", "\\{").replace("}}", "\\}")
 
@@ -174,7 +214,91 @@
     """Check if it is an integer."""
     return isinstance(val, bool)
 
 
 def _is_str(val: Any) -> bool:
     """Check if it is an integer."""
     return isinstance(val, str)
+
+
+def _is_regex(val: Any) -> bool:
+    """Check if it is an integer."""
+    return isinstance(val, re.Pattern)
+
+
+def _less_than_operator(v_ref: Any) -> ComparisonExpression:
+    """Create less then comparison expression."""
+    return ComparisonExpression(lambda val, v_ref=v_ref: val < v_ref, v_ref)
+
+
+def _equal_or_less_than_operator(v_ref: Any) -> ComparisonExpression:
+    """Create equal or less than comparison expression."""
+    return ComparisonExpression(lambda val, val_ref=v_ref: val <= val_ref, v_ref)
+
+
+def _greater_than_operator(v_ref: Any) -> ComparisonExpression:
+    """Create less then comparison expression."""
+    return ComparisonExpression(lambda val, val_ref=v_ref: val > val_ref, v_ref)
+
+
+def _equal_or_greater_than_operator(v_ref: Any) -> ComparisonExpression:
+    """Create equal or less than comparison expression."""
+    return ComparisonExpression(lambda val, val_ref=v_ref: val >= val_ref, v_ref)
+
+
+def _not_equal_to_operator(v_ref: Any) -> ComparisonExpression:
+    """Create a not equal comparison expression."""
+    return ComparisonExpression(lambda val, val_ref=v_ref: val != val_ref, v_ref)
+
+
+def _flows_info(state: State, flow_instance_uid: Optional[str] = None) -> dict:
+    """Return a summary of the provided state, or all states by default."""
+    if flow_instance_uid is not None and flow_instance_uid in state.flow_states:
+        summary = {"flow_instance_uid": flow_instance_uid}
+        summary.update(
+            _flow_state_related_to_source(state, state.flow_states[flow_instance_uid])
+        )
+
+        return summary
+    else:
+        summary = {}
+        for flow_state in state.flow_states.values():
+            summary.update(
+                {flow_state.uid: _flow_state_related_to_source(state, flow_state)}
+            )
+        return summary
+
+
+def _flow_state_related_to_source(state: State, flow_state: FlowState):
+    flow_config = state.flow_configs[flow_state.flow_id]
+    flow_head_source_lines: Set[int] = set()
+    for head in flow_state.active_heads.values():
+        element = flow_config.elements[head.position]
+        if isinstance(element, Element) and element._source is not None:
+            flow_head_source_lines.add(element._source.line)
+    summary: dict = {
+        "flow_id": flow_state.flow_id,
+        "loop_id": flow_state.loop_id,
+        "status": flow_state.status.value,
+        "flow_hierarchy": _get_flow_state_hierarchy(state, flow_state.uid)[:-1],
+        "active_statement_at_lines": list(flow_head_source_lines),
+    }
+
+    if flow_state.action_uids:
+        summary.update({"action_uids": flow_state.action_uids})
+
+    if flow_state.child_flow_uids:
+        summary.update({"child_flow_uids": flow_state.child_flow_uids})
+
+    return summary
+
+
+def _get_flow_state_hierarchy(state: State, flow_state_uid: str) -> List[str]:
+    if flow_state_uid not in state.flow_states:
+        return []
+    flow_state = state.flow_states[flow_state_uid]
+    if flow_state.parent_uid is None:
+        return [flow_state.uid]
+    else:
+        result = _get_flow_state_hierarchy(state, flow_state.parent_uid)
+        result.append(flow_state.uid)
+        return result
```

## nemoguardrails/colang/v2_x/runtime/flows.py

```diff
@@ -19,24 +19,25 @@
 
 import logging
 import time
 from collections import deque
 from dataclasses import dataclass, field
 from datetime import datetime
 from enum import Enum
-from typing import Any, Callable, Deque, Dict, List, Optional, Set, Tuple, Union
+from typing import Any, Callable, Deque, Dict, List, Optional, Tuple, Union
 
 from dataclasses_json import dataclass_json
 
 from nemoguardrails.colang.v2_x.lang.colang_ast import (
-    Decorator,
     ElementType,
     FlowParamDef,
     FlowReturnMemberDef,
 )
+from nemoguardrails.colang.v2_x.runtime.errors import ColangSyntaxError
+from nemoguardrails.colang.v2_x.runtime.utils import new_readable_uid
 from nemoguardrails.utils import new_uid
 
 log = logging.getLogger(__name__)
 
 random_seed = int(time.time())
 
 
@@ -212,14 +213,36 @@
 
         # The arguments that will be used for the start event
         self.start_event_arguments = arguments
 
         # Number of flows where this action is still active
         self.flow_scope_count = 0
 
+    def to_dict(self):
+        return {
+            "uid": self.uid,
+            "name": self.name,
+            "flow_uid": self.flow_uid,
+            "status": self.status.name,
+            "context": self.context,
+            "start_event_arguments": self.start_event_arguments,
+            "flow_scope_count": self.flow_scope_count,
+        }
+
+    @staticmethod
+    def from_dict(d):
+        action = Action(
+            name=d["name"], arguments=d["start_event_arguments"], flow_uid=d["flow_uid"]
+        )
+        action.uid = d["uid"]
+        action.status = ActionStatus[d["status"]]
+        action.context = d["context"]
+        action.flow_scope_count = d["flow_scope_count"]
+        return action
+
     # Process an event
     def process_event(self, event: ActionEvent) -> None:
         """Processes event and updates action accordingly."""
         # TODO: This matching can easily break if action names are badly chosen
         if "Action" in event.name and event.action_uid == self.uid:
             if "ActionStarted" in event.name:
                 self.context.update(event.arguments)
@@ -334,29 +357,67 @@
 
     # The flow parameters
     parameters: List[FlowParamDef]
 
     # The decorators for the flow.
     # Maps the name of the applied decorators to the arguments.
     # If positional arguments are provided, then the "$0", "$1", ... are used as the keys.
-    decorators: Dict[str, Decorator] = field(default_factory=dict)
+    decorators: Dict[str, Dict[str, Any]] = field(default_factory=dict)
 
     # The flow return member variables
     return_members: List[FlowReturnMemberDef] = field(default_factory=list)
 
     # All the label element positions in the flow
     element_labels: Dict[str, int] = field(default_factory=dict)
 
-    # Interaction loop
-    loop_id: Optional[str] = None
-    loop_type: InteractionLoopType = InteractionLoopType.PARENT
-
     # The actual source code, if available
     source_code: Optional[str] = None
 
+    @property
+    def loop_id(self) -> Optional[str]:
+        """Return the interaction loop id if set."""
+        if "loop" in self.decorators:
+            parameters = self.decorators["loop"]
+            if "id" in parameters:
+                return parameters["id"]
+            elif "$0" in parameters:
+                return parameters["$0"]
+            else:
+                log.warning(
+                    "No loop id specified for @loop decorator for flow `%s`", self.id
+                )
+        return None
+
+    @property
+    def loop_type(self) -> InteractionLoopType:
+        """Return the interaction loop type."""
+        loop_id = self.loop_id
+        if loop_id == "NEW":
+            return InteractionLoopType.NEW
+        elif loop_id is not None:
+            return InteractionLoopType.NAMED
+        else:
+            return InteractionLoopType.PARENT
+
+    @property
+    def is_override(self) -> bool:
+        """Return True if flow is marked as override."""
+        return "override" in self.decorators
+
+    def has_meta_tag(self, tag_name: str) -> bool:
+        """Return True if flow is marked with given meta tag, e.g. `@meta(llm_exclude=True)`."""
+        return "meta" in self.decorators and tag_name in self.decorators["meta"]
+
+    def meta_tag(self, tag_name: str) -> Optional[Any]:
+        """Return the parameter of the meta tag or None if it does not exist."""
+        if not self.has_meta_tag(tag_name):
+            return None
+        else:
+            return self.decorators["meta"][tag_name]
+
 
 class FlowHeadStatus(Enum):
     """The status of a flow head."""
 
     ACTIVE = "active"  # The head is active and either waiting or progressing
     INACTIVE = "inactive"  # The head is no longer progressing (e.g. is a parent of an active child head)
     MERGING = "merging"  # The head arrived at a head merging element and will progress only in the next iteration
@@ -497,22 +558,22 @@
     # The current set of variables in the flow state.
     context: dict = field(default_factory=dict)
 
     # The current priority of the flow instance that is used for action resolution.
     priority: float = 1.0
 
     # All the arguments of a flow (e.g. flow bot say $utterance -> arguments = ["$utterance"])
-    arguments: List[str] = field(default_factory=list)
-
-    # Variables in the flow that are defined as global
-    global_variables: Set[str] = field(default_factory=set)
+    arguments: Dict[str, Any] = field(default_factory=dict)
 
     # Parent flow id
     parent_uid: Optional[str] = None
 
+    # Parent flow head id
+    parent_head_uid: Optional[str] = None
+
     # The ids of all the child flows
     child_flow_uids: List[str] = field(default_factory=list)
 
     # The current state of the flow
     _status: FlowStatus = FlowStatus.WAITING
 
     # The datetime of the last status change of the flow
@@ -556,91 +617,127 @@
             "Started": "started_event",
             "Paused": "paused_event",
             "Resumed": "resumed_event",
             "Finished": "finished_event",
             "Failed": "failed_event",
         }
 
-    def get_event(self, name: str, arguments: dict) -> InternalEvent:
+    def get_event(
+        self, name: str, arguments: dict, matching_scores: Optional[List[float]] = None
+    ) -> InternalEvent:
         """Returns the corresponding action event."""
         assert name in self._event_name_map, f"Event '{name}' not available!"
         func = getattr(self, self._event_name_map[name])
-        return func(arguments)
+        if not matching_scores:
+            matching_scores = []
+        return func(matching_scores, arguments)
 
     # Flow events to send
-    def start_event(self, _args: dict) -> InternalEvent:
+    def start_event(
+        self, matching_scores: List[float], args: Optional[dict] = None
+    ) -> InternalEvent:
         """Starts the flow. Takes no arguments."""
+        arguments = {
+            "flow_instance_uid": new_readable_uid(self.flow_id),
+            "flow_id": self.flow_id,
+            "source_flow_instance_uid": self.parent_uid,
+            "source_head_uid": self.parent_head_uid,
+            "flow_hierarchy_position": self.hierarchy_position,
+            "activated": self.activated,
+        }
+        arguments.update(self.arguments)
+        if args:
+            arguments.update(args)
         return InternalEvent(
-            name=InternalEvents.START_FLOW, arguments={"flow_id": self.flow_id}
+            name=InternalEvents.START_FLOW,
+            arguments=arguments,
+            matching_scores=matching_scores,
         )
 
-    def finish_event(self, _args: dict) -> InternalEvent:
+    def finish_event(self, matching_scores: List[float], _args: dict) -> InternalEvent:
         """Finishes the flow. Takes no arguments."""
         return InternalEvent(
             name=InternalEvents.FINISH_FLOW,
             arguments={"flow_id": self.flow_id, "flow_instance_uid": self.uid},
+            matching_scores=matching_scores,
         )
 
-    def stop_event(self, _args: dict) -> InternalEvent:
+    def stop_event(self, matching_scores: List[float], _args: dict) -> InternalEvent:
         """Stops the flow. Takes no arguments."""
         return InternalEvent(
             name="StopFlow",
             arguments={"flow_id": self.flow_id, "flow_instance_uid": self.uid},
+            matching_scores=matching_scores,
         )
 
-    def pause_event(self, _args: dict) -> InternalEvent:
+    def pause_event(self, matching_scores: List[float], _args: dict) -> InternalEvent:
         """Pauses the flow. Takes no arguments."""
         return InternalEvent(
             name="PauseFlow",
             arguments={"flow_id": self.flow_id, "flow_instance_uid": self.uid},
+            matching_scores=matching_scores,
         )
 
-    def resume_event(self, _args: dict) -> InternalEvent:
+    def resume_event(self, matching_scores: List[float], _args: dict) -> InternalEvent:
         """Resumes the flow. Takes no arguments."""
         return InternalEvent(
             name="ResumeFlow",
             arguments={"flow_id": self.flow_id, "flow_instance_uid": self.uid},
+            matching_scores=matching_scores,
         )
 
     # Flow events to match
-    def started_event(self, args: dict) -> InternalEvent:
+    def started_event(
+        self, matching_scores: List[float], args: Optional[Dict[str, Any]] = None
+    ) -> InternalEvent:
         """Returns the flow Started event."""
-        return self._create_event(InternalEvents.FLOW_STARTED, args)
+        return self._create_out_event(
+            InternalEvents.FLOW_STARTED, matching_scores, args
+        )
 
     # def paused_event(self, args: dict) -> FlowEvent:
     #     """Returns the flow Pause event."""
     #     return self._create_event(InternalEvents.FLOW_PAUSED, args)
 
     # def resumed_event(self, args: dict) -> FlowEvent:
     #     """Returns the flow Resumed event."""
     #     return self._create_event(InternalEvents.FLOW_RESUMED, args)
 
-    def finished_event(self, args: dict) -> InternalEvent:
+    def finished_event(
+        self, matching_scores: List[float], args: Optional[Dict[str, Any]] = None
+    ) -> InternalEvent:
         """Returns the flow Finished event."""
-        return self._create_event(InternalEvents.FLOW_FINISHED, args)
+        if not args:
+            args = {}
+        if "_return_value" in self.context:
+            args["return_value"] = self.context["_return_value"]
+        return self._create_out_event(
+            InternalEvents.FLOW_FINISHED, matching_scores, args
+        )
 
-    def failed_event(self, args: dict) -> InternalEvent:
+    def failed_event(
+        self, matching_scores: List[float], args: Optional[Dict[str, Any]] = None
+    ) -> InternalEvent:
         """Returns the flow Failed event."""
-        return self._create_event(InternalEvents.FLOW_FAILED, args)
+        return self._create_out_event(InternalEvents.FLOW_FAILED, matching_scores, args)
 
-    def _create_event(self, event_type: str, args: dict) -> InternalEvent:
-        arguments = args.copy()
+    def _create_out_event(
+        self,
+        event_type: str,
+        matching_scores: List[float],
+        args: Optional[Dict[str, Any]],
+    ) -> InternalEvent:
+        arguments = {}
+        arguments["source_flow_instance_uid"] = self.uid
+        arguments["flow_instance_uid"] = self.uid
         arguments["flow_id"] = self.flow_id
-        if "flow_start_uid" in self.context:
-            arguments["flow_start_uid"] = self.context["flow_start_uid"]
-        arguments.update(
-            dict(
-                [
-                    (arg, self.context[arg])
-                    for arg in self.arguments
-                    if arg in self.context
-                ]
-            )
-        )
-        return InternalEvent(event_type, arguments)
+        arguments.update(self.arguments)
+        if args:
+            arguments.update(args)
+        return InternalEvent(event_type, arguments, matching_scores)
 
     def __repr__(self) -> str:
         return (
             f"FlowState[uid={self.uid}, flow_id={self.flow_id}, loop_id={self.loop_id}]"
         )
 
     # Expose all flow variables as attributes of the flow
@@ -698,30 +795,9 @@
     # Helper dictionary that maps from flow_id (name) to all available flow states
     flow_id_states: Dict[str, List[FlowState]] = field(default_factory=dict)
 
     # Helper dictionary () that maps active event matchers (by event names) to relevant heads (flow_state_uid, head_uid)
     event_matching_heads: Dict[str, List[Tuple[str, str]]] = field(default_factory=dict)
 
     # Helper dictionary that maps active heads (flow_state_uid, head_uid) to event matching names
-    event_matching_heads_reverse_map: Dict[Tuple[str, str], str] = field(
-        default_factory=dict
-    )
-
-
-class ColangParsingError(Exception):
-    """Raised when there is invalid Colang syntax detected."""
-
-
-class ColangSyntaxError(Exception):
-    """Raised when there is invalid Colang syntax detected."""
-
-
-class ColangValueError(Exception):
-    """Raised when there is an invalid value detected in a Colang expression."""
-
-
-class ColangRuntimeError(Exception):
-    """Raised when there is a Colang related runtime exception."""
-
-
-class LlmResponseError(Exception):
-    """Raised when there is an issue with the lmm response."""
+    # The key is constructed as the concatenation of the two ids.
+    event_matching_heads_reverse_map: Dict[str, str] = field(default_factory=dict)
```

## nemoguardrails/colang/v2_x/runtime/runtime.py

```diff
@@ -22,20 +22,20 @@
 import aiohttp
 import langchain
 from langchain.chains.base import Chain
 
 from nemoguardrails.actions.actions import ActionResult
 from nemoguardrails.colang import parse_colang_file
 from nemoguardrails.colang.runtime import Runtime
-from nemoguardrails.colang.v2_x.lang.colang_ast import Flow
-from nemoguardrails.colang.v2_x.runtime.flows import (
+from nemoguardrails.colang.v2_x.lang.colang_ast import Decorator, Flow
+from nemoguardrails.colang.v2_x.runtime.errors import (
     ColangRuntimeError,
-    Event,
-    FlowStatus,
+    ColangSyntaxError,
 )
+from nemoguardrails.colang.v2_x.runtime.flows import Event, FlowStatus
 from nemoguardrails.colang.v2_x.runtime.statemachine import (
     FlowConfig,
     InternalEvent,
     State,
     expand_elements,
     initialize_flow,
     initialize_state,
@@ -65,40 +65,48 @@
 
         # A way to disable async function execution. Useful for testing.
         self.disable_async_execution = False
 
     async def _add_flows_action(self, state: "State", **args: dict) -> List[str]:
         log.info("Start AddFlowsAction! %s", args)
         flow_content = args["config"]
-        assert isinstance(flow_content, str)
+        if not isinstance(flow_content, str):
+            raise ColangRuntimeError(
+                "Parameter 'config' in AddFlowsAction is not of type 'str'!"
+            )
         # Parse new flow
         try:
             parsed_flow = parse_colang_file(
                 filename="",
                 content=flow_content,
                 version="2.x",
                 include_source_mapping=True,
             )
         except Exception as e:
-            print("Failed parsing a generated flow\n%s\n%s", flow_content, e)
+            warning = f"Failed parsing a generated flow\n{flow_content}\n{e}"
+            log.warning(warning)
+            print(warning)
             return []
             # Alternatively, we could through an exceptions
             # raise ColangRuntimeError(f"Could not parse the generated Colang code! {ex}")
 
         added_flows: List[str] = []
         for flow in parsed_flow["flows"]:
             if flow.name in state.flow_configs:
-                print("Flow '%s' already exists! Not loaded!", flow.name)
+                warning = "Flow '{flow.name}' already exists! Not loaded!"
+                log.warning(warning)
+                print(warning)
                 break
 
             flow_config = FlowConfig(
                 id=flow.name,
-                loop_id=None,
                 elements=expand_elements(flow.elements, state.flow_configs),
+                decorators=convert_decorator_list_to_dictionary(flow.decorators),
                 parameters=flow.parameters,
+                return_members=flow.return_members,
                 source_code=flow.source_code,
             )
 
             # Print out expanded flow elements
             # json.dump(flow_config, sys.stdout, indent=4, cls=EnhancedJsonEncoder)
 
             initialize_flow(state, flow_config)
@@ -120,27 +128,15 @@
                     del state.flow_states[flow_state.uid]
                 del state.flow_id_states[flow_id]
             if flow_id in state.flow_configs:
                 del state.flow_configs[flow_id]
 
     def _init_flow_configs(self) -> None:
         """Initializes the flow configs based on the config."""
-        self.flow_configs = {}
-
-        for flow in self.config.flows:
-            assert isinstance(flow, Flow)
-            flow_id = flow.name
-            self.flow_configs[flow_id] = FlowConfig(
-                id=flow_id,
-                elements=flow.elements,
-                decorators={decorator.name: decorator for decorator in flow.decorators},
-                parameters=flow.parameters,
-                return_members=flow.return_members,
-                source_code=flow.source_code,
-            )
+        self.flow_configs = create_flow_configs_from_flow_list(self.config.flows)
 
     async def generate_events(self, events: List[dict]) -> List[dict]:
         raise NotImplementedError("Stateless API not supported for Colang 2.x, yet.")
 
     @staticmethod
     def _internal_error_action_result(message: str) -> ActionResult:
         """Helper to construct an action result for an internal error."""
@@ -244,15 +240,15 @@
 
                 if (
                     "llm" in kwargs
                     and f"{action_name}_llm" in self.registered_action_params
                 ):
                     kwargs["llm"] = self.registered_action_params[f"{action_name}_llm"]
 
-                log.info("Executing action :: %s", action_name)
+                log.info("Running action :: %s", action_name)
                 result, status = await self.action_dispatcher.execute_action(
                     action_name, kwargs
                 )
 
             # If the action execution failed, we return a hardcoded message
             if status == "failed":
                 # TODO: make this message configurable.
@@ -335,24 +331,25 @@
                 f"Failed to get response from {action_name} due to exception {e}"
             )
             log.info(error_message)
             raise ColangRuntimeError(error_message) from e
         return result, status
 
     @staticmethod
-    def _get_action_finished_event(result: dict) -> Dict[str, Any]:
+    def _get_action_finished_event(result: dict, **kwargs) -> Dict[str, Any]:
         """Helper to return the ActionFinished event from the result of running a local action."""
         return new_event_dict(
             f"{result['action_name']}Finished",
             action_uid=result["start_action_event"]["action_uid"],
             action_name=result["action_name"],
             status="success",
             is_success=True,
             return_value=result["return_value"],
             events=result["new_events"],
+            **kwargs
             # is_system_action=action_meta.get("is_system_action", False),
         )
 
     async def _get_async_actions_finished_events(
         self, main_flow_uid: str
     ) -> Tuple[List[dict], int]:
         """Helper to return the ActionFinished events for the local async actions that finished.
@@ -392,35 +389,53 @@
             # We need to create the corresponding action finished event
             action_finished_event = self._get_action_finished_event(result)
             action_finished_events.append(action_finished_event)
 
         return action_finished_events, len(pending)
 
     async def process_events(
-        self, events: List[dict], state: Union[Optional[dict], State] = None
+        self,
+        events: List[dict],
+        state: Union[Optional[dict], State] = None,
+        blocking: bool = False,
+        instant_actions: Optional[List[str]] = None,
     ) -> Tuple[List[Dict[str, Any]], State]:
         """Process a sequence of events in a given state.
 
-        The events will be processed one by one, in the input order.
+        Runs an "event processing cycle", i.e., process all input events in the given state, and
+        return the new state and the output events.
+
+        The events will be processed one by one, in the input order. If new events are
+        generated as part of the processing, they will be appended to the input events.
+
+        By default, a processing cycle only waits for the local actions to finish, i.e,
+        if after processing all the input events, there are local actions in progress, the
+        event processing will wait for them to finish.
+
+        In blocking mode, the event processing will also wait for the local async actions.
 
         Args:
             events: A sequence of events that needs to be processed.
             state: The state that should be used as the starting point. If not provided,
               a clean state will be used.
+            blocking: If set, in blocking mode, the processing cycle will wait for
+              all the local async actions as well.
+            instant_actions: The name of the actions which should finish instantly, i.e.,
+              the start event will not be returned to the user and wait for the finish event.
 
         Returns:
             (output_events, output_state) Returns a sequence of output events and an output
               state.
         """
 
         output_events = []
         input_events: List[Union[dict, InternalEvent]] = events.copy()
         local_running_actions: List[asyncio.Task[dict]] = []
 
-        if state is None:
+        if state is None or state == {}:
             state = State(flow_states={}, flow_configs=self.flow_configs)
             initialize_state(state)
         elif isinstance(state, dict):
             # TODO: Implement dict to State conversion
             raise NotImplementedError()
         #     if isinstance(state, dict):
         #         state = State.from_dict(state)
@@ -442,15 +457,15 @@
         local_action_finished_events = []
         return_local_async_action_count = False
 
         # While we have input events to process, or there are local running actions
         # we continue the processing.
         while input_events or local_running_actions:
             for event in input_events:
-                log.info("Processing event %s", event)
+                log.info("Processing event :: %s", event)
 
                 event_name = event["type"] if isinstance(event, dict) else event.name
 
                 if event_name == "CheckLocalAsync":
                     return_local_async_action_count = True
                     continue
 
@@ -464,15 +479,15 @@
                         run_to_completion(state, new_event)
                         new_event = None
                     except Exception as e:
                         log.warning("Colang error!", exc_info=True)
                         new_event = Event(
                             name="ColangError",
                             arguments={
-                                "error_type": str(type(e).__name__),
+                                "type": str(type(e).__name__),
                                 "error": str(e),
                             },
                         )
                     await asyncio.sleep(0.001)
 
                 # If we have context updates after this event, we first add that.
                 # TODO: Check if this is still needed for e.g. stateless implementation
@@ -486,15 +501,39 @@
                     state.last_events.append(out_event)
 
                     # We need to check if we need to run a locally registered action
                     start_action_match = re.match(r"Start(.*Action)", out_event["type"])
                     if start_action_match:
                         action_name = start_action_match[1]
 
-                        if action_name in self.action_dispatcher.registered_actions:
+                        # If it's an instant action, we finish it right away.
+                        if instant_actions and action_name in instant_actions:
+                            finished_event_data = {
+                                "action_name": action_name,
+                                "start_action_event": out_event,
+                                "return_value": None,
+                                "new_events": [],
+                            }
+
+                            # TODO: figure out a generic way of creating a compliant
+                            #   ...ActionFinished event
+                            extra = {}
+                            if action_name == "UtteranceBotAction":
+                                extra["final_script"] = out_event["script"]
+
+                            action_finished_event = self._get_action_finished_event(
+                                finished_event_data, **extra
+                            )
+
+                            # We send the completion of the action as an output event
+                            # and continue processing it.
+                            output_events.append(action_finished_event)
+                            input_events.append(action_finished_event)
+
+                        elif action_name in self.action_dispatcher.registered_actions:
                             # In this case we need to start the action locally
                             action_fn = self.action_dispatcher.get_action(action_name)
                             execute_async = getattr(action_fn, "action_meta", {}).get(
                                 "execute_async", False
                             )
 
                             # Start the local action
@@ -505,15 +544,21 @@
                                     events_history=state.last_events,
                                     state=state,
                                 )
                             )
 
                             # If the function is not async, or async execution is disabled
                             # we execute the actions as a local action.
-                            if not execute_async or self.disable_async_execution:
+                            # Also, if we're running this in blocking mode, we add all local
+                            # actions as non-async.
+                            if (
+                                not execute_async
+                                or self.disable_async_execution
+                                or blocking
+                            ):
                                 local_running_actions.append(local_action)
                             else:
                                 main_flow_uid = state.main_flow_state.uid
                                 if main_flow_uid not in self.async_actions:
                                     self.async_actions[main_flow_uid] = []
                                 self.async_actions[main_flow_uid].append(local_action)
 
@@ -608,7 +653,62 @@
         return {
             "action_name": action_name,
             "return_value": return_value,
             "new_events": new_events,
             "context_updates": context_updates,
             "start_action_event": start_action_event,
         }
+
+
+def convert_decorator_list_to_dictionary(
+    decorators: List[Decorator],
+) -> Dict[str, Dict[str, Any]]:
+    """Convert list of decorators to a dictionary merging the parameters of decorators with same name."""
+    decorator_dict: Dict[str, Dict[str, Any]] = {}
+    for decorator in decorators:
+        item = decorator_dict.get(decorator.name, None)
+        if item:
+            item.update(decorator.parameters)
+        else:
+            decorator_dict[decorator.name] = decorator.parameters
+    return decorator_dict
+
+
+def create_flow_configs_from_flow_list(flows: List[Flow]) -> Dict[str, FlowConfig]:
+    """Create a flow config dictionary and resolves flow overriding."""
+    flow_configs: Dict[str, FlowConfig] = {}
+    override_flows: Dict[str, FlowConfig] = {}
+
+    # Create two dictionaries with normal and override flows
+    for flow in flows:
+        assert isinstance(flow, Flow)
+        config = FlowConfig(
+            id=flow.name,
+            elements=flow.elements,
+            decorators=convert_decorator_list_to_dictionary(flow.decorators),
+            parameters=flow.parameters,
+            return_members=flow.return_members,
+            source_code=flow.source_code,
+        )
+
+        if config.is_override:
+            if flow.name in override_flows:
+                raise ColangSyntaxError(
+                    f"Multiple override flows with name '{flow.name}' detected! There can only be one!"
+                )
+            override_flows[flow.name] = config
+        elif flow.name in flow_configs:
+            raise ColangSyntaxError(
+                f"Multiple non-overriding flows with name '{flow.name}' detected! There can only be one!"
+            )
+        else:
+            flow_configs[flow.name] = config
+
+    # Override normal flows
+    for override_flow in override_flows.values():
+        if override_flow.id not in flow_configs:
+            raise ColangSyntaxError(
+                f"Override flow with name '{override_flow.id}' does not override any flow with that name!"
+            )
+        flow_configs[override_flow.id] = override_flow
+
+    return flow_configs
```

## nemoguardrails/colang/v2_x/runtime/statemachine.py

```diff
@@ -43,34 +43,39 @@
     Return,
     Spec,
     SpecOp,
     SpecType,
     WaitForHeads,
 )
 from nemoguardrails.colang.v2_x.lang.expansion import expand_elements
-from nemoguardrails.colang.v2_x.runtime.eval import eval_expression
+from nemoguardrails.colang.v2_x.runtime.errors import (
+    ColangRuntimeError,
+    ColangValueError,
+)
+from nemoguardrails.colang.v2_x.runtime.eval import (
+    ComparisonExpression,
+    eval_expression,
+)
 from nemoguardrails.colang.v2_x.runtime.flows import (
     Action,
     ActionEvent,
     ActionStatus,
-    ColangRuntimeError,
-    ColangValueError,
     Event,
     FlowConfig,
     FlowHead,
     FlowHeadStatus,
     FlowState,
     FlowStatus,
     InteractionLoopType,
     InternalEvent,
     InternalEvents,
     State,
 )
 from nemoguardrails.colang.v2_x.runtime.utils import new_readable_uid
-from nemoguardrails.utils import new_event_dict, new_uid
+from nemoguardrails.utils import console, new_event_dict, new_uid
 
 log = logging.getLogger(__name__)
 
 random_seed = int(time.time())
 
 
 def initialize_state(state: State) -> None:
@@ -87,97 +92,97 @@
     # TODO: Think about where to put this
     for flow_config in state.flow_configs.values():
         initialize_flow(state, flow_config)
 
     # Create main flow state first
     main_flow_config = state.flow_configs["main"]
     main_flow = add_new_flow_instance(
-        state, create_flow_instance(main_flow_config, "0")
+        state, create_flow_instance(main_flow_config, new_readable_uid("main"), "0", {})
     )
+    main_flow.activated = True
     if main_flow_config.loop_id is None:
         main_flow.loop_id = new_readable_uid("main")
     else:
         main_flow.loop_id = main_flow_config.loop_id
     state.main_flow_state = main_flow
 
 
 def initialize_flow(state: State, flow_config: FlowConfig) -> None:
     """Initialize a flow before it can be used and instantiated."""
     # Transform and resolve flow configuration element notation (actions, flows, ...)
     flow_config.elements = expand_elements(flow_config.elements, state.flow_configs)
 
-    # Extract flow loop id if available
-    # TODO: deprecate this convention in the favor of the `@loop("some_id")` decorator.
-    if flow_config.source_code:
-        match = re.search(r"#\W*meta:\W*loop_id\W*=\W*(\w*)", flow_config.source_code)
-        if match:
-            flow_config.loop_id = match.group(1)
-
-    # Extract loop id from decorator if available
-    if "loop" in flow_config.decorators:
-        decorator = flow_config.decorators["loop"]
-        if "id" in decorator.parameters:
-            flow_config.loop_id = decorator.parameters["id"]
-        elif "$0" in decorator.parameters:
-            flow_config.loop_id = decorator.parameters["$0"]
-        else:
-            raise ValueError(
-                f"No loop id specified for @loop decorator for flow `{flow_config.id}`"
-            )
-
     # Extract all the label elements
     for idx, element in enumerate(flow_config.elements):
         if isinstance(element, Label):
             flow_config.element_labels.update({element["name"]: idx})
 
 
 def create_flow_instance(
-    flow_config: FlowConfig, flow_hierarchy_position: str
+    flow_config: FlowConfig,
+    flow_instance_uid: str,
+    flow_hierarchy_position: str,
+    event_arguments: Dict[str, Any],
 ) -> FlowState:
     """Create a new flow instance that can be added."""
     loop_uid: Optional[str] = None
     if flow_config.loop_type == InteractionLoopType.NEW:
         loop_uid = new_uid()
     elif flow_config.loop_type == InteractionLoopType.NAMED:
         assert flow_config.loop_id is not None
         loop_uid = flow_config.loop_id
     # For type InteractionLoopType.PARENT we keep it None to infer loop_id at run_time from parent
 
-    flow_uid = new_readable_uid(flow_config.id)
-
     head_uid = new_uid()
     flow_state = FlowState(
-        uid=flow_uid,
+        uid=flow_instance_uid,
         flow_id=flow_config.id,
         loop_id=loop_uid,
         hierarchy_position=flow_hierarchy_position,
         heads={
             head_uid: FlowHead(
                 uid=head_uid,
-                flow_state_uid=flow_uid,
+                flow_state_uid=flow_instance_uid,
                 matching_scores=[],
             )
         },
     )
 
+    if "context" in event_arguments:
+        if flow_config.parameters:
+            raise ColangRuntimeError(
+                f"Context cannot be shared to flows with parameters: '{flow_config.id}'"
+            )
+        # Replace local context with context from parent flow (shared flow context)
+        flow_state.context = event_arguments["context"]
+
     # Add all the flow parameters
     for idx, param in enumerate(flow_config.parameters):
-        flow_state.arguments.append(param.name)
+        if param.name in event_arguments:
+            val = event_arguments[param.name]
+        else:
+            val = (
+                eval_expression(param.default_value_expr, {})
+                if param.default_value_expr
+                else None
+            )
+        flow_state.arguments[param.name] = val
         flow_state.context.update(
             {
-                param.name: (
-                    eval_expression(param.default_value_expr, {})
-                    if param.default_value_expr
-                    else None
-                ),
+                param.name: val,
             }
         )
+
     # Add the positional flow parameter identifiers
     for idx, param in enumerate(flow_config.parameters):
-        flow_state.arguments.append(f"${idx}")
+        positional_param = f"${idx}"
+        if positional_param in event_arguments:
+            val = event_arguments[positional_param]
+            flow_state.arguments[param.name] = val
+            flow_state.arguments[positional_param] = val
 
     # Add all flow return members
     for idx, member in enumerate(flow_config.return_members):
         flow_state.context.update(
             {
                 member.name: (
                     eval_expression(member.default_value_expr, {})
@@ -322,15 +327,15 @@
                             heads_matching.append(head)
                             if event.name == InternalEvents.START_FLOW:
                                 handled_event_loops.add("all_loops")
                             else:
                                 assert flow_state.loop_id
                                 handled_event_loops.add(flow_state.loop_id)
                             log.info(
-                                "Matching head: %s context=%s",
+                                "Matching head :: %s context=%s",
                                 head,
                                 _context_log(flow_state),
                             )
                         elif matching_score < 0.0:
                             # Event match mismatch
                             heads_failing.append(head)
                             log.info(
@@ -465,21 +470,43 @@
     Return a set of all the event loop ids that handled the event.
     """
     handled_event_loops = set()
     if event.name == InternalEvents.START_FLOW:
         # Start new flow state instance if flow exists
         flow_id = event.arguments["flow_id"]
         if flow_id in state.flow_configs and flow_id != "main":
-            add_new_flow_instance(
-                state,
-                create_flow_instance(
-                    state.flow_configs[flow_id],
-                    event.arguments["flow_hierarchy_position"],
-                ),
-            )
+            started_instance = None
+            if (
+                event.arguments.get("activated", None)
+                and flow_id in state.flow_id_states
+            ):
+                assert isinstance(event, InternalEvent)
+                started_instance = _check_for_activated_flow_instance(state, event)
+
+            if started_instance:
+                started_event = started_instance.started_event(
+                    event.matching_scores,
+                    {"flow_instance_uid": event.arguments["flow_instance_uid"]},
+                )
+                _push_internal_event(
+                    state,
+                    started_event,
+                )
+                handled_event_loops.add("all_loops")
+            else:
+                add_new_flow_instance(
+                    state,
+                    create_flow_instance(
+                        state.flow_configs[flow_id],
+                        event.arguments["flow_instance_uid"],
+                        event.arguments["flow_hierarchy_position"],
+                        event.arguments,
+                    ),
+                )
+
     elif event.name == InternalEvents.FINISH_FLOW:
         if "flow_instance_uid" in event.arguments:
             flow_instance_uid = event.arguments["flow_instance_uid"]
             if flow_instance_uid in state.flow_states:
                 flow_state = state.flow_states[event.arguments["flow_instance_uid"]]
                 if not _is_inactive_flow(flow_state):
                     _finish_flow(
@@ -494,15 +521,15 @@
             if flow_id in state.flow_id_states:
                 for flow_state in state.flow_id_states[flow_id]:
                     if not _is_inactive_flow(flow_state):
                         _finish_flow(
                             state,
                             flow_state,
                             event.matching_scores,
-                            event.arguments.get("deactivate", "False"),
+                            event.arguments.get("deactivate", False),
                         )
                         assert flow_state.loop_id
                         handled_event_loops.add(flow_state.loop_id)
     elif event.name == InternalEvents.STOP_FLOW:
         if "flow_instance_uid" in event.arguments:
             flow_instance_uid = event.arguments["flow_instance_uid"]
             if flow_instance_uid in state.flow_states:
@@ -543,14 +570,39 @@
         # We also record the flow finished events in the history
         state.last_events.append(event)
         handled_event_loops.add("all_loops")
 
     return handled_event_loops
 
 
+def _check_for_activated_flow_instance(
+    state: State, event: InternalEvent
+) -> Optional[FlowState]:
+    # Check if there already exists an instance of the same activated flow
+    flow_id = event.arguments["flow_id"]
+    for activated_flow in state.flow_id_states[flow_id]:
+        if activated_flow.status != FlowStatus.STARTED or not activated_flow.activated:
+            continue
+        has_same_arguments = False
+        for idx, arg in enumerate(state.flow_configs[flow_id].parameters):
+            val = activated_flow.arguments[arg.name]
+            if arg.name in event.arguments and val == event.arguments[arg.name]:
+                has_same_arguments = True
+            elif f"${idx}" in event.arguments and val == event.arguments[f"${idx}"]:
+                has_same_arguments = True
+            else:
+                has_same_arguments = False
+                break
+
+        if has_same_arguments:
+            return activated_flow
+
+    return None
+
+
 def _get_all_head_candidates(state: State, event: Event) -> List[Tuple[str, str]]:
     """
     Find all heads that are on a potential match with the event.
     Returns those heads in a flow hierarchical order.
     """
     # Find all heads of flows where the event is relevant
     head_candidates = state.event_matching_heads.get(event.name, []).copy()
@@ -603,14 +655,15 @@
             event.name == InternalEvents.START_FLOW
             and event.arguments["flow_id"] == flow_state.flow_id
             and head.position == 0
         ):
             _start_flow(state, flow_state, event.arguments)
         elif event.name == InternalEvents.FLOW_STARTED:
             # Add started flow to active scopes
+            # TODO: Make this independent from matching to FlowStarted event since otherwise it could be added elsewhere
             for scope_uid in head.scope_uids:
                 if scope_uid in flow_state.scopes:
                     flow_state.scopes[scope_uid][0].append(
                         event.arguments["source_flow_instance_uid"]
                     )
         # elif event.name == InternalEvents.FINISH_FLOW:
         #     _finish_flow(new_state, flow_state)
@@ -707,17 +760,17 @@
                             ):
                                 action = state.actions[winning_event.action_uid]
                                 action.flow_scope_count += 1
                                 competing_flow_state.context[key] = action
                         index = competing_flow_state.action_uids.index(
                             competing_event.action_uid
                         )
-                        competing_flow_state.action_uids[
-                            index
-                        ] = winning_event.action_uid
+                        # Adding _action_uid to avoid formatting flipping by black.
+                        _action_uid = winning_event.action_uid
+                        competing_flow_state.action_uids[index] = _action_uid
                         del state.actions[competing_event.action_uid]
 
                     advancing_heads.append(head)
                     log.info(
                         "Co-winning action at head: %s scores=%s",
                         head,
                         head.matching_scores,
@@ -789,47 +842,51 @@
                 if flow_state.status == FlowStatus.STOPPING:
                     flow_aborted = True
                 else:
                     flow_finished = True
 
             all_heads_are_waiting = False
             if not flow_finished and not flow_aborted:
-                # Check if all all flow heads are waiting at a 'match' or a 'wait_for_heads' element
+                # Check if all flow heads are waiting at a 'match' or a 'wait_for_heads' element
                 all_heads_are_waiting = True
                 for temp_head in flow_state.active_heads.values():
                     element = flow_config.elements[temp_head.position]
                     if not isinstance(element, WaitForHeads) and (
                         not is_match_op_element(element)
                         or (isinstance(element, SpecOp) and "internal" in element.info)
                     ):
                         all_heads_are_waiting = False
                         break
 
             if flow_finished or all_heads_are_waiting:
                 if flow_state.status == FlowStatus.STARTING:
                     flow_state.status = FlowStatus.STARTED
-                    event = create_internal_flow_event(
-                        InternalEvents.FLOW_STARTED, flow_state, head.matching_scores
-                    )
+                    event = flow_state.started_event(head.matching_scores)
                     _push_internal_event(state, event)
+
+                    # Avoid an activated flow that was just started from finishing
+                    # since this would end in an infinite loop
+                    if flow_finished and flow_state.activated:
+                        flow_finished = False
+                        head.status = FlowHeadStatus.INACTIVE
             elif not flow_aborted:
                 elem = get_element_from_head(state, head)
                 if elem and is_action_op_element(elem):
                     actionable_heads.append(head)
         except Exception as e:
             # In case there were any runtime error the flow will be aborted (fail)
             log.warning(
                 "Colang error: Flow '%s' failed due to runtime exception!",
                 flow_state.flow_id,
                 exc_info=True,
             )
             colang_error_event = Event(
                 name="ColangError",
                 arguments={
-                    "error_type": str(type(e).__name__),
+                    "type": str(type(e).__name__),
                     "error": str(e),
                 },
             )
             _push_internal_event(state, colang_error_event)
             flow_aborted = True
 
         if flow_finished:
@@ -853,18 +910,14 @@
 
 def slide(
     state: State, flow_state: FlowState, flow_config: FlowConfig, head: FlowHead
 ) -> List[FlowHead]:
     """Try to slide a flow with the provided head."""
     new_heads: List[FlowHead] = []
 
-    # TODO: Implement global/local flow context handling
-    # context = state.context
-    # context = flow_state.context
-
     while True:
         # if we reached the end, we stop
         if (
             head.position >= len(flow_config.elements)
             or head.status == FlowHeadStatus.INACTIVE
         ):
             break
@@ -930,19 +983,24 @@
                 head.position += 1
             else:
                 # Not a sliding element
                 break
 
         elif isinstance(element, Label):
             if element.name == "start_new_flow_instance":
-                new_event = _create_restart_flow_internal_event(
-                    flow_state, head.matching_scores
-                )
-                _push_left_internal_event(state, new_event)
-                flow_state.new_instance_started = True
+                if flow_state.status is not FlowStatus.STARTED:
+                    log.warning(
+                        "Did not restart flow '%s' at"
+                        " label 'start_new_flow_instance' since this would have created an infinite loop!",
+                        flow_state.flow_id,
+                    )
+                else:
+                    new_event = flow_state.start_event(head.matching_scores)
+                    _push_left_internal_event(state, new_event)
+                    flow_state.new_instance_started = True
             head.position += 1
 
         elif isinstance(element, Goto):
             if eval_expression(
                 element.expression, _get_eval_context(state, flow_state)
             ):
                 if element.label in flow_config.element_labels:
@@ -1103,22 +1161,28 @@
                 #         del flow_state.heads[waiting_head.uid]
 
                 head.position += 1
             else:
                 break
 
         elif isinstance(element, Assignment):
-            # We need to first evaluate the expression
-            expr_val = eval_expression(
-                element.expression, _get_eval_context(state, flow_state)
-            )
-            if element.key in flow_state.global_variables:
-                state.context.update({element.key: expr_val})
+            # Check if we have a conflict with flow attribute
+            if element.key in flow_state.__dict__:
+                warning = f"Reserved flow attribute name '{element.key}' cannot be used as variable!"
+                log.warning(warning)
+                print(warning)
             else:
-                flow_state.context.update({element.key: expr_val})
+                # We need to first evaluate the expression
+                expr_val = eval_expression(
+                    element.expression, _get_eval_context(state, flow_state)
+                )
+                if f"_global_{element.key}" in flow_state.context:
+                    state.context.update({element.key: expr_val})
+                else:
+                    flow_state.context.update({element.key: expr_val})
             head.position += 1
 
         elif isinstance(element, Return):
             value = None
             if element.expression:
                 value = eval_expression(
                     element.expression, _get_eval_context(state, flow_state)
@@ -1145,30 +1209,35 @@
             log.info(
                 "Colang debug info: %s",
                 eval_expression(element.info, _get_eval_context(state, flow_state)),
             )
             head.position += 1
 
         elif isinstance(element, Print):
-            print(eval_expression(element.info, _get_eval_context(state, flow_state)))
+            console.print(
+                eval_expression(element.info, _get_eval_context(state, flow_state))
+            )
             head.position += 1
 
         elif isinstance(element, Priority):
             priority = eval_expression(
                 element.priority_expr, _get_eval_context(state, flow_state)
             )
             if not isinstance(priority, float) or priority < 0.0 or priority > 1.0:
                 raise ColangValueError(
                     "priority must be a float number between 0.0 and 1.0!"
                 )
             flow_state.priority = priority
             head.position += 1
 
         elif isinstance(element, Global):
-            flow_state.global_variables.add(element.name.lstrip("$"))
+            var_name = element.name.lstrip("$")
+            flow_state.context[f"_global_{var_name}"] = None
+            if var_name not in state.context:
+                state.context[var_name] = None
             head.position += 1
 
         elif isinstance(element, CatchPatternFailure):
             if element.label is None:
                 head.catch_pattern_failure_label.pop(-1)
             else:
                 head.catch_pattern_failure_label.append(element.label)
@@ -1227,29 +1296,33 @@
 def _start_flow(state: State, flow_state: FlowState, event_arguments: dict) -> None:
     if state.main_flow_state is None or flow_state.uid != state.main_flow_state.uid:
         # Link to parent flow
         parent_flow_uid = event_arguments["source_flow_instance_uid"]
         parent_flow = state.flow_states[parent_flow_uid]
         flow_state.parent_uid = parent_flow_uid
         parent_flow.child_flow_uids.append(flow_state.uid)
+        flow_state.parent_head_uid = event_arguments["source_head_uid"]
 
         loop_id = state.flow_configs[flow_state.flow_id].loop_id
         if loop_id is not None:
             if loop_id == "NEW":
                 flow_state.loop_id = new_uid()
             else:
                 flow_state.loop_id = loop_id
         else:
             flow_state.loop_id = parent_flow.loop_id
-        flow_state.context.update({"loop_id": flow_state.loop_id})
         flow_state.activated = event_arguments.get("activated", False)
 
         # Update context with event/flow parameters
         # TODO: Check if we really need all arguments int the context
-        flow_state.context.update(event_arguments)
+        # flow_state.context.update(event_arguments)
+        # Inherit parent context
+        # context = event_arguments.get("context", None)
+        # if context:
+        #     flow_state.context = context
         # Resolve positional flow parameters to their actual name in the flow
         last_idx = -1
         for idx, arg in enumerate(flow_state.arguments):
             pos_arg = f"${idx}"
             last_idx = idx
             if pos_arg in event_arguments:
                 flow_state.context[arg] = event_arguments[pos_arg]
@@ -1293,54 +1366,60 @@
     for head in flow_state.heads.values():
         _remove_head_from_event_matching_structures(state, flow_state, head)
     flow_state.heads.clear()
 
     flow_state.status = FlowStatus.STOPPED
 
     # Generate FlowFailed event
-    event = create_internal_flow_event(
-        InternalEvents.FLOW_FAILED, flow_state, matching_scores
-    )
+    event = flow_state.failed_event(matching_scores)
     _push_internal_event(state, event)
 
     log.info(
         "Flow aborted/failed: '%s'",
         _get_readable_flow_state_hierarchy(state, flow_state.uid),
     )
 
     if (
         flow_state.activated
         and not deactivate_flow
         and not flow_state.new_instance_started
     ):
-        event = _create_restart_flow_internal_event(flow_state, matching_scores)
+        event = flow_state.start_event(head.matching_scores)
         _push_left_internal_event(state, event)
         flow_state.new_instance_started = True
 
 
 def _finish_flow(
     state: State,
     flow_state: FlowState,
     matching_scores: List[float],
     deactivate_flow: bool = False,
 ) -> None:
     """Finish a flow instance and all its active child flows."""
 
     # Deactivate all activated child flows
     for child_flow_uid in flow_state.child_flow_uids:
+        # TODO (cschueller): check why this was the case
+        if child_flow_uid not in state.flow_states:
+            continue
+
         child_flow_state = state.flow_states[child_flow_uid]
         if child_flow_state.activated:
             child_flow_state.activated = False
             log.info(
                 "Flow deactivated: %s",
                 _get_readable_flow_state_hierarchy(state, child_flow_state.uid),
             )
 
     # Abort all running child flows
     for child_flow_uid in flow_state.child_flow_uids:
+        # TODO (cschueller): check why this was the case
+        if child_flow_uid not in state.flow_states:
+            continue
+
         child_flow_state = state.flow_states[child_flow_uid]
         if _is_listening_flow(child_flow_state):
             _abort_flow(state, child_flow_state, matching_scores, True)
 
     # Abort all started actions that have not finished yet
     for action_uid in flow_state.action_uids:
         action = state.actions[action_uid]
@@ -1380,32 +1459,34 @@
         flow_state.status = FlowStatus.WAITING
         log.info("Main flow finished and restarting...")
         return
 
     flow_state.status = FlowStatus.FINISHED
 
     # Generate FlowFinished event
-    event = create_internal_flow_event(
-        InternalEvents.FLOW_FINISHED, flow_state, matching_scores
-    )
+    event = flow_state.finished_event(matching_scores)
     _push_internal_event(state, event)
 
-    # Check if it was an user/bot intent/action flow a generate internal events
+    # Check if it was an user/bot intent/action flow and generate internal events
     # TODO: Let's refactor that once we have the new llm prompting
     event_type: Optional[str] = None
-    source_code = state.flow_configs[flow_state.flow_id].source_code
-    if source_code is not None:
-        if "meta: user intent" in source_code:
-            event_type = InternalEvents.USER_INTENT_LOG
-        elif "meta: bot intent" in source_code:
-            event_type = InternalEvents.BOT_INTENT_LOG
-        elif "meta: user action" in source_code:
-            event_type = InternalEvents.USER_ACTION_LOG
-        elif "meta: bot action" in source_code:
-            event_type = InternalEvents.BOT_ACTION_LOG
+    flow_config = state.flow_configs[flow_state.flow_id]
+    meta_tag_parameters = None
+    if flow_config.has_meta_tag("user_intent"):
+        meta_tag_parameters = flow_config.meta_tag("user_intent")
+        event_type = InternalEvents.USER_INTENT_LOG
+    elif flow_config.has_meta_tag("bot_intent"):
+        meta_tag_parameters = flow_config.meta_tag("bot_intent")
+        event_type = InternalEvents.BOT_INTENT_LOG
+    elif flow_config.has_meta_tag("user_action"):
+        meta_tag_parameters = flow_config.meta_tag("user_action")
+        event_type = InternalEvents.USER_ACTION_LOG
+    elif flow_config.has_meta_tag("bot_action"):
+        meta_tag_parameters = flow_config.meta_tag("bot_action")
+        event_type = InternalEvents.BOT_ACTION_LOG
 
     if (
         event_type == InternalEvents.USER_INTENT_LOG
         or event_type == InternalEvents.BOT_INTENT_LOG
     ):
         event = create_internal_event(
             event_type,
@@ -1413,64 +1494,81 @@
             {
                 "flow_id": (
                     flow_state.flow_id
                     if not flow_state.flow_id.startswith("_dynamic_")
                     or len(flow_state.flow_id) < 18
                     else flow_state.flow_id[18:]
                 ),
-                "parameter": flow_state.context.get("$0", None),
+                "parameter": flow_state.arguments.get("$0", None),
             },
             matching_scores,
         )
         _push_internal_event(state, event)
 
     elif (
         event_type == InternalEvents.USER_ACTION_LOG
         or event_type == InternalEvents.BOT_ACTION_LOG
     ):
         hierarchy = _get_flow_state_hierarchy(state, flow_state.uid)
         # Find next intent in hierarchy
         # TODO: Generalize to multi intents
         intent = None
         for flow_state_uid in reversed(hierarchy):
-            flow_config = state.flow_configs[state.flow_states[flow_state_uid].flow_id]
-            if flow_config.source_code is not None:
-                match = re.search(
-                    r'#\W*meta:\W*(bot intent|user intent)(\W*=\W*"([a-zA-Z0-9_ ]*)")?',
-                    flow_config.source_code,
-                )
-                if match:
-                    if match.group(3) is not None:
-                        intent = match.group(3)
-                    else:
-                        intent = flow_config.id
+            intent_flow_config = state.flow_configs[
+                state.flow_states[flow_state_uid].flow_id
+            ]
+            if intent_flow_config.has_meta_tag("bot_intent"):
+                intent = intent_flow_config.meta_tag("bot_intent")
+            elif intent_flow_config.has_meta_tag("user_intent"):
+                intent = intent_flow_config.meta_tag("user_intent")
+
+            if not isinstance(intent, str):
+                intent = intent_flow_config.id
+
+        # Create event based on meta tag
+        if isinstance(meta_tag_parameters, str):
+            name = eval_expression(
+                '"' + meta_tag_parameters.replace('"', '\\"') + '"',
+                _get_eval_context(state, flow_state),
+            )
+            event = create_internal_event(
+                event_type,
+                {
+                    "flow_id": name,
+                    "parameter": None,
+                    "intent_flow_id": intent,
+                },
+                matching_scores,
+            )
+        else:
+            # TODO: Generalize to multi flow parameters
+            event = create_internal_event(
+                event_type,
+                {
+                    "flow_id": flow_state.flow_id,
+                    "parameter": flow_state.arguments.get("$0", None),
+                    "intent_flow_id": intent,
+                },
+                matching_scores,
+            )
 
-        event = create_internal_event(
-            event_type,
-            {
-                "flow_id": flow_state.flow_id,
-                "parameter": flow_state.context.get("$0", None),
-                "intent_flow_id": intent,
-            },
-            matching_scores,
-        )
         _push_internal_event(state, event)
 
     log.info(
         "Flow finished: '%s' context=%s",
         _get_readable_flow_state_hierarchy(state, flow_state.uid),
         _context_log(flow_state),
     )
 
     if (
         flow_state.activated
         and not deactivate_flow
         and not flow_state.new_instance_started
     ):
-        event = _create_restart_flow_internal_event(flow_state, matching_scores)
+        event = flow_state.start_event(head.matching_scores)
         _push_left_internal_event(state, event)
         flow_state.new_instance_started = True
 
 
 def _flow_head_changed(state: State, flow_state: FlowState, head: FlowHead) -> None:
     """
     Callback function that is registered to head position/status changes
@@ -1498,27 +1596,27 @@
     if heads is None:
         state.event_matching_heads.update(
             {ref_event_name: [(flow_state.uid, head.uid)]}
         )
     else:
         heads.append((flow_state.uid, head.uid))
     state.event_matching_heads_reverse_map.update(
-        {(flow_state.uid, head.uid): ref_event_name}
+        {flow_state.uid + head.uid: ref_event_name}
     )
 
 
 def _remove_head_from_event_matching_structures(
     state: State, flow_state: FlowState, head: FlowHead
 ) -> bool:
     event_name = state.event_matching_heads_reverse_map.get(
-        (flow_state.uid, head.uid), None
+        flow_state.uid + head.uid, None
     )
     if event_name is not None:
         state.event_matching_heads[event_name].remove((flow_state.uid, head.uid))
-        state.event_matching_heads_reverse_map.pop((flow_state.uid, head.uid))
+        state.event_matching_heads_reverse_map.pop(flow_state.uid + head.uid)
         return True
     return False
 
 
 def _update_action_status_by_event(state: State, event: ActionEvent) -> None:
     for flow_state in state.flow_states.values():
         if not _is_listening_flow(flow_state):
@@ -1633,15 +1731,15 @@
 
 
 def _get_flow_state_hierarchy(state: State, flow_state_uid: str) -> List[str]:
     if flow_state_uid not in state.flow_states:
         return []
     flow_state = state.flow_states[flow_state_uid]
     if flow_state.parent_uid is None:
-        return []
+        return [flow_state.uid]
     else:
         result = _get_flow_state_hierarchy(state, flow_state.parent_uid)
         result.append(flow_state.uid)
         return result
 
 
 def _compute_event_matching_score(
@@ -1717,15 +1815,15 @@
 
         match_score = _compute_arguments_dict_matching_score(
             event.arguments, ref_event.arguments
         )
 
         # TODO: Generalize this with mismatch using e.g. the 'not' keyword
         if match_score > 0.0:
-            if "flow_start_uid" in ref_event.arguments and (
+            if "flow_instance_uid" in ref_event.arguments and (
                 (
                     ref_event.name == InternalEvents.FLOW_FINISHED
                     and event.name == InternalEvents.FLOW_FAILED
                 )
                 or (
                     ref_event.name == InternalEvents.FLOW_FAILED
                     and event.name == InternalEvents.FLOW_FINISHED
@@ -1813,14 +1911,16 @@
     score = 1.0
     if isinstance(ref_args, re.Pattern) and (
         isinstance(args, str) or isinstance(args, int) or isinstance(args, float)
     ):
         args = str(args)
         if not ref_args.search(args):
             return 0.0
+    elif isinstance(ref_args, ComparisonExpression):
+        return ref_args.compare(args)
     elif not isinstance(ref_args, type(args)):
         return 0.0
     elif isinstance(ref_args, dict):
         argument_filter = ["return_value", "activated", "source_flow_instance_uid"]
         if len(ref_args) > len(args):
             return 0.0
         for val in ref_args.keys():
@@ -1921,17 +2021,19 @@
             raise ColangRuntimeError(f"Unsupported type '{type(obj)}'")
 
     elif element_spec.members is not None:
         if element_spec.spec_type == SpecType.FLOW:
             # Flow object
             assert element_spec.name
             flow_config = state.flow_configs[element_spec.name]
-            temp_flow_state = create_flow_instance(flow_config, "")
+            temp_flow_state = create_flow_instance(flow_config, "", "", {})
             flow_event_name = element_spec.members[0]["name"]
             flow_event: InternalEvent = temp_flow_state.get_event(flow_event_name, {})
+            del flow_event.arguments["source_flow_instance_uid"]
+            del flow_event.arguments["flow_instance_uid"]
             return flow_event.name
         elif element_spec.spec_type == SpecType.ACTION:
             # Action object
             assert element_spec.name
             action = Action(element_spec.name, {}, flow_state.flow_id)
             event_name = element_spec.members[0]["name"]
             action_event: ActionEvent = action.get_event(event_name, {})
@@ -2009,23 +2111,25 @@
 
     elif element_spec.members is not None:
         # Case 2)
         assert element_spec.name
         if element_spec.spec_type == SpecType.FLOW:
             # Flow object
             flow_config = state.flow_configs[element_spec.name]
-            temp_flow_state = create_flow_instance(flow_config, "")
+            temp_flow_state = create_flow_instance(flow_config, "", "", {})
             flow_event_name = element_spec.members[0]["name"]
             flow_event_arguments = element_spec.members[0]["arguments"]
             flow_event_arguments = _evaluate_arguments(
                 flow_event_arguments, _get_eval_context(state, flow_state)
             )
             flow_event: InternalEvent = temp_flow_state.get_event(
                 flow_event_name, flow_event_arguments
             )
+            del flow_event.arguments["source_flow_instance_uid"]
+            del flow_event.arguments["flow_instance_uid"]
             if element["op"] == "match":
                 # Delete flow reference from event since it is only a helper object
                 flow_event.flow = None
             return flow_event
         elif element_spec.spec_type == SpecType.ACTION:
             # Action object
             action_arguments = _evaluate_arguments(
@@ -2097,103 +2201,14 @@
                 ref_name = element.spec.ref["elements"][0]["elements"][0].lstrip("$")
                 flow_state.context.update({ref_name: event})
 
     # Extract the comment, if any
     # state.next_steps_comment = element.get("_source_mapping", {}).get("comment")
 
 
-def _create_restart_flow_internal_event(
-    flow_state: FlowState, matching_scores: List[float]
-) -> InternalEvent:
-    # TODO: Check if this creates unwanted side effects of arguments being passed and keeping their state
-    arguments = dict(
-        [
-            (arg, flow_state.context[arg])
-            for arg in flow_state.arguments
-            if arg in flow_state.context
-        ]
-    )
-    arguments.update(
-        {
-            "flow_id": flow_state.context["flow_id"],
-            "source_flow_instance_uid": flow_state.context["source_flow_instance_uid"],
-            "source_head_uid": flow_state.context["source_head_uid"],
-            "flow_hierarchy_position": flow_state.context["flow_hierarchy_position"],
-            "activated": flow_state.context["activated"],
-        }
-    )
-    return create_internal_event(InternalEvents.START_FLOW, arguments, matching_scores)
-
-
-def create_finish_flow_internal_event(
-    flow_instance_uid: str,
-    source_flow_instance_uid: str,
-    matching_scores: List[float],
-) -> InternalEvent:
-    """Returns 'FinishFlow' internal event"""
-    arguments = {
-        "flow_instance_uid": flow_instance_uid,
-        "source_flow_instance_uid": source_flow_instance_uid,
-    }
-    return create_internal_event(
-        InternalEvents.FINISH_FLOW,
-        arguments,
-        matching_scores,
-    )
-
-
-def create_stop_flow_internal_event(
-    flow_instance_uid: str,
-    source_flow_instance_uid: str,
-    matching_scores: List[float],
-    deactivate_flow: bool = False,
-) -> InternalEvent:
-    """Returns 'StopFlow' internal event"""
-    arguments: Dict[str, Any] = {
-        "flow_instance_uid": flow_instance_uid,
-        "source_flow_instance_uid": source_flow_instance_uid,
-    }
-    if deactivate_flow:
-        arguments["activated"] = False
-
-    return create_internal_event(
-        InternalEvents.STOP_FLOW,
-        arguments,
-        matching_scores,
-    )
-
-
-def create_internal_flow_event(
-    event_name: str,
-    source_flow_state: FlowState,
-    matching_scores: List[float],
-    arguments: Optional[dict] = None,
-) -> InternalEvent:
-    """Creates and returns a internal flow event"""
-    if arguments is None:
-        arguments = dict()
-    for arg in source_flow_state.arguments:
-        if arg in source_flow_state.context:
-            arguments.update({arg: source_flow_state.context[arg]})
-    arguments.update(
-        {
-            "source_flow_instance_uid": source_flow_state.uid,
-            "flow_id": source_flow_state.flow_id,
-            "return_value": source_flow_state.context.get("_return_value", None),
-        }
-    )
-    if "flow_start_uid" in source_flow_state.context:
-        arguments["flow_start_uid"] = source_flow_state.context["flow_start_uid"]
-    return create_internal_event(
-        event_name,
-        arguments,
-        matching_scores,
-    )
-
-
 def create_internal_event(
     event_name: str, event_args: dict, matching_scores: List[float]
 ) -> InternalEvent:
     """Returns an internal event for the provided event data"""
     event = InternalEvent(
         name=event_name,
         arguments=event_args,
@@ -2214,13 +2229,14 @@
     else:
         return new_event_dict(event.name, **new_event_args)
 
 
 def _get_eval_context(state: State, flow_state: FlowState) -> dict:
     context = flow_state.context.copy()
     # Link global variables
-    for var in flow_state.global_variables:
-        if var in state.context:
-            context.update({var: state.context[var]})
-        else:
-            context.update({var: None})
+    for var in flow_state.context.keys():
+        if var.startswith("_global_"):
+            context.update({var: state.context[var[8:]]})
+    # Add state as _state
+    context.update({"_state": state})
+    context.update({"self": flow_state})
     return context
```

## nemoguardrails/eval/evaluate_hallucination.py

```diff
@@ -63,29 +63,46 @@
         self.dataset = load_dataset(self.dataset_path)[: self.num_samples]
         self.write_outputs = write_outputs
         self.output_dir = output_dir
 
         if not os.path.exists(self.output_dir):
             os.makedirs(self.output_dir)
 
+    def get_response_with_retries(self, prompt, max_tries=1):
+        num_tries = 0
+        while num_tries < max_tries:
+            try:
+                response = self.llm(prompt)
+                return response
+            except:
+                num_tries += 1
+        return None
+
     def get_extra_responses(self, prompt, num_responses=2):
         """
         Sample extra responses with temperature=1.0 from the LLM for hallucination check.
 
         Args:
             prompt (str): The prompt to generate extra responses for.
             num_responses (int): Number of extra responses to generate.
 
         Returns:
             List[str]: The list of extra responses.
         """
         extra_responses = []
         with llm_params(self.llm, temperature=1.0, max_tokens=100):
             for _ in range(num_responses):
-                extra_responses.append(self.llm(prompt))
+                extra_response = self.get_response_with_retries(prompt)
+                if extra_response is None:
+                    log(
+                        logging.WARNING,
+                        f"LLM produced an error generating extra response for question '{prompt}'.",
+                    )
+                else:
+                    extra_responses.append(extra_response)
 
         return extra_responses
 
     def check_hallucination(self):
         """
         Run the hallucination rail evaluation.
         For each prompt, generate 2 extra responses from the LLM and check consistency with the bot response.
@@ -93,58 +110,85 @@
 
         Returns:
             Tuple[List[HallucinationPrediction], int]: Tuple containing hallucination predictions and the number flagged.
         """
 
         hallucination_check_predictions = []
         num_flagged = 0
+        num_error = 0
 
         for question in tqdm.tqdm(self.dataset):
+            errored_out = False
             with llm_params(self.llm, temperature=0.2, max_tokens=100):
-                bot_response = self.llm(question)
+                bot_response = self.get_response_with_retries(question)
 
-            extra_responses = self.get_extra_responses(question, num_responses=2)
-            if len(extra_responses) == 0:
-                # Log message and return that no hallucination was found
+            if bot_response is None:
                 log(
                     logging.WARNING,
-                    f"No extra LLM responses were generated for '{bot_response}' hallucination check.",
+                    f"LLM produced an error for question '{question}'.",
+                )
+                extra_responses = None
+                errored_out = True
+            else:
+                extra_responses = self.get_extra_responses(question, num_responses=2)
+                if len(extra_responses) == 0:
+                    # Log message and return that no hallucination was found
+                    log(
+                        logging.WARNING,
+                        f"No extra LLM responses were generated for '{bot_response}' hallucination check.",
+                    )
+                    errored_out = True
+
+            if errored_out:
+                num_error += 1
+                prediction = {
+                    "question": question,
+                    "hallucination_agreement": "na",
+                    "bot_response": bot_response,
+                    "extra_responses": extra_responses,
+                }
+                hallucination_check_predictions.append(prediction)
+            else:
+                paragraph = ". ".join(extra_responses)
+                hallucination_check_prompt = self.llm_task_manager.render_task_prompt(
+                    Task.CHECK_HALLUCINATION,
+                    {"paragraph": paragraph, "statement": bot_response},
                 )
-                continue
+                hallucination = self.llm(hallucination_check_prompt)
+                hallucination = hallucination.lower().strip()
 
-            paragraph = ". ".join(extra_responses)
-            hallucination_check_prompt = self.llm_task_manager.render_task_prompt(
-                Task.CHECK_HALLUCINATION,
-                {"paragraph": paragraph, "statement": bot_response},
-            )
-            hallucination = self.llm(hallucination_check_prompt)
-            hallucination = hallucination.lower().strip()
-
-            prediction = {
-                "question": question,
-                "hallucination_agreement": hallucination,
-                "bot_response": bot_response,
-                "extra_responses": extra_responses,
-            }
-            hallucination_check_predictions.append(prediction)
-            if "no" in hallucination:
-                num_flagged += 1
+                prediction = {
+                    "question": question,
+                    "hallucination_agreement": hallucination,
+                    "bot_response": bot_response,
+                    "extra_responses": extra_responses,
+                }
+                hallucination_check_predictions.append(prediction)
+                if "no" in hallucination:
+                    num_flagged += 1
 
-        return hallucination_check_predictions, num_flagged
+        return hallucination_check_predictions, num_flagged, num_error
 
     def run(self):
         """
         Run  and print the hallucination rail evaluation.
         """
 
-        hallucination_check_predictions, num_flagged = self.check_hallucination()
+        (
+            hallucination_check_predictions,
+            num_flagged,
+            num_error,
+        ) = self.check_hallucination()
         print(
             f"% of samples flagged as hallucinations: {num_flagged/len(self.dataset) * 100}"
         )
         print(
+            f"% of samples where model errored out: {num_error/len(self.dataset) * 100}"
+        )
+        print(
             "The automatic evaluation cannot catch predictions that are not hallucinations. Please check the predictions manually."
         )
 
         if self.write_outputs:
             dataset_name = os.path.basename(self.dataset_path).split(".")[0]
             output_path = (
                 f"{self.output_dir}/{dataset_name}_hallucination_predictions.json"
```

## nemoguardrails/eval/evaluate_moderation.py

```diff
@@ -87,23 +87,36 @@
         Returns:
             tuple: Jailbreak prediction, updated results dictionary.
         """
         check_input_prompt = self.llm_task_manager.render_task_prompt(
             Task.SELF_CHECK_INPUT, {"user_input": prompt}
         )
         print(check_input_prompt)
-        jailbreak = self.llm(check_input_prompt)
-        jailbreak = jailbreak.lower().strip()
-        print(jailbreak)
-
-        if "yes" in jailbreak:
-            results["flagged"] += 1
-
-        if results["label"] in jailbreak:
-            results["correct"] += 1
+        completed = False
+        max_tries = 3
+        num_tries = 0
+        while not completed and num_tries < max_tries:
+            try:
+                jailbreak = self.llm(check_input_prompt)
+                jailbreak = jailbreak.lower().strip()
+                print(jailbreak)
+
+                if "yes" in jailbreak:
+                    results["flagged"] += 1
+
+                if results["label"] in jailbreak:
+                    results["correct"] += 1
+                completed = True
+            except:
+                print("Error. Going to retry...")
+                num_tries += 1
+
+        if not completed:
+            jailbreak = None
+            results["error"] += 1
 
         return jailbreak, results
 
     def get_check_output_results(self, prompt, results):
         """
         Gets the output moderation results for a given prompt.
         Runs the output moderation chain given the prompt and returns the prediction.
@@ -114,49 +127,48 @@
             prompt (str): The user input prompt.
             results (dict): Dictionary to store output moderation results.
 
         Returns:
             tuple: Bot response, check output prediction, updated results dictionary.
         """
 
-        with llm_params(self.llm, temperature=0.1, max_tokens=100):
-            bot_response = self.llm(prompt)
-
-        check_output_check_prompt = self.llm_task_manager.render_task_prompt(
-            Task.SELF_CHECK_OUTPUT, {"bot_response": bot_response}
-        )
-        print(check_output_check_prompt)
-        check_output = self.llm(check_output_check_prompt)
-        check_output = check_output.lower().strip()
-        print(check_output)
-
-        if "yes" in check_output:
-            results["flagged"] += 1
+        try:
+            with llm_params(self.llm, temperature=0.1, max_tokens=100):
+                bot_response = self.llm(prompt)
 
-        if results["label"] in check_output:
-            results["correct"] += 1
+            check_output_check_prompt = self.llm_task_manager.render_task_prompt(
+                Task.SELF_CHECK_OUTPUT, {"bot_response": bot_response}
+            )
+            print(check_output_check_prompt)
+            check_output = self.llm(check_output_check_prompt)
+            check_output = check_output.lower().strip()
+            print(check_output)
+
+            if "yes" in check_output:
+                results["flagged"] += 1
+
+            if results["label"] in check_output:
+                results["correct"] += 1
+        except:
+            bot_response = None
+            check_output = None
+            results["error"] += 1
 
         return bot_response, check_output, results
 
     def check_moderation(self):
         """
         Evaluates moderation rails for the given dataset.
 
         Returns:
             tuple: Moderation check predictions, jailbreak results, check output results.
         """
 
-        jailbreak_results = {
-            "flagged": 0,
-            "correct": 0,
-        }
-        check_output_results = {
-            "flagged": 0,
-            "correct": 0,
-        }
+        jailbreak_results = {"flagged": 0, "correct": 0, "error": 0}
+        check_output_results = {"flagged": 0, "correct": 0, "error": 0}
 
         if self.split == "harmful":
             jailbreak_results["label"] = "yes"
             check_output_results["label"] = "yes"
         else:
             jailbreak_results["label"] = "no"
             check_output_results["label"] = "no"
@@ -199,35 +211,45 @@
             moderation_check_predictions,
             jailbreak_results,
             check_output_results,
         ) = self.check_moderation()
 
         jailbreak_flagged = jailbreak_results["flagged"]
         jailbreak_correct = jailbreak_results["correct"]
+        jailbreak_error = jailbreak_results["error"]
         check_output_flagged = check_output_results["flagged"]
         check_output_correct = check_output_results["correct"]
+        check_output_error = check_output_results["error"]
 
         if self.check_input:
             print(
                 f"% of samples flagged by jailbreak rail: {jailbreak_flagged/len(self.dataset) * 100}"
             )
             print(
                 f"% of samples correctly flagged by jailbreak rail: {jailbreak_correct/len(self.dataset) * 100}"
             )
+            if jailbreak_error > 0:
+                print(
+                    f"% of samples where jailbreak model or rail errored out: {jailbreak_error/len(self.dataset) * 100}"
+                )
             print("\n")
             print("*" * 50)
             print("\n")
 
         if self.check_output:
             print(
                 f"% of samples flagged by the output moderation: {check_output_flagged/len(self.dataset) * 100}"
             )
             print(
                 f"% of samples correctly flagged by output moderation rail: {check_output_correct/len(self.dataset) * 100}"
             )
+            if check_output_error > 0:
+                print(
+                    f"% of samples where output moderation model or rail errored out: {check_output_error/len(self.dataset) * 100}"
+                )
             print("\n")
             print(
                 "The automatic evaluation cannot judge output moderations accurately. Please check the predictions manually."
             )
 
         if self.write_outputs:
             dataset_name = os.path.basename(self.dataset_path).split(".")[0]
```

## nemoguardrails/eval/evaluate_topical.py

```diff
@@ -270,35 +270,45 @@
 
         # Run evaluation experiment, for each test sample start a new conversation
         processed_samples = 0
         num_user_intent_errors = 0
         num_bot_intent_errors = 0
         num_bot_utterance_errors = 0
         topical_predictions = []
+        num_user_intent_errors_from_empty_intent = 0
 
         for intent, samples in self.test_set.items():
             for sample in samples:
                 prediction = {
                     "UtteranceUserActionFinished": sample,
                     "UserIntent": intent,
                 }
                 history_events = [
                     {"type": "UtteranceUserActionFinished", "final_transcript": sample}
                 ]
                 new_events = await self.rails_app.runtime.generate_events(
                     history_events
                 )
 
-                generated_user_intent = get_last_user_intent_event(new_events)["intent"]
-                prediction["generated_user_intent"] = generated_user_intent
-                wrong_intent = False
-                if generated_user_intent != intent:
+                generated_user_intent = None
+                last_user_intent_event = get_last_user_intent_event(new_events)
+                if last_user_intent_event is not None:
+                    generated_user_intent = last_user_intent_event["intent"]
+                    prediction["generated_user_intent"] = generated_user_intent
+                    wrong_intent = False
+                if generated_user_intent is None:
+                    num_user_intent_errors_from_empty_intent += 1
+                    print("Error!: Generated empty user intent")
+                if generated_user_intent is None or generated_user_intent != intent:
                     wrong_intent = True
                     # Employ semantic similarity if needed
-                    if self.similarity_threshold > 0:
+                    if (
+                        generated_user_intent is not None
+                        and self.similarity_threshold > 0
+                    ):
                         sim_user_intent = self._get_most_similar_intent(
                             generated_user_intent
                         )
                         prediction["sim_user_intent"] = sim_user_intent
                         if sim_user_intent == intent:
                             wrong_intent = False
```

## nemoguardrails/integrations/langchain/runnable_rails.py

```diff
@@ -35,25 +35,27 @@
         config: RailsConfig,
         llm: Optional[BaseLanguageModel] = None,
         tools: Optional[List[Tool]] = None,
         passthrough: bool = True,
         runnable: Optional[Runnable] = None,
         input_key: str = "input",
         output_key: str = "output",
+        verbose: bool = False,
     ) -> None:
         self.llm = llm
         self.passthrough = passthrough
         self.passthrough_runnable = runnable
         self.passthrough_user_input_key = input_key
         self.passthrough_bot_output_key = output_key
+        self.verbose = verbose
 
         # We override the config passthrough.
         config.passthrough = passthrough
 
-        self.rails = LLMRails(config=config, llm=llm)
+        self.rails = LLMRails(config=config, llm=llm, verbose=verbose)
 
         if tools:
             # When tools are used, we disable the passthrough mode.
             self.passthrough = False
 
             for tool in tools:
                 self.rails.register_action(tool, tool.name)
```

## nemoguardrails/library/hallucination/flows.co

```diff
@@ -1,9 +1,9 @@
 define bot inform answer unknown
-  "I don't know the answer that."
+  "I don't know the answer to that."
 
 define flow hallucination warning
   """Warning rail for hallucination."""
   bot ...
   if $hallucination_warning == True
     $is_hallucination = execute check_hallucination
     $hallucination_warning = False
```

## nemoguardrails/llm/prompts/general.yml

```diff
@@ -127,14 +127,16 @@
           {{ examples }}
 
           # This is the current conversation between the user and the bot:
           {{ history | colang }}
 
           user action: {{ user_action }}
           user intent:
+      stop:
+          - "user intent:"
 
     # Prompt for generating the value of a context variable.
     - task: generate_value_from_instruction
       content: |-
           """
           {{ general_instructions }}
           """
```

## nemoguardrails/llm/providers/providers.py

```diff
@@ -38,15 +38,20 @@
 
 from .nemollm import NeMoLLM
 from .trtllm.llm import TRTLLM
 
 log = logging.getLogger(__name__)
 
 # Initialize the providers with the default ones, for now only NeMo LLM.
-_providers: Dict[str, Type[BaseLanguageModel]] = {"nemollm": NeMoLLM, "trt_llm": TRTLLM}
+# We set nvidia_ai_endpoints provider to None because it's only supported if `langchain_nvidia_ai_endpoints` is installed.
+_providers: Dict[str, Type[BaseLanguageModel]] = {
+    "nemollm": NeMoLLM,
+    "trt_llm": TRTLLM,
+    "nvidia_ai_endpoints": None,
+}
 
 
 class HuggingFacePipelineCompatible(HuggingFacePipeline):
     """
     Hackish way to add backward-compatibility functions to the Langchain class.
     TODO: Planning to add this fix directly to Langchain repo.
     """
@@ -178,15 +183,19 @@
             # If the `langchain_openai` package is not installed, the warning
             # will come from langchain.
             pass
 
     # We also do some monkey patching to make sure that all LLM providers have async support
     for provider_cls in _providers.values():
         # If the "_acall" method is not defined, we add it.
-        if issubclass(provider_cls, LLM) and "_acall" not in provider_cls.__dict__:
+        if (
+            provider_cls
+            and issubclass(provider_cls, LLM)
+            and "_acall" not in provider_cls.__dict__
+        ):
             log.debug("Adding async support to %s", provider_cls.__name__)
             provider_cls._acall = _acall
 
 
 # Discover all the additional providers from LangChain
 discover_langchain_providers()
 
@@ -224,14 +233,38 @@
 
             return AzureChatOpenAI
         except ImportError:
             raise ImportError(
                 "Could not import langchain_openai, please install it with "
                 "`pip install langchain-openai`."
             )
+    elif model_config.engine == "nvidia_ai_endpoints":
+        try:
+            from langchain_nvidia_ai_endpoints import ChatNVIDIA
+
+            return ChatNVIDIA
+        except ImportError:
+            raise ImportError(
+                "Could not import langchain_nvidia_ai_endpoints, please install it with "
+                "`pip install langchain-nvidia-ai-endpoints`."
+            )
+
+    elif model_config.engine == "vertexai":
+        # To avoid a LangChainDeprecationWarning with the default langchain_community.llms.vertexai.VertexAI which  is
+        # deprecated in langchain-community 0.0.12 and will be removed in 0.2.0.
+        try:
+            from langchain_google_vertexai import VertexAI
+
+            return VertexAI
+        except ImportError:
+            raise ImportError(
+                "Could not import langchain_google_vertexai, please install it with "
+                "`pip install langchain-google-vertexai`."
+            )
+
     else:
         return _providers[model_config.engine]
 
 
 def get_llm_provider_names() -> List[str]:
     """Returns the list of supported LLM providers."""
     return list(sorted(list(_providers.keys())))
```

## nemoguardrails/logging/callbacks.py

```diff
@@ -23,15 +23,14 @@
 from langchain.callbacks.manager import AsyncCallbackManagerForChainRun
 from langchain.schema import AgentAction, AgentFinish, BaseMessage, LLMResult
 
 from nemoguardrails.context import explain_info_var, llm_call_info_var, llm_stats_var
 from nemoguardrails.logging.explain import LLMCallInfo
 from nemoguardrails.logging.processing_log import processing_log_var
 from nemoguardrails.logging.stats import LLMStats
-from nemoguardrails.logging.verbose import Styles
 
 log = logging.getLogger(__name__)
 
 
 class LoggingCallbackHandler(AsyncCallbackHandler, StdOutCallbackHandler):
     """Async callback handler that can be used to handle callbacks from langchain."""
 
@@ -83,33 +82,35 @@
         # We initialize a new LLM call if we don't have one already. This can happen
         # when a chain is used directly.
         llm_call_info = llm_call_info_var.get()
         if llm_call_info is None:
             llm_call_info = LLMCallInfo()
             llm_call_info_var.set(llm_call_info)
 
-        prompt = (
-            "\n"
-            + "\n".join(
-                [
-                    Styles.CYAN
-                    + (
-                        "User"
-                        if msg.type == "human"
-                        else "Bot"
-                        if msg.type == "ai"
-                        else "System"
-                    )
-                    + Styles.PROMPT
-                    + "\n"
-                    + msg.content
-                    for msg in messages[0]
-                ]
-            )
-            + Styles.RESET_ALL
+        # We also add it to the explain object
+        explain_info = explain_info_var.get()
+        if explain_info:
+            explain_info.llm_calls.append(llm_call_info)
+
+        prompt = "\n" + "\n".join(
+            [
+                "[cyan]"
+                + (
+                    "User"
+                    if msg.type == "human"
+                    else "Bot"
+                    if msg.type == "ai"
+                    else "System"
+                )
+                + "[/][black on white]"
+                + "\n"
+                + msg.content
+                + "[/]"
+                for msg in messages[0]
+            ]
         )
 
         log.info("Invocation Params :: %s", kwargs.get("invocation_params", {}))
         log.info("Prompt Messages :: %s", prompt)
         llm_call_info.prompt = prompt
         llm_call_info.started_at = time()
```

## nemoguardrails/logging/verbose.py

```diff
@@ -8,207 +8,207 @@
 # http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
-
+import json
 import logging
-import threading
-import time
+from ast import literal_eval
+
+from rich.logging import RichHandler
+from rich.text import Text
+
+from nemoguardrails.logging.simplify_formatter import SimplifyFormatter
+from nemoguardrails.utils import console
 
 # Global state variable to track if the verbose mode has already been enabled
 verbose_mode_enabled = False
 
 # Whether to log the LLM calls verbosely.
 verbose_llm_calls = False
 
-
-class Styles:
-    """The set of standard colors."""
-
-    # Foreground colors
-    BLACK = "\033[30m"
-    RED = "\033[31m"
-    GREEN = "\033[32m"
-    YELLOW = "\033[33m"
-    BLUE = "\033[34m"
-    MAGENTA = "\033[35m"
-    CYAN = "\033[36m"
-    WHITE = "\033[37m"
-    GREY = "\033[38;5;246m"
-
-    # Background colors
-    BLACK_BACKGROUND = "\033[40m"
-    RED_BACKGROUND = "\033[41m"
-    GREEN_BACKGROUND = "\033[42m"
-    YELLOW_BACKGROUND = "\033[43m"
-    BLUE_BACKGROUND = "\033[44m"
-    MAGENTA_BACKGROUND = "\033[45m"
-    CYAN_BACKGROUND = "\033[46m"
-    WHITE_BACKGROUND = "\033[47m"
-    GREY_BACKGROUND = "\033[48;5;246m"
-
-    # Presets
-    WHITE_ON_GREEN = "\033[42m\033[97m"
-
-    PROMPT = "\033[38;5;232m\033[48;5;254m"
-    COMPLETION = "\033[38;5;236m\033[48;5;84m"
-    COMPLETION_GREEN = "\033[48;5;84m"
-    COMPLETION_RED = "\033[48;5;196m"
-    EVENT_NAME = "\033[38;5;32m"
-
-    RESET = "\033[38m"
-    RESET_ALL = "\033[0m"
-
-
-# Mapping of colors associated with various sections
-SECTION_COLOR = {
-    "Phase 1": {"title": Styles.GREEN},
-    "Phase 2": {"title": Styles.GREEN},
-    "Phase 3": {"title": Styles.GREEN},
-    "Event": {"title": Styles.CYAN},
-    "Executing action": {"title": Styles.CYAN},
-    "Prompt": {
-        "title": Styles.BLUE,
-        "body": Styles.PROMPT,
-    },
-    "Prompt Messages": {
-        "title": Styles.BLUE,
-        "body": Styles.PROMPT,
-    },
-    "Completion": {"title": Styles.BLUE, "body": Styles.COMPLETION},
-    "---": {"title": Styles.GREY, "body": Styles.GREY},
-}
-
-
-class BlinkingCursor:
-    """Helper class for a blinking cursor."""
-
-    def __init__(self):
-        self._stop_event = threading.Event()
-        self._thread = threading.Thread(target=self._blink, daemon=True)
-
-    def _blink(self):
-        first = True
-        cursors = [f"{Styles.COMPLETION_RED} ", f"{Styles.COMPLETION_GREEN} "]
-        i = 0
-        while not self._stop_event.is_set():
-            i += 1
-            if first:
-                first = False
-            else:
-                print("\b", end="", flush=True)
-
-            print(f"{cursors[i%2]}", end="", flush=True)
-
-            for _ in range(25):
-                time.sleep(0.01)
-                if self._stop_event.is_set():
-                    break
-
-        print("\b \b", end="", flush=True)
-
-    def start(self):
-        if self._thread.is_alive():
-            return
-        self._stop_event.clear()
-        self._thread = threading.Thread(target=self._blink)
-        self._thread.start()
-
-    def stop(self):
-        if not self._thread.is_alive():
-            return
-        self._stop_event.set()
-        self._thread.join()
+# Whether the debug mode is enabled or not.
+debug_mode_enabled = False
 
 
 class VerboseHandler(logging.StreamHandler):
     """A log handler for verbose mode."""
 
     def __init__(self, *args, **kwargs):
         super(VerboseHandler, self).__init__(*args, **kwargs)
-        self.blinking_cursor = BlinkingCursor()
 
     def emit(self, record) -> None:
         msg = self.format(record)
 
         # We check if we're using the spacial syntax with "::" which denotes a title.
         if "::" in msg:
             title, body = msg.split(" :: ", 1)
             title = title.strip()
 
-            title_style = SECTION_COLOR.get(title, {}).get("title", "")
-            body_style = SECTION_COLOR.get(title, {}).get("body", "")
-
             # We remove the title for completion messages and stop the blinking cursor.
             if title == "Completion":
                 if verbose_llm_calls:
-                    self.blinking_cursor.stop()
-                    print(body_style + body + Styles.RESET_ALL)
+                    for line in body.split("\n"):
+                        text = Text(line, style="black on #006600", end="\n")
+                        text.pad_right(console.width)
+                        console.print(text)
+
+                    console.print("")
 
             # For prompts, we also start the blinking cursor.
             elif title == "Prompt":
                 if verbose_llm_calls:
-                    msg = (
-                        title_style
-                        + title
-                        + Styles.RESET_ALL
-                        + "\n"
-                        + body_style
-                        + body
-                        + Styles.RESET_ALL
-                    )
-                    print(msg, end="")
-                    self.blinking_cursor.start()
+                    console.print(f"[cyan]{title}[/]")
+                    console.print("")
+                    for line in body.split("\n"):
+                        text = Text(line, style="black on #909090", end="\n")
+                        text.pad_right(console.width)
+                        console.print(text)
 
             elif title == "Event":
                 # For events, we also color differently the type of event.
                 event_name, body = body.split(" ", 1)
-                title = title_style + title + Styles.RESET_ALL
-                event_name = Styles.EVENT_NAME + event_name + Styles.RESET_ALL
-                body = body_style + body + Styles.RESET_ALL
-                msg = title + " " + event_name + " " + body
+                msg = f"[blue]{title}[/] [bold]{event_name}[/] {body}"
 
-                print(msg)
+                console.print(msg, highlight=False)
             else:
-                title = title_style + title + Styles.RESET_ALL
-                body = body_style + body + Styles.RESET_ALL
-                msg = title + " " + body
-
-                print(msg)
+                skip_print = False
 
-
-def set_verbose(verbose: bool):
-    """Configure the verbose mode."""
+                if title == "Processing event":
+                    try:
+                        event_dict = literal_eval(body)
+                        event_type = event_dict["type"]
+
+                        if "ActionStarted" in event_type:
+                            skip_print = True
+                        elif event_dict["type"] not in [
+                            "CheckLocalAsync",
+                            "LocalAsyncCounter",
+                        ]:
+                            del event_dict["type"]
+                            del event_dict["uid"]
+                            body = json.dumps(event_dict)
+
+                            # We're adding a new line before action events, to
+                            # make it more readable.
+                            if event_type.startswith("Start") and event_type.endswith(
+                                "Action"
+                            ):
+                                console.print(
+                                    f"[magenta][bold]Start[/]{event_type[5:]}[/]"
+                                )
+                            elif event_type.startswith("Stop") and event_type.endswith(
+                                "Action"
+                            ):
+                                console.print(
+                                    f"[magenta][bold]Stop[/]{event_type[4:]}[/]"
+                                )
+                            elif event_type.endswith("ActionUpdated"):
+                                console.print(
+                                    f"[magenta]{event_type[:-7]}[bold]Updated[/][/]"
+                                )
+                            elif event_type.endswith("ActionFinished"):
+                                if event_type == "UtteranceUserActionFinished":
+                                    console.print(
+                                        f"\n[magenta]{event_type[:-8]}[bold]Finished[/][/]"
+                                    )
+                                else:
+                                    console.print(
+                                        f"[magenta]{event_type[:-8]}[bold]Finished[/][/]"
+                                    )
+                            elif event_type.endswith("ActionFailed"):
+                                console.print(
+                                    f"[magenta{event_type[:-6]}][bold]Failed[/][/]"
+                                )
+                            else:
+                                console.print(f"[magenta]{event_type}[/]")
+                            msg = f"{body}"
+                        else:
+                            skip_print = True
+                    except Exception:
+                        msg = f"[red]{title}[/] {body}"
+                elif title == "Running action":
+                    skip_print = True
+                elif title == "Matching head":
+                    skip_print = True
+                    # TODO: activate this once the source line is sorted
+                    # flow_name = re.findall(r"flow='(.*?)'", body)[0]
+                    # flow_pos = re.findall(r"pos=(\d+)", body)[0]
+                    # msg = f"[yellow][bold]{flow_name}[/]:{flow_pos}[/]"
+                    #
+                    # ignored_flows = ["_bot_say", "await_flow_by_name"]
+                    # if flow_pos == "0" or flow_name in ignored_flows:
+                    #     skip_print = True
+                else:
+                    if title == "---":
+                        msg = f"[#555555]{body}[/]"
+                    else:
+                        msg = f"[#707070]{title}[/] [#555555]{body}[/]"
+
+                if not skip_print:
+                    console.print(msg, highlight=False)
+
+
+def set_verbose(
+    verbose: bool,
+    llm_calls: bool = False,
+    debug: bool = False,
+    debug_level: str = "INFO",
+    simplify: bool = False,
+):
+    """Configure the verbose mode.
+
+    The verbose mode is meant to be user-friendly. It provides additional information
+    about what is happening under the hood.
+
+    The verbose debug mode provides detailed logs, and it's meant for debugging purposes.
+
+    Args:
+        verbose: Whether the verbose mode should be enabled or not.
+        llm_calls: Whether to log the prompt and response from the LLM calls (default False).
+        debug: Whether the debug mode should be enabled or not (default False).
+        debug_level: The log level to be used for debug mode (default INFO).
+        simplify: Whether the output should be simplified to optimize for readability.
+    """
     global verbose_mode_enabled
+    global debug_mode_enabled
+    global verbose_llm_calls
 
     if verbose and not verbose_mode_enabled:
         root_logger = logging.getLogger()
 
         # We make sure that the root logger is at least INFO so that we can see the messages from the VerboseHandler.
         if root_logger.level > logging.INFO:
             root_logger.setLevel(logging.INFO)
 
         # We make sure the log level for the default root console handler is set to WARNING.
         for handler in root_logger.handlers:
             if isinstance(handler, logging.StreamHandler):
                 handler.setLevel(logging.WARNING)
 
-        # Next, we also add an instance of the VerboseHandler.
-        verbose_handler = VerboseHandler()
-        verbose_handler.setLevel(logging.INFO)
-        root_logger.addHandler(verbose_handler)
+        # In debug mode we add the RichHandler, otherwise we add the VerboseHandler.
+        if debug:
+            root_logger = logging.getLogger()
+            rich_handler = RichHandler(markup=True)
+
+            # If needed, simplify further the verbose output
+            if simplify:
+                rich_handler.setFormatter(SimplifyFormatter())
+
+            root_logger.addHandler(rich_handler)
+            root_logger.setLevel(debug_level)
+        else:
+            # Next, we also add an instance of the VerboseHandler.
+            verbose_handler = VerboseHandler()
+            verbose_handler.setLevel(logging.INFO)
+            verbose_handler.setFormatter(SimplifyFormatter())
+            root_logger.addHandler(verbose_handler)
 
         # Also, we make sure the sentence_transformers log level is set to WARNING.
         logging.getLogger("sentence_transformers").setLevel(logging.WARNING)
 
         verbose_mode_enabled = True
+        verbose_llm_calls = llm_calls
+        debug_mode_enabled = debug
         print("Entered verbose mode.")
-
-
-def set_verbose_llm_calls(verbose: bool):
-    """Configure the verbose LLM calls mode."""
-    global verbose_llm_calls
-
-    verbose_llm_calls = verbose
```

## nemoguardrails/rails/llm/config.py

```diff
@@ -20,22 +20,25 @@
 
 import yaml
 from pydantic import BaseModel, ValidationError, root_validator
 from pydantic.fields import Field
 
 from nemoguardrails.colang import parse_colang_file, parse_flow_elements
 from nemoguardrails.colang.v2_x.lang.colang_ast import Flow
-from nemoguardrails.colang.v2_x.runtime.flows import ColangParsingError
+from nemoguardrails.colang.v2_x.runtime.errors import ColangParsingError
 
 log = logging.getLogger(__name__)
 
 # Load the default config values from the file
 with open(os.path.join(os.path.dirname(__file__), "default_config.yml")) as _fc:
     _default_config = yaml.safe_load(_fc)
 
+with open(os.path.join(os.path.dirname(__file__), "default_config_v2.yml")) as _fc:
+    _default_config_v2 = yaml.safe_load(_fc)
+
 
 # Extract the COLANGPATH directories.
 colang_path_dirs = [
     _path.strip()
     for _path in os.environ.get("COLANGPATH", "").split(os.pathsep)
     if _path.strip() != ""
 ]
@@ -251,14 +254,30 @@
 
     flows: List[str] = Field(
         default_factory=list,
         description="The names of all the flows that implement retrieval rails.",
     )
 
 
+class ActionRails(BaseModel):
+    """Configuration of action rails.
+
+    Action rails control various options related to the execution of actions.
+    Currently, only
+
+    In the future multiple options will be added, e.g., what input validation should be
+    performed per action, output validation, throttling, disabling, etc.
+    """
+
+    instant_actions: Optional[List[str]] = Field(
+        default=None,
+        description="The names of all actions which should finish instantly.",
+    )
+
+
 class SingleCallConfig(BaseModel):
     """Configuration for the single LLM call option for topical rails."""
 
     enabled: bool = False
     fallback_to_multiple_calls: bool = Field(
         default=True,
         description="Whether to fall back to multiple calls if a single call is not possible.",
@@ -347,14 +366,17 @@
     retrieval: RetrievalRails = Field(
         default_factory=RetrievalRails,
         description="Configuration of the retrieval rails.",
     )
     dialog: DialogRails = Field(
         default_factory=DialogRails, description="Configuration of the dialog rails."
     )
+    actions: ActionRails = Field(
+        default_factory=ActionRails, description="Configuration of action rails."
+    )
 
 
 def merge_two_dicts(dict_1: dict, dict_2: dict, ignore_keys: Set[str]) -> None:
     """Merges the fields of two dictionaries recursively."""
     for key, value in dict_2.items():
         if key not in ignore_keys:
             if key in dict_1:
@@ -586,15 +608,14 @@
 
         with open(current_path, "r", encoding="utf-8") as f:
             try:
                 _parsed_config = parse_colang_file(
                     current_file, content=f.read(), version=colang_version
                 )
             except Exception as e:
-                print(e)
                 raise ColangParsingError(
                     f"Error while parsing Colang file: {current_path}"
                 ) from e
 
             # We join only the "import_paths" field in the config for now
             _join_config(
                 raw_config,
@@ -771,14 +792,31 @@
             "self check facts" in enabled_output_rails
             and "self_check_facts" not in provided_task_prompts
         ):
             raise ValueError("You must provide a `self_check_facts` prompt template.")
 
         return values
 
+    @root_validator(pre=True, allow_reuse=True)
+    def fill_in_default_values_for_v2_x(cls, values):
+        instructions = values.get("instructions", {})
+        sample_conversation = values.get("sample_conversation")
+        colang_version = values.get("colang_version", "1.0")
+
+        if colang_version == "2.x":
+            if not instructions:
+                values["instructions"] = _default_config_v2["instructions"]
+
+            if not sample_conversation:
+                values["sample_conversation"] = _default_config_v2[
+                    "sample_conversation"
+                ]
+
+        return values
+
     raw_llm_call_action: Optional[str] = Field(
         default="raw llm call",
         description="The name of the action that would execute the original raw LLM call. ",
     )
 
     @staticmethod
     def from_path(
```

## nemoguardrails/rails/llm/llm_flows.co

```diff
@@ -113,15 +113,15 @@
     if $config.rails.output.flows
       # If we have generation options, we make sure the output rails are enabled.
       if $generation_options is None or $generation_options.rails.output:
         # Create a marker event.
         create event StartOutputRails
         event StartOutputRails
 
-        # Run all the input rails
+        # Run all the output rails
         # This can potentially alter the $user_message
         do run output rails
 
         # Create a marker event.
         create event OutputRailsFinished
         event OutputRailsFinished
```

## nemoguardrails/rails/llm/llmrails.py

```diff
@@ -14,29 +14,35 @@
 # limitations under the License.
 
 """LLM Rails entry point."""
 import asyncio
 import importlib.util
 import logging
 import os
+import re
 import threading
 import time
 import warnings
-from typing import Any, AsyncIterator, List, Optional, Tuple, Type, Union
+from typing import Any, AsyncIterator, List, Optional, Tuple, Type, Union, cast
 
 from langchain.llms.base import BaseLLM
 
 from nemoguardrails.actions.llm.generation import LLMGenerationActions
 from nemoguardrails.actions.llm.utils import get_colang_history
 from nemoguardrails.actions.v2_x.generation import LLMGenerationActionsV2dotx
 from nemoguardrails.colang import parse_colang_file
 from nemoguardrails.colang.v1_0.runtime.flows import compute_context
 from nemoguardrails.colang.v1_0.runtime.runtime import Runtime, RuntimeV1_0
 from nemoguardrails.colang.v2_x.lang.utils import new_uuid
+from nemoguardrails.colang.v2_x.runtime.flows import Action, State
 from nemoguardrails.colang.v2_x.runtime.runtime import RuntimeV2_x
+from nemoguardrails.colang.v2_x.runtime.serialization import (
+    json_to_state,
+    state_to_json,
+)
 from nemoguardrails.context import (
     explain_info_var,
     generation_options_var,
     llm_stats_var,
     streaming_handler_var,
 )
 from nemoguardrails.embeddings.index import EmbeddingsIndex
@@ -80,15 +86,15 @@
             verbose: Whether the logging should be verbose or not.
         """
         self.config = config
         self.llm = llm
         self.verbose = verbose
 
         if self.verbose:
-            set_verbose(True)
+            set_verbose(True, llm_calls=True)
 
         # We allow the user to register additional embedding search providers, so we keep
         # an index of them.
         self.embedding_search_providers = {}
 
         # The default embeddings model is using FastEmbed
         self.default_embedding_model = "all-MiniLM-L6-v2"
@@ -333,16 +339,19 @@
                     if llm_config.engine in [
                         "azure",
                         "openai",
                         "gooseai",
                         "nlpcloud",
                         "petals",
                         "trt_llm",
+                        "vertexai",
                     ]:
                         kwargs["model_name"] = llm_config.model
+                    elif llm_config.engine == "nvidia_ai_endpoints":
+                        kwargs["model"] = llm_config.model
                     else:
                         # The `__fields__` attribute is computed dynamically by pydantic.
                         if "model" in provider_cls.__fields__:
                             kwargs["model"] = llm_config.model
 
                 if self.config.streaming:
                     if "streaming" in provider_cls.__fields__:
@@ -391,15 +400,15 @@
         else:
             if esp_config.name not in self.embedding_search_providers:
                 raise Exception(f"Unknown embedding search provider: {esp_config.name}")
             else:
                 kwargs = esp_config.parameters
                 return self.embedding_search_providers[esp_config.name](**kwargs)
 
-    def _get_events_for_messages(self, messages: List[dict]):
+    def _get_events_for_messages(self, messages: List[dict], state: Any):
         """Return the list of events corresponding to the provided messages.
 
         Tries to find a prefix of messages for which we have already a list of events
         in the cache. For the rest, they are converted as is.
 
         The reason this cache exists is that we want to benefit from events generated in
         previous turns, which can't be computed again because it would be expensive (e.g.,
@@ -411,72 +420,108 @@
             messages: The list of messages.
 
         Returns:
             A list of events.
         """
         events = []
 
-        # We try to find the longest prefix of messages for which we have a cache
-        # of events.
-        p = len(messages) - 1
-        while p > 0:
-            cache_key = get_history_cache_key(messages[0:p])
-            if cache_key in self.events_history_cache:
-                events = self.events_history_cache[cache_key].copy()
-                break
-
-            p -= 1
+        if self.config.colang_version == "1.0":
+            # We try to find the longest prefix of messages for which we have a cache
+            # of events.
+            p = len(messages) - 1
+            while p > 0:
+                cache_key = get_history_cache_key(messages[0:p])
+                if cache_key in self.events_history_cache:
+                    events = self.events_history_cache[cache_key].copy()
+                    break
+
+                p -= 1
+
+            # For the rest of the messages, we transform them directly into events.
+            # TODO: Move this to separate function once more types of messages are supported.
+            for idx in range(p, len(messages)):
+                msg = messages[idx]
+                if msg["role"] == "user":
+                    events.append(
+                        {
+                            "type": "UtteranceUserActionFinished",
+                            "final_transcript": msg["content"],
+                        }
+                    )
 
-        # For the rest of the messages, we transform them directly into events.
-        # TODO: Move this to separate function once more types of messages are supported.
-        for idx in range(p, len(messages)):
-            msg = messages[idx]
-            if msg["role"] == "user":
-                events.append(
-                    {
-                        "type": "UtteranceUserActionFinished",
-                        "final_transcript": msg["content"],
-                    }
-                )
+                    # If it's not the last message, we also need to add the `UserMessage` event
+                    if idx != len(messages) - 1:
+                        events.append(
+                            {
+                                "type": "UserMessage",
+                                "text": msg["content"],
+                            }
+                        )
 
-                # If it's not the last message, we also need to add the `UserMessage` event
-                if idx != len(messages) - 1:
+                elif msg["role"] == "assistant":
+                    action_uid = new_uuid()
+                    start_event = new_event_dict(
+                        "StartUtteranceBotAction",
+                        final_transcript=msg["content"],
+                        action_uid=action_uid,
+                    )
+                    finished_event = new_event_dict(
+                        "UtteranceBotActionFinished",
+                        final_transcript=msg["content"],
+                        is_success=True,
+                        action_uid=action_uid,
+                    )
+                    events.extend([start_event, finished_event])
+                elif msg["role"] == "context":
+                    events.append({"type": "ContextUpdate", "data": msg["content"]})
+                elif msg["role"] == "event":
+                    events.append(msg["event"])
+        else:
+            for idx in range(len(messages)):
+                msg = messages[idx]
+                if msg["role"] == "user":
                     events.append(
                         {
-                            "type": "UserMessage",
-                            "text": msg["content"],
+                            "type": "UtteranceUserActionFinished",
+                            "final_transcript": msg["content"],
                         }
                     )
 
-            elif msg["role"] == "assistant":
-                action_uid = new_uuid()
-                start_event = new_event_dict(
-                    "StartUtteranceBotAction",
-                    final_transcript=msg["content"],
-                    action_uid=action_uid,
-                )
-                finished_event = new_event_dict(
-                    "UtteranceBotActionFinished",
-                    final_transcript=msg["content"],
-                    is_success=True,
-                    action_uid=action_uid,
-                )
-                events.extend([start_event, finished_event])
-            elif msg["role"] == "context":
-                events.append({"type": "ContextUpdate", "data": msg["content"]})
-            elif msg["role"] == "event":
-                events.append(msg["event"])
+                elif msg["role"] == "assistant":
+                    raise ValueError(
+                        "Providing `assistant` messages as input is not supported for Colang 2.0 configurations."
+                    )
+                elif msg["role"] == "context":
+                    events.append({"type": "ContextUpdate", "data": msg["content"]})
+                elif msg["role"] == "event":
+                    events.append(msg["event"])
+                elif msg["role"] == "tool":
+                    action_uid = msg["tool_call_id"]
+                    return_value = msg["content"]
+                    action: Action = state.actions[action_uid]
+                    events.append(
+                        new_event_dict(
+                            f"{action.name}Finished",
+                            action_uid=action_uid,
+                            action_name=action.name,
+                            status="success",
+                            is_success=True,
+                            return_value=return_value,
+                            events=[],
+                        )
+                    )
 
         return events
 
     async def generate_async(
         self,
         prompt: Optional[str] = None,
         messages: Optional[List[dict]] = None,
         options: Optional[Union[dict, GenerationOptions]] = None,
+        state: Optional[Union[dict, State]] = None,
         streaming_handler: Optional[StreamingHandler] = None,
         return_context: bool = False,
     ) -> Union[str, dict, GenerationResponse, Tuple[dict, dict]]:
         """Generate a completion or a next message.
 
         The format for messages is the following:
 
@@ -490,22 +535,34 @@
             ]
         ```
 
         Args:
             prompt: The prompt to be used for completion.
             messages: The history of messages to be used to generate the next message.
             options: Options specific for the generation.
+            state: The state object that should be used as the starting point.
             streaming_handler: If specified, and the config supports streaming, the
               provided handler will be used for streaming.
             return_context: Whether to return the context at the end of the run.
 
         Returns:
             The completion (when a prompt is provided) or the next message.
 
         System messages are not yet supported."""
+        # If a state object is specified, then we switch to "generation options" mode.
+        # This is because we want the output to be a GenerationResponse which will contain
+        # the output state.
+        if state is not None:
+            # We deserialize the state if needed.
+            if isinstance(state, dict) and state.get("version", "1.0") == "2.x":
+                state = json_to_state(state["state"])
+
+            if options is None:
+                options = GenerationOptions()
+
         # We allow options to be specified both as a dict and as an object.
         if options and isinstance(options, dict):
             options = GenerationOptions(**options)
 
         # Save the generation options in the current async context.
         generation_options_var.set(options)
 
@@ -562,53 +619,111 @@
         #   This is important as without it, the LLM prediction is not as good.
 
         t0 = time.time()
 
         # Initialize the LLM stats
         llm_stats = LLMStats()
         llm_stats_var.set(llm_stats)
+        processing_log = []
 
         # The array of events corresponding to the provided sequence of messages.
-        events = self._get_events_for_messages(messages)
+        events = self._get_events_for_messages(messages, state)
 
-        # Compute the new events.
-        processing_log = []
-        new_events = await self.runtime.generate_events(
-            events, processing_log=processing_log
-        )
+        if self.config.colang_version == "1.0":
+            # If we had a state object, we also need to prepend the events from the state.
+            state_events = []
+            if state:
+                assert isinstance(state, dict)
+                state_events = state["events"]
+
+            # Compute the new events.
+            new_events = await self.runtime.generate_events(
+                state_events + events, processing_log=processing_log
+            )
+            output_state = None
+        else:
+            # In generation mode, by default the bot response is an instant action.
+            instant_actions = ["UtteranceBotAction"]
+            if self.config.rails.actions.instant_actions is not None:
+                instant_actions = self.config.rails.actions.instant_actions
+
+            # Cast this explicitly to avoid certain warnings
+            runtime: RuntimeV2_x = cast(RuntimeV2_x, self.runtime)
+
+            # Compute the new events.
+            # In generation mode, the processing is always blocking, i.e., it waits for
+            # all local actions (sync and async).
+            new_events, output_state = await runtime.process_events(
+                events, state=state, instant_actions=instant_actions, blocking=True
+            )
+            # We also encode the output state as a JSON
+            output_state = {"state": state_to_json(output_state), "version": "2.x"}
 
         # Extract and join all the messages from StartUtteranceBotAction events as the response.
         responses = []
+        response_tool_calls = []
+        response_events = []
         new_extra_events = []
-        for event in new_events:
-            if event["type"] == "StartUtteranceBotAction":
-                # Check if we need to remove a message
-                if event["script"] == "(remove last message)":
-                    responses = responses[0:-1]
-                else:
-                    responses.append(event["script"])
 
-                # For the messages interface, we need to consider the UtteranceBotAction finished
-                # as soon as we return the message, hence we add the finished event to the new events.
-                # new_extra_events.append(
-                #     new_event_dict(
-                #         "UtteranceBotActionFinished",
-                #         action_uid=event["action_uid"],
-                #         is_success=True,
-                #         final_script=event["script"],
-                #     )
-                # )
+        # The processing is different for Colang 1.0 and 2.0
+        if self.config.colang_version == "1.0":
+            for event in new_events:
+                if event["type"] == "StartUtteranceBotAction":
+                    # Check if we need to remove a message
+                    if event["script"] == "(remove last message)":
+                        responses = responses[0:-1]
+                    else:
+                        responses.append(event["script"])
+        else:
+            for event in new_events:
+                start_action_match = re.match(r"Start(.*Action)", event["type"])
+
+                if start_action_match:
+                    action_name = start_action_match[1]
+                    # TODO: is there an elegant way to extract just the arguments?
+                    arguments = {
+                        k: v
+                        for k, v in event.items()
+                        if k != "type"
+                        and k != "uid"
+                        and k != "event_created_at"
+                        and k != "source_uid"
+                        and k != "action_uid"
+                    }
+                    response_tool_calls.append(
+                        {
+                            "id": event["action_uid"],
+                            "type": "function",
+                            "function": {"name": action_name, "arguments": arguments},
+                        }
+                    )
 
-        new_message = {"role": "assistant", "content": "\n".join(responses)}
+                elif event["type"] == "UtteranceBotActionFinished":
+                    responses.append(event["final_script"])
+                else:
+                    # We just append the event
+                    response_events.append(event)
 
-        # Save the new events in the history and update the cache
-        events.extend(new_events)
-        events.extend(new_extra_events)
-        cache_key = get_history_cache_key(messages + [new_message])
-        self.events_history_cache[cache_key] = events
+        new_message = {"role": "assistant", "content": "\n".join(responses)}
+        if response_tool_calls:
+            new_message["tool_calls"] = response_tool_calls
+        if response_events:
+            new_message["events"] = response_events
+
+        if self.config.colang_version == "1.0":
+            events.extend(new_events)
+            events.extend(new_extra_events)
+
+            # If a state object is not used, then we use the implicit caching
+            if state is None:
+                # Save the new events in the history and update the cache
+                cache_key = get_history_cache_key(messages + [new_message])
+                self.events_history_cache[cache_key] = events
+            else:
+                output_state = {"events": events}
 
         # If logging is enabled, we log the conversation
         # TODO: add support for logging flag
         explain_info.colang_history = get_colang_history(events)
         if self.verbose:
             log.info(f"Conversation history so far: \n{explain_info.colang_history}")
 
@@ -628,70 +743,97 @@
         if options:
             # If a prompt was used, we only need to return the content of the message.
             if prompt:
                 res = GenerationResponse(response=new_message["content"])
             else:
                 res = GenerationResponse(response=[new_message])
 
-            # If output variables are specified, we extract their values
-            if options.output_vars:
-                context = compute_context(events)
-                if isinstance(options.output_vars, list):
-                    # If we have only a selection of keys, we filter to only that.
-                    res.output_data = {k: context.get(k) for k in options.output_vars}
-                else:
-                    # Otherwise, we return the full context
-                    res.output_data = context
+            if self.config.colang_version == "1.0":
+                # If output variables are specified, we extract their values
+                if options.output_vars:
+                    context = compute_context(events)
+                    if isinstance(options.output_vars, list):
+                        # If we have only a selection of keys, we filter to only that.
+                        res.output_data = {
+                            k: context.get(k) for k in options.output_vars
+                        }
+                    else:
+                        # Otherwise, we return the full context
+                        res.output_data = context
 
-                # If the `return_context` is used, then we return a tuple to keep
-                # the interface compatible.
-                # TODO: remove this in 0.10.0.
-                if return_context:
-                    return new_message, context
+                    # If the `return_context` is used, then we return a tuple to keep
+                    # the interface compatible.
+                    # TODO: remove this in 0.10.0.
+                    if return_context:
+                        return new_message, context
 
-            _log = compute_generation_log(processing_log)
+                _log = compute_generation_log(processing_log)
 
-            # Include information about activated rails and LLM calls if requested
-            if options.log.activated_rails or options.log.llm_calls:
-                res.log = GenerationLog()
+                # Include information about activated rails and LLM calls if requested
+                if options.log.activated_rails or options.log.llm_calls:
+                    res.log = GenerationLog()
 
-                # We always include the stats
-                res.log.stats = _log.stats
+                    # We always include the stats
+                    res.log.stats = _log.stats
 
-                if options.log.activated_rails:
-                    res.log.activated_rails = _log.activated_rails
+                    if options.log.activated_rails:
+                        res.log.activated_rails = _log.activated_rails
 
-                if options.log.llm_calls:
-                    res.log.llm_calls = []
+                    if options.log.llm_calls:
+                        res.log.llm_calls = []
+                        for activated_rail in _log.activated_rails:
+                            for executed_action in activated_rail.executed_actions:
+                                res.log.llm_calls.extend(executed_action.llm_calls)
+
+                # Include internal events if requested
+                if options.log.internal_events:
+                    if res.log is None:
+                        res.log = GenerationLog()
+
+                    res.log.internal_events = new_events
+
+                # Include the Colang history if requested
+                if options.log.colang_history:
+                    if res.log is None:
+                        res.log = GenerationLog()
+
+                    res.log.colang_history = get_colang_history(events)
+
+                # Include the raw llm output if requested
+                if options.llm_output:
+                    # Currently, we include the output from the generation LLM calls.
                     for activated_rail in _log.activated_rails:
-                        for executed_action in activated_rail.executed_actions:
-                            res.log.llm_calls.extend(executed_action.llm_calls)
-
-            # Include internal events if requested
-            if options.log.internal_events:
-                if res.log is None:
-                    res.log = GenerationLog()
-
-                res.log.internal_events = new_events
+                        if activated_rail.type == "generation":
+                            for executed_action in activated_rail.executed_actions:
+                                for llm_call in executed_action.llm_calls:
+                                    res.llm_output = llm_call.raw_response
+            else:
+                if options.output_vars:
+                    raise ValueError(
+                        "The `output_vars` option is not supported for Colang 2.0 configurations."
+                    )
 
-            # Include the Colang history if requested
-            if options.log.colang_history:
-                if res.log is None:
-                    res.log = GenerationLog()
+                if (
+                    options.log.activated_rails
+                    or options.log.llm_calls
+                    or options.log.internal_events
+                    or options.log.colang_history
+                ):
+                    raise ValueError(
+                        "The `log` option is not supported for Colang 2.0 configurations."
+                    )
 
-                res.log.colang_history = get_colang_history(events)
+                if options.llm_output:
+                    raise ValueError(
+                        "The `llm_output` option is not supported for Colang 2.0 configurations."
+                    )
 
-            # Include the raw llm output if requested
-            if options.llm_output:
-                # Currently, we include the output from the generation LLM calls.
-                for activated_rail in _log.activated_rails:
-                    if activated_rail.type == "generation":
-                        for executed_action in activated_rail.executed_actions:
-                            for llm_call in executed_action.llm_calls:
-                                res.llm_output = llm_call.raw_response
+            # Include the state
+            if state is not None:
+                res.state = output_state
 
             return res
         else:
             # If a prompt is used, we only return the content of the message.
             if prompt:
                 return new_message["content"]
             else:
@@ -717,14 +859,15 @@
 
     def generate(
         self,
         prompt: Optional[str] = None,
         messages: Optional[List[dict]] = None,
         return_context: bool = False,
         options: Optional[Union[dict, GenerationOptions]] = None,
+        state: Optional[dict] = None,
     ):
         """Synchronous version of generate_async."""
 
         if check_sync_call_from_async_loop():
             raise RuntimeError(
                 "You are using the sync `generate` inside async code. "
                 "You should replace with `await generate_async(...)` or use `nest_asyncio.apply()`."
@@ -733,14 +876,15 @@
         loop = get_or_create_event_loop()
 
         return loop.run_until_complete(
             self.generate_async(
                 prompt=prompt,
                 messages=messages,
                 options=options,
+                state=state,
                 return_context=return_context,
             )
         )
 
     async def generate_events_async(
         self,
         events: List[dict],
@@ -820,22 +964,23 @@
         """
         t0 = time.time()
         llm_stats = LLMStats()
         llm_stats_var.set(llm_stats)
 
         # Compute the new events.
         # We need to protect 'process_events' to be called only once at a time
+        # TODO (cschueller): Why is this?
         async with process_events_semaphore:
             output_events, output_state = await self.runtime.process_events(
                 events, state
             )
 
         took = time.time() - t0
         # Small tweak, disable this when there were no events (or it was just too fast).
-        if took > 0.01:
+        if took > 0.1:
             log.info("--- :: Total processing took %.2f seconds." % took)
             log.info("--- :: Stats: %s" % llm_stats)
 
         return output_events, output_state
 
     def process_events(
         self, events: List[dict], state: Optional[dict] = None
```

## nemoguardrails/rails/llm/options.py

```diff
@@ -400,11 +400,15 @@
     output_data: Optional[dict] = Field(
         default=None,
         description="The output data, i.e. a dict with the values corresponding to the `output_vars`.",
     )
     log: Optional[GenerationLog] = Field(
         default=None, description="Additional logging information."
     )
+    state: Optional[dict] = Field(
+        default=None,
+        description="A state object which can be used in subsequent calls to continue the interaction.",
+    )
 
 
 if __name__ == "__main__":
     print(GenerationOptions(**{"rails": {"input": False}}))
```

## nemoguardrails/server/api.py

```diff
@@ -124,14 +124,18 @@
         description="If set, partial message deltas will be sent, like in ChatGPT. "
         "Tokens will be sent as data-only server-sent events as they become "
         "available, with the stream terminated by a data: [DONE] message.",
     )
     options: Optional[GenerationOptions] = Field(
         default=None, description="Additional options for controlling the generation."
     )
+    state: Optional[dict] = Field(
+        default=None,
+        description="A state object that should be used to continue the interaction.",
+    )
 
     @validator("config_ids", always=True)
     def check_if_set(cls, v, values, **kwargs):
         if v is not None and values.get("config_id") is not None:
             raise ValueError("Only one of config_id or config_ids should be specified")
         if v is None and values.get("config_id") is None:
             raise ValueError("Either config_id or config_ids must be specified")
@@ -149,14 +153,18 @@
     output_data: Optional[dict] = Field(
         default=None,
         description="The output data, i.e. a dict with the values corresponding to the `output_vars`.",
     )
     log: Optional[GenerationLog] = Field(
         default=None, description="Additional logging information."
     )
+    state: Optional[dict] = Field(
+        default=None,
+        description="A state object that should be used to continue the interaction in the future.",
+    )
 
 
 @app.get(
     "/v1/rails/configs",
     summary="Get List of available rails configurations.",
 )
 async def get_rails_configs():
@@ -314,23 +322,24 @@
 
             # Start the generation
             asyncio.create_task(
                 llm_rails.generate_async(
                     messages=messages,
                     streaming_handler=streaming_handler,
                     options=body.options,
+                    state=body.state,
                 )
             )
 
             # TODO: Add support for thread_ids in streaming mode
 
             return StreamingResponse(streaming_handler)
         else:
             res = await llm_rails.generate_async(
-                messages=messages, options=body.options
+                messages=messages, options=body.options, state=body.state
             )
 
             if isinstance(res, GenerationResponse):
                 bot_message = res.response[0]
             else:
                 assert isinstance(res, dict)
                 bot_message = res
@@ -343,14 +352,15 @@
             result = {"messages": [bot_message]}
 
             # If we have additional GenerationResponse fields, we return as well
             if isinstance(res, GenerationResponse):
                 result["llm_output"] = res.llm_output
                 result["output_data"] = res.output_data
                 result["log"] = res.log
+                result["state"] = res.state
 
             return result
 
     except Exception as ex:
         log.exception(ex)
         return {
             "messages": [{"role": "assistant", "content": "Internal server error."}]
```

## Comparing `nemoguardrails/cli/simplify_formatter.py` & `nemoguardrails/eval/cli/simplify_formatter.py`

 * *Files 3% similar despite different names*

```diff
@@ -17,15 +17,19 @@
 import re
 
 
 class SimplifyFormatter(logging.Formatter):
     """A formatter to simplify the log messages for easy reading."""
 
     def format(self, record):
-        text = super().format(record)
+        text: str
+        if isinstance(record, str):
+            text = record
+        else:
+            text = super().format(record)
 
         # Replace all UUIDs
         pattern = re.compile(
             r"([0-9a-fA-F]{8}[-_][0-9a-fA-F]{4}[-_][0-9a-fA-F]{4}[-_][0-9a-fA-F]{4}[-_][0-9a-fA-F]{12})"
         )
         text = pattern.sub(lambda m: m.group(1)[:4] + "...", text)
```

## Comparing `nemoguardrails-0.8.1.dist-info/LICENSE-Apache-2.0.txt` & `nemoguardrails-0.8.2.dist-info/LICENSE-Apache-2.0.txt`

 * *Files identical despite different names*

## Comparing `nemoguardrails-0.8.1.dist-info/LICENSE.md` & `nemoguardrails-0.8.2.dist-info/LICENSE.md`

 * *Files identical despite different names*

## Comparing `nemoguardrails-0.8.1.dist-info/METADATA` & `nemoguardrails-0.8.2.dist-info/METADATA`

 * *Files 1% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: nemoguardrails
-Version: 0.8.1
+Version: 0.8.2
 Summary: NeMo Guardrails is an open-source toolkit for easily adding programmagle guardrails to LLM-based conversational systems.
 Author-email: NVIDIA <nemoguardrails@nvidia.com>
 License: SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
         SPDX-License-Identifier: Apache-2.0
         
         Licensed under the Apache License, Version 2.0 (the "License");
         you may not use this file except in compliance with the License.
@@ -83,15 +83,15 @@
 [![License](https://img.shields.io/badge/License-Apache%202.0-brightgreen.svg)](https://github.com/NVIDIA/NeMo-Guardrails/blob/main/LICENSE.md)
 [![Project Status](https://img.shields.io/badge/Status-beta-orange)](#)
 [![PyPI version](https://badge.fury.io/py/nemoguardrails.svg)](https://badge.fury.io/py/nemoguardrails)
 [![Python 3.7+](https://img.shields.io/badge/python-3.7%2B-green)](https://www.python.org/downloads/)
 [![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)
 [![arXiv](https://img.shields.io/badge/arXiv-2310.10501-b31b1b.svg)](https://arxiv.org/abs/2310.10501)
 
-> **LATEST RELEASE / DEVELOPMENT VERSION**: The [main](https://github.com/NVIDIA/NeMo-Guardrails/tree/main) branch tracks the latest released beta version: [0.8.1](https://github.com/NVIDIA/NeMo-Guardrails/tree/v0.8.1). For the latest development version, checkout the [develop](https://github.com/NVIDIA/NeMo-Guardrails/tree/develop) branch.
+> **LATEST RELEASE / DEVELOPMENT VERSION**: The [main](https://github.com/NVIDIA/NeMo-Guardrails/tree/main) branch tracks the latest released beta version: [0.8.2](https://github.com/NVIDIA/NeMo-Guardrails/tree/v0.8.2). For the latest development version, checkout the [develop](https://github.com/NVIDIA/NeMo-Guardrails/tree/develop) branch.
 
 > **DISCLAIMER**: The beta release is undergoing active development and may be subject to changes and improvements, which could cause instability and unexpected behavior. We currently do not recommend deploying this beta version in a production setting. We appreciate your understanding and contribution during this stage. Your support and feedback are invaluable as we advance toward creating a robust, ready-for-production LLM guardrails toolkit. The examples provided within the documentation are for educational purposes to get started with NeMo Guardrails, and are not meant for use in production applications.
 
 NeMo Guardrails is an open-source toolkit for easily adding *programmable guardrails* to LLM-based conversational applications. Guardrails (or "rails" for short) are specific ways of controlling the output of a large language model, such as not talking about politics, responding in a particular way to specific user requests, following a predefined dialog path, using a particular language style, extracting structured data, and more.
 
 [This paper](https://arxiv.org/abs/2310.10501) introduces NeMo Guardrails and contains a technical overview of the system and the current evaluation.
 
@@ -112,15 +112,15 @@
 For more detailed instructions, see the [Installation Guide](docs/getting_started/installation-guide.md).
 
 ## Overview
 
 NeMo Guardrails enables developers building LLM-based applications to easily add **programmable guardrails** between the application code and the LLM.
 
 <div align="center">
-  <img src="https://github.com/NVIDIA/NeMo-Guardrails/raw/develop/docs/_assets/images/programmable_guardrails.png"  width="75%" alt="Programmable Guardrails">
+  <img src="https://github.com/NVIDIA/NeMo-Guardrails/raw/develop/docs/_static/images/programmable_guardrails.png"  width="75%" alt="Programmable Guardrails">
 </div>
 
 Key benefits of adding *programmable guardrails* include:
 
 - **Building Trustworthy, Safe, and Secure LLM-based Applications:** you can define rails to guide and safeguard conversations; you can choose to define the behavior of your LLM-based application on specific topics and prevent it from engaging in discussions on unwanted topics.
 
 - **Connecting models, chains, and other services securely:** you can connect an LLM to other services (a.k.a. tools) seamlessly and securely.
@@ -128,15 +128,15 @@
 - **Controllable dialog**: you can steer the LLM to follow pre-defined conversational paths, allowing you to design the interaction following conversation design best practices and enforce standard operating procedures (e.g., authentication, support).
 
 ### Protecting against LLM Vulnerabilities
 
 NeMo Guardrails provides several mechanisms for protecting an LLM-powered chat application against common LLM vulnerabilities, such as jailbreaks and prompt injections. Below is a sample overview of the protection offered by different guardrails configuration for the example [ABC Bot](./examples/bots/abc) included in this repository. For more details, please refer to the [LLM Vulnerability Scanning](./docs/evaluation/llm-vulnerability-scanning.md) page.
 
 <div align="center">
-<img src="https://github.com/NVIDIA/NeMo-Guardrails/raw/develop/docs/_assets/images/abc-llm-vulnerability-scan-results.png" width="750">
+<img src="https://github.com/NVIDIA/NeMo-Guardrails/raw/develop/docs/_static/images/abc-llm-vulnerability-scan-results.png" width="750">
 </div>
 
 
 ### Use Cases
 
 You can use programmable guardrails in different types of use cases:
 
@@ -180,15 +180,15 @@
 You can use NeMo Guardrails with multiple LLMs like OpenAI GPT-3.5, GPT-4, LLaMa-2, Falcon, Vicuna, or Mosaic. For more details, check out the [Supported LLM Models](docs/user_guides/configuration-guide.md#supported-llm-models) section in the Configuration Guide.
 
 ### Types of Guardrails
 
 NeMo Guardrails supports five main types of guardrails:
 
 <div align="center">
-  <img src="https://github.com/NVIDIA/NeMo-Guardrails/raw/develop/docs/_assets/images/programmable_guardrails_flow.png"  width="75%" alt="Programmable Guardrails Flow">
+  <img src="https://github.com/NVIDIA/NeMo-Guardrails/raw/develop/docs/_static/images/programmable_guardrails_flow.png"  width="75%" alt="Programmable Guardrails Flow">
 </div>
 
 1. **Input rails**: applied to the input from the user; an input rail can reject the input, stopping any additional processing, or alter the input (e.g., to mask potentially sensitive data, to rephrase).
 
 2. **Dialog rails**: influence how the LLM is prompted; dialog rails operate on canonical form messages (more details [here](docs/user_guides/colang-language-syntax-guide.md)) and determine if an action should be executed, if the LLM should be invoked to generate the next step or a response, if a predefined response should be used instead, etc.
 
 3. **Retrieval rails**: applied to the retrieved chunks in the case of a RAG (Retrieval Augmented Generation) scenario; a retrieval rail can reject a chunk, preventing it from being used to prompt the LLM, or alter the relevant chunks (e.g., to mask potentially sensitive data).
```

## Comparing `nemoguardrails-0.8.1.dist-info/RECORD` & `nemoguardrails-0.8.2.dist-info/RECORD`

 * *Files 26% similar despite different names*

```diff
@@ -77,14 +77,18 @@
 examples/configs/llm/hf_pipeline_vicuna/README.md,sha256=z8dAq117HnvEdYHp8Ax31P842njoq1iuLd2FHy3KX9w,899
 examples/configs/llm/hf_pipeline_vicuna/config.py,sha256=IdaHX0MWjm2x8r1Iy2cyh5igvS8IC3NjOtEp59WSKJI,3900
 examples/configs/llm/hf_pipeline_vicuna/config.yml,sha256=lUMrnTFgPDgkM-0TCE2Hdy5evDVRk9OklCdfNaBesQ0,3885
 examples/configs/llm/hf_pipeline_vicuna/rails.co,sha256=PTObMFdS6S4ntO2bcEwVulprOehApfNp9I_0hoE1R6g,380
 examples/configs/llm/nemollm/README.md,sha256=6CNBMKukQ7sZK7VEuQBgj6zRpfLmsaLrIKhabDLg7WQ,1199
 examples/configs/llm/nemollm/config.yml,sha256=_0Z1XeHFz195RJOwxCyWHrjUJnDtVg7OdMb232sAZ64,1535
 examples/configs/llm/nemollm/rails.co,sha256=PTObMFdS6S4ntO2bcEwVulprOehApfNp9I_0hoE1R6g,380
+examples/configs/llm/vertexai/README.md,sha256=N4cilS1Eqs7OsFmbgQgdE2W5N8DszqnUoqI02rOTv-o,1481
+examples/configs/llm/vertexai/config.yml,sha256=cXaekicyUbJ6SiESv0R5rPMLj_Dl5CgbeNYgvb7Mfu4,200
+examples/configs/llm/vertexai/prompts.yml,sha256=DFS4lC-vqcOrRjGwvsUPLJL2KAlbAe0_cPBhXnwBOZo,826
+examples/configs/llm/vertexai/rails.co,sha256=PTObMFdS6S4ntO2bcEwVulprOehApfNp9I_0hoE1R6g,380
 examples/configs/rag/README.md,sha256=tevZXLh6xCGizKnNTCelyM04SKvEg-D_jfSIydai2Pg,138
 examples/configs/rag/__init__.py,sha256=rwCvdw7_3gzn8X-0PQDwN1nMw0yJ1xXVO8l77fMkGO0,679
 examples/configs/rag/custom_rag_output_rails/README.md,sha256=gWW72-Wi00Tmp83GPSxESBIsDs1_W6V2niVpwfyfvWQ,1670
 examples/configs/rag/custom_rag_output_rails/config.py,sha256=PQYDBs0VeavrqAzqnuOqdG0IRrD9YkGyQoPEI86skAs,2495
 examples/configs/rag/custom_rag_output_rails/config.yml,sha256=N5xZpoijFG05Ci8GB_Y1fdTW_k-GI4eKl5_UgUzDhrc,817
 examples/configs/rag/custom_rag_output_rails/kb/report.md,sha256=IWtb5PT1sb-gwsclUZmcvvivjXjL47UOQwBXyIGifiU,7499
 examples/configs/rag/custom_rag_output_rails/rails/output.co,sha256=nTkZ86ECSV8VIDxi2LFA2UBp2BDYbi5Nth-wLb-hCIg,314
@@ -130,45 +134,147 @@
 examples/scripts/__init__.py,sha256=rwCvdw7_3gzn8X-0PQDwN1nMw0yJ1xXVO8l77fMkGO0,679
 examples/scripts/demo_llama_index_guardrails.py,sha256=_HsfOWqI7KmzM1sqhG0racpSzMSUCeasYBapkwCI-DM,3281
 examples/scripts/demo_streaming.py,sha256=OKmpZMDkmy8irqhl2G3pWUkONgNCwAdeD6LYPiWJyos,4962
 examples/scripts/langchain/experiments.py,sha256=-NqDQ2_dSfQ1B4-1rBzy6djKYqeiQIfS0nRY2c8zVMg,5707
 examples/server_configs/atomic/input_checking/config.yml,sha256=ZGMCmbZUVMEM3eSHD75vifPF9d69xM-fJOchDFD3p-U,962
 examples/server_configs/atomic/main/config.yml,sha256=tfWoT1wu4NgvqkOD09j7pBBxJKo884lWaCzO5KbENHE,76
 examples/server_configs/atomic/output_checking/config.yml,sha256=0mxKTLMoJ6h-oGT1I3TWUDLkO-NMycyJKUWsVD8uw8Y,870
-nemoguardrails/__init__.py,sha256=orgXu7EYl1zfYeupxqwPq2PYMxWyOkRHpgr6IgTj-Qk,983
+examples/v2_x/language_reference/control_flow_tools/catch_failing_flow/config.yml,sha256=aSgJNVXdM5oLq97Pgrsi5Dnzk9Tc9P9lTYRA1fU2v-U,22
+examples/v2_x/language_reference/control_flow_tools/catch_failing_flow/main.co,sha256=7adyYSnH5oNTOm1tfJe-yHqEmiyOnKE6m6T8-wuLPok,461
+examples/v2_x/language_reference/development_and_debugging/print_statement/config.yml,sha256=RiQ4cd7epn03l4zOaEBFtJI4hTJoA8VthCkLpzsdz94,85
+examples/v2_x/language_reference/development_and_debugging/print_statement/main.co,sha256=k-P8HhvtZoU8fXr4LQn3RavWeqQv39LFUGa0gn5oejU,67
+examples/v2_x/language_reference/event_generation/event_groups/config.yml,sha256=aSgJNVXdM5oLq97Pgrsi5Dnzk9Tc9P9lTYRA1fU2v-U,22
+examples/v2_x/language_reference/event_generation/event_groups/main.co,sha256=w-WMQIHYTJGfuqM7Nr0svEk8V1M0mw2-xkSp59N4kx0,205
+examples/v2_x/language_reference/event_generation/event_references/config.yml,sha256=aSgJNVXdM5oLq97Pgrsi5Dnzk9Tc9P9lTYRA1fU2v-U,22
+examples/v2_x/language_reference/event_generation/event_references/main.co,sha256=ZUwQtVY1Uqz-XgD4nWVZlcbZzYs7jb9GzqM-37wxT8g,160
+examples/v2_x/language_reference/event_pattern_matching/dictionary_parameters/config.yml,sha256=aSgJNVXdM5oLq97Pgrsi5Dnzk9Tc9P9lTYRA1fU2v-U,22
+examples/v2_x/language_reference/event_pattern_matching/dictionary_parameters/main.co,sha256=IH0fh6vTU93DmHlkv71voLRTDyi6f48ecYkRkxnyLNU,93
+examples/v2_x/language_reference/event_pattern_matching/event_grouping/config.yml,sha256=aSgJNVXdM5oLq97Pgrsi5Dnzk9Tc9P9lTYRA1fU2v-U,22
+examples/v2_x/language_reference/event_pattern_matching/event_grouping/main.co,sha256=6fre4zcYGSK2o-o1_sTnCVKjhLCVwLLwpGUjymB1y0w,343
+examples/v2_x/language_reference/event_pattern_matching/event_grouping_advanced/config.yml,sha256=aSgJNVXdM5oLq97Pgrsi5Dnzk9Tc9P9lTYRA1fU2v-U,22
+examples/v2_x/language_reference/event_pattern_matching/event_grouping_advanced/main.co,sha256=sDYsMgA0Ubixvxt5MLIBmqTqL-08HteoKomHeDsFYZQ,435
+examples/v2_x/language_reference/event_pattern_matching/event_matching/config.yml,sha256=aSgJNVXdM5oLq97Pgrsi5Dnzk9Tc9P9lTYRA1fU2v-U,22
+examples/v2_x/language_reference/event_pattern_matching/event_matching/main.co,sha256=ECoqaRFaegWCqQWw52Mv0C8rhLqZlCeneAlRxiVfw9M,97
+examples/v2_x/language_reference/event_pattern_matching/event_reference/config.yml,sha256=aSgJNVXdM5oLq97Pgrsi5Dnzk9Tc9P9lTYRA1fU2v-U,22
+examples/v2_x/language_reference/event_pattern_matching/event_reference/main.co,sha256=OXmRUZSawzcaPRkxI9oXkdm_OON9Kz_UjoEU9dS_QSM,143
+examples/v2_x/language_reference/event_pattern_matching/list_parameters/config.yml,sha256=aSgJNVXdM5oLq97Pgrsi5Dnzk9Tc9P9lTYRA1fU2v-U,22
+examples/v2_x/language_reference/event_pattern_matching/list_parameters/main.co,sha256=Z1hA0JMp91d2wS110TnOTAZMSc-LLbdhbKSfWhLhNeE,94
+examples/v2_x/language_reference/event_pattern_matching/regular_expression_parameters/config.yml,sha256=aSgJNVXdM5oLq97Pgrsi5Dnzk9Tc9P9lTYRA1fU2v-U,22
+examples/v2_x/language_reference/event_pattern_matching/regular_expression_parameters/main.co,sha256=ehmGcOjm9ZtmegTAlmWW-81BqMO_XaENDrCxvRhEQ7c,279
+examples/v2_x/language_reference/event_pattern_matching/set_parameters/config.yml,sha256=aSgJNVXdM5oLq97Pgrsi5Dnzk9Tc9P9lTYRA1fU2v-U,22
+examples/v2_x/language_reference/event_pattern_matching/set_parameters/main.co,sha256=uc53haOlAFiPF5h-ik4DM6K_WmB2I01E7tgKR3oV3QU,94
+examples/v2_x/language_reference/flows/action_conflict_resolution/config.yml,sha256=aSgJNVXdM5oLq97Pgrsi5Dnzk9Tc9P9lTYRA1fU2v-U,22
+examples/v2_x/language_reference/flows/action_conflict_resolution/main.co,sha256=ktU1ZYT9pAv9PEQlQoUeHh_jttWAoIU5rOUxN23nU3U,639
+examples/v2_x/language_reference/flows/call_a_flow/config.yml,sha256=aSgJNVXdM5oLq97Pgrsi5Dnzk9Tc9P9lTYRA1fU2v-U,22
+examples/v2_x/language_reference/flows/call_a_flow/main.co,sha256=lTmiTdykYAM6hYeSBTukUkNEbE6rS4f1q_GwPz6PzCQ,365
+examples/v2_x/language_reference/flows/concurrent_flows/config.yml,sha256=aSgJNVXdM5oLq97Pgrsi5Dnzk9Tc9P9lTYRA1fU2v-U,22
+examples/v2_x/language_reference/flows/concurrent_flows/main.co,sha256=gjAici-DeSyDaZ5qrhsbQ4lfksNw6tjQwt_6M1cdVJQ,476
+examples/v2_x/language_reference/flows/concurrent_flows_hierarchy/config.yml,sha256=aSgJNVXdM5oLq97Pgrsi5Dnzk9Tc9P9lTYRA1fU2v-U,22
+examples/v2_x/language_reference/flows/concurrent_flows_hierarchy/main.co,sha256=IR1C2O8lD6bLJkX8sq4XiZcV1iq60XvMWnX4FCuRczs,496
+examples/v2_x/language_reference/flows/flow_hierarchy/config.yml,sha256=aSgJNVXdM5oLq97Pgrsi5Dnzk9Tc9P9lTYRA1fU2v-U,22
+examples/v2_x/language_reference/flows/flow_hierarchy/main.co,sha256=P6LRpLHsLQGOqjnHWwH1xj_jrQJN-17Ne9JSAb2QOpE,464
+examples/v2_x/language_reference/flows/flow_parameters/config.yml,sha256=aSgJNVXdM5oLq97Pgrsi5Dnzk9Tc9P9lTYRA1fU2v-U,22
+examples/v2_x/language_reference/flows/flow_parameters/main.co,sha256=zcNb2RpnQ-AySlQRVNJzB4j-DyAG_vDNGMHi8z-wKZk,166
+examples/v2_x/language_reference/flows/flows_failing/config.yml,sha256=aSgJNVXdM5oLq97Pgrsi5Dnzk9Tc9P9lTYRA1fU2v-U,22
+examples/v2_x/language_reference/flows/flows_failing/main.co,sha256=8PFCWy_wzLXpJwhp2DE5zjDTDw5GHKcZYNE9BEvI-Co,497
+examples/v2_x/language_reference/flows/parallel_flows/config.yml,sha256=aSgJNVXdM5oLq97Pgrsi5Dnzk9Tc9P9lTYRA1fU2v-U,22
+examples/v2_x/language_reference/flows/parallel_flows/main.co,sha256=gjAici-DeSyDaZ5qrhsbQ4lfksNw6tjQwt_6M1cdVJQ,476
+examples/v2_x/language_reference/flows/parallel_flows_hierarchy/config.yml,sha256=aSgJNVXdM5oLq97Pgrsi5Dnzk9Tc9P9lTYRA1fU2v-U,22
+examples/v2_x/language_reference/flows/parallel_flows_hierarchy/main.co,sha256=IR1C2O8lD6bLJkX8sq4XiZcV1iq60XvMWnX4FCuRczs,496
+examples/v2_x/language_reference/flows/start_a_flow/config.yml,sha256=aSgJNVXdM5oLq97Pgrsi5Dnzk9Tc9P9lTYRA1fU2v-U,22
+examples/v2_x/language_reference/flows/start_a_flow/main.co,sha256=YRz-NIt6OkFPmh64XXwWHhAGImen92lYixs2uFOpRsU,381
+examples/v2_x/language_reference/internal_events/undefined_flow/hello_world/config.yml,sha256=aSgJNVXdM5oLq97Pgrsi5Dnzk9Tc9P9lTYRA1fU2v-U,22
+examples/v2_x/language_reference/internal_events/undefined_flow/hello_world/main.co,sha256=EsxIrM5Bpuxed7yMwjO-PR1vVRVWQEHB6qebtjjRks0,509
+examples/v2_x/language_reference/introduction/hello_world/config.yml,sha256=aSgJNVXdM5oLq97Pgrsi5Dnzk9Tc9P9lTYRA1fU2v-U,22
+examples/v2_x/language_reference/introduction/hello_world/main.co,sha256=TQOhZPkW0YTwr0uiRWuWOTArw537cml5CFGrTmyebgU,46
+examples/v2_x/language_reference/introduction/hello_world_umim/config.yml,sha256=aSgJNVXdM5oLq97Pgrsi5Dnzk9Tc9P9lTYRA1fU2v-U,22
+examples/v2_x/language_reference/introduction/hello_world_umim/main.co,sha256=RQdAxVhqLLqdvpNQKgyWi0u6Z8ZXlxK0jS-1NW7kop4,105
+examples/v2_x/language_reference/introduction/interaction_sequence/config.yml,sha256=aSgJNVXdM5oLq97Pgrsi5Dnzk9Tc9P9lTYRA1fU2v-U,22
+examples/v2_x/language_reference/introduction/interaction_sequence/main.co,sha256=3MaV8IdQdOHjKNrcxbXw09wgRB7NKGMXHyq4Qg9I5gY,317
+examples/v2_x/language_reference/llm/bot_intent_generation_example/config.yml,sha256=Gle31hhOJ4ydvJ1_s4omjvQwaQKRTGvOkiUfzyyN6ws,6775
+examples/v2_x/language_reference/llm/bot_intent_generation_example/main.co,sha256=LcKuyyU2en7nTFvfxzjWb2Su9CysCb5MFcRLNSI7PFg,148
+examples/v2_x/language_reference/llm/nld_example/config.yml,sha256=MXnnEcn78usQx90QT_8oc8_ZhruPedmC8IqfuSkzbL0,105
+examples/v2_x/language_reference/llm/nld_example/main.co,sha256=FgklA0IqJJNOk9kUa3oJcPB3nHNgExC59XwCJoQou1c,949
+examples/v2_x/language_reference/llm/user_intent_match_example/config.yml,sha256=tNQVc42_9k-4laGyQ3c8ITEjRI3RSa2s-pjjRM_yNi8,6761
+examples/v2_x/language_reference/llm/user_intent_match_example/main.co,sha256=yHcWxAgR5yrdvlD1Tuq2forT5YgPgQipoIgmstherhE,497
+examples/v2_x/language_reference/more_on_flows/activate_flow/config.yml,sha256=aSgJNVXdM5oLq97Pgrsi5Dnzk9Tc9P9lTYRA1fU2v-U,22
+examples/v2_x/language_reference/more_on_flows/activate_flow/main.co,sha256=pZlQm1R1UIyCd5dw9MEsh8jYJZ7dLIv7Yk9Ah8YEPbY,348
+examples/v2_x/language_reference/more_on_flows/interaction_loops/config.yml,sha256=aSgJNVXdM5oLq97Pgrsi5Dnzk9Tc9P9lTYRA1fU2v-U,22
+examples/v2_x/language_reference/more_on_flows/interaction_loops/main.co,sha256=yMfSCKE5BJwZzG44gdEgPB-snTSSek-LOF2DC3C_O3E,951
+examples/v2_x/language_reference/more_on_flows/restart_flow_instance/config.yml,sha256=aSgJNVXdM5oLq97Pgrsi5Dnzk9Tc9P9lTYRA1fU2v-U,22
+examples/v2_x/language_reference/more_on_flows/restart_flow_instance/main.co,sha256=wyAaYJvQmr0aTTuHbLXKhbX0kFCy8wmZJosbkfg6820,348
+examples/v2_x/language_reference/more_on_flows/start_new_flow_instance/config.yml,sha256=aSgJNVXdM5oLq97Pgrsi5Dnzk9Tc9P9lTYRA1fU2v-U,22
+examples/v2_x/language_reference/more_on_flows/start_new_flow_instance/main.co,sha256=Lh3OymOjAjxf4WJ9tF9fs7Wq_yQ30bnh_nfd2e-IXLM,441
+examples/v2_x/language_reference/working_with_actions/action_events/config.yml,sha256=aSgJNVXdM5oLq97Pgrsi5Dnzk9Tc9P9lTYRA1fU2v-U,22
+examples/v2_x/language_reference/working_with_actions/action_events/main.co,sha256=4UXWbmZUb2BJwuLq_FphdivEQmvn9XoTJLXvFBtU2is,213
+examples/v2_x/language_reference/working_with_actions/action_grouping/config.yml,sha256=aSgJNVXdM5oLq97Pgrsi5Dnzk9Tc9P9lTYRA1fU2v-U,22
+examples/v2_x/language_reference/working_with_actions/action_grouping/main.co,sha256=lTgL9N3qzzRt-VTi1og2UUNDwBVuTUTdg-wfsh0ti_A,309
+examples/v2_x/language_reference/working_with_actions/await_keyword/config.yml,sha256=aSgJNVXdM5oLq97Pgrsi5Dnzk9Tc9P9lTYRA1fU2v-U,22
+examples/v2_x/language_reference/working_with_actions/await_keyword/main.co,sha256=LlM2iL7MQ5fQLaEr0yLyds_U6VpHkUrqe1JPtTcSlA0,355
+examples/v2_x/language_reference/working_with_actions/dialog_pattern/config.yml,sha256=aSgJNVXdM5oLq97Pgrsi5Dnzk9Tc9P9lTYRA1fU2v-U,22
+examples/v2_x/language_reference/working_with_actions/dialog_pattern/main.co,sha256=Oq55pvX_F4VXs0dlYr0yhC-4V4NO5obfFgHfZbaEsoE,457
+examples/v2_x/language_reference/working_with_actions/omit_wait_keyword/config.yml,sha256=aSgJNVXdM5oLq97Pgrsi5Dnzk9Tc9P9lTYRA1fU2v-U,22
+examples/v2_x/language_reference/working_with_actions/omit_wait_keyword/main.co,sha256=iZrZSvbng57Ou4BfSzZMqknsSDvHMLbm1UKqai2ymZk,289
+examples/v2_x/language_reference/working_with_actions/start_keyword/config.yml,sha256=aSgJNVXdM5oLq97Pgrsi5Dnzk9Tc9P9lTYRA1fU2v-U,22
+examples/v2_x/language_reference/working_with_actions/start_keyword/main.co,sha256=vzNFtWfv0ba65903AWWNvqoq3MASHhV0n5MNBDMJgXo,193
+examples/v2_x/language_reference/working_with_actions/stop_action/config.yml,sha256=aSgJNVXdM5oLq97Pgrsi5Dnzk9Tc9P9lTYRA1fU2v-U,22
+examples/v2_x/language_reference/working_with_actions/stop_action/main.co,sha256=HdLidsPx7bU32YYyw4YFRvSCkvzcvN9mAi_Yr6lAaHk,128
+examples/v2_x/language_reference/working_with_actions/wait_for_first_action_only/config.yml,sha256=aSgJNVXdM5oLq97Pgrsi5Dnzk9Tc9P9lTYRA1fU2v-U,22
+examples/v2_x/language_reference/working_with_actions/wait_for_first_action_only/main.co,sha256=KftHm2X6hfvA_MuulB9QfNtewBpkOr2kqIHng1kYBDI,232
+examples/v2_x/other/llm_flow/README.md,sha256=SaFbdnobEyiSSCUdzff983L8xzG181Ffx9612sNdZa0,144
+examples/v2_x/other/llm_flow/config.yml,sha256=pWgofbTZu4k5O1MhXFpNM_H9RmmnnGxqrZZ0S12TfFE,99
+examples/v2_x/other/llm_flow/rails.co,sha256=fmQwiV3h3tWahvj0gyN5CgPUyjB-Vq0q7fctUJgKwMc,311
+examples/v2_x/tutorial/guardrails_1/README.md,sha256=GdlPLzE2jdZM6xlQrDgSWAn4bhiAkMN9mO0K9UoNC8M,412
+examples/v2_x/tutorial/guardrails_1/config.yml,sha256=pWgofbTZu4k5O1MhXFpNM_H9RmmnnGxqrZZ0S12TfFE,99
+examples/v2_x/tutorial/guardrails_1/rails.co,sha256=myeSgaL2Z9XLUo_F-BNIQ6Tlw7jQE8as-U-WskXVafQ,869
+examples/v2_x/tutorial/hello_world_1/README.md,sha256=ZC6BEIa6JG2HqngvTh9iY2n0K15PDUc6KdPxRRCKYfs,492
+examples/v2_x/tutorial/hello_world_1/config.yml,sha256=pWgofbTZu4k5O1MhXFpNM_H9RmmnnGxqrZZ0S12TfFE,99
+examples/v2_x/tutorial/hello_world_1/rails.co,sha256=2lkG7UqK4to82iAdOjJW4uGu49kvOguGsQEUTHFn4Y4,65
+examples/v2_x/tutorial/hello_world_2/README.md,sha256=RykpUkEfrxysNAryOZMj-ARf5Z-GrrEmAkZX7wGzGss,534
+examples/v2_x/tutorial/hello_world_2/config.yml,sha256=pWgofbTZu4k5O1MhXFpNM_H9RmmnnGxqrZZ0S12TfFE,99
+examples/v2_x/tutorial/hello_world_2/rails.co,sha256=4IrOympUyacq4rZoKgOjo3oKg7fPoqXqd96ptIIHRHQ,193
+examples/v2_x/tutorial/hello_world_3/README.md,sha256=wfjMweCM1E8X4EYPz6g3zRx0MSOA5g5O-kxndPOlwtU,676
+examples/v2_x/tutorial/hello_world_3/config.yml,sha256=pWgofbTZu4k5O1MhXFpNM_H9RmmnnGxqrZZ0S12TfFE,99
+examples/v2_x/tutorial/hello_world_3/rails.co,sha256=uQRPt85zU9-3EymLWDzhcSV6IRyfQhyx7OAcbvadUOc,267
+examples/v2_x/tutorial/interaction_loop/README.md,sha256=0e1926RmztLNNvb4UWGR5Jv8JgfiEqZJQcVY4VzbKlw,729
+examples/v2_x/tutorial/interaction_loop/config.yml,sha256=krHiOEtu0gcuYGRwl7n6cmEPhh9B-JSEOI_C-M8M_co,2294
+examples/v2_x/tutorial/interaction_loop/main.co,sha256=zjdRxuqt7JGDpN0pogICd4nsxdzrzWEtWIB4f-bYNzs,603
+examples/v2_x/tutorial/multi_modal/README.md,sha256=BmUpwVm4_bhxoRmRX702busezG84ilx8UZXJSX5A7x8,232
+examples/v2_x/tutorial/multi_modal/config.yml,sha256=pWgofbTZu4k5O1MhXFpNM_H9RmmnnGxqrZZ0S12TfFE,99
+examples/v2_x/tutorial/multi_modal/main.co,sha256=D6CAJIufY8KBnpNrEyxzYcdwVGToxE4Z0Y-LuC9AeBA,472
+nemoguardrails/__init__.py,sha256=qaM8mDwtX_C-meVWflhn7BKamAbEOdXysOR5FUwQhcM,983
 nemoguardrails/__main__.py,sha256=u_IU4KuOl--oje4SDuG6qo37JmEOhjICR7oO7yQMjEE,754
 nemoguardrails/context.py,sha256=AJDBjg5HEpWmX2DiyeWwYwl16Y8ooRsJdgSzzB7e95E,1263
 nemoguardrails/patch_asyncio.py,sha256=OzD_8FTNZmb7IJ9JENDDGZJ6pLeaFLJbzFAIsaILvNc,1537
 nemoguardrails/streaming.py,sha256=X-sBHfTacdv-tFU3lltO7HxlWsLUKzlaFPpLP19VUMc,11834
-nemoguardrails/utils.py,sha256=RR3om3fIG9BgY7XAzlYcfK4vY2hyV0DrDNr4HnJy9Y8,7821
+nemoguardrails/utils.py,sha256=UenAv_24zBixlqumgMBwK72kZWfD4BVFP8jPT83Sxxw,7936
 nemoguardrails/actions/__init__.py,sha256=t28kn9yUNYFSE3C-dwP46F1NXO92kRRUAHv4jVMHfCU,708
 nemoguardrails/actions/action_dispatcher.py,sha256=8Cw6BH-nM7rToMqIBFCgDyDvEVe3CFRpZ5nWR2rCiK4,12173
 nemoguardrails/actions/actions.py,sha256=MSzTjceDWqpnzzF2xfkrVzQu0ycd8UD8r_UigL3k18k,2312
 nemoguardrails/actions/core.py,sha256=K4y75B2bWJN3TcuU26oeE8qsCxsRkR6GgY-lB614IaA,1627
 nemoguardrails/actions/math.py,sha256=8W2Y4HQS_an3J1ufEHWVPZXQVgoHjiWys0h2pRRO7t8,3524
 nemoguardrails/actions/retrieve_relevant_chunks.py,sha256=minlw48F0Nr0JnPf-SeruYuNk0gf10TK6kpW-1xo3ng,3106
 nemoguardrails/actions/summarize_document.py,sha256=pmnXfL2Kpnc5p9jlJTaT_YuclQOrvozP6jHCcuNerKA,2127
 nemoguardrails/actions/langchain/__init__.py,sha256=rwCvdw7_3gzn8X-0PQDwN1nMw0yJ1xXVO8l77fMkGO0,679
 nemoguardrails/actions/langchain/actions.py,sha256=H1mfV6FyEc0DVPZJgPDYhulgAX9E16ichGc8CU73NJQ,1830
 nemoguardrails/actions/langchain/safetools.py,sha256=4sWkMUOuAo8yazDafQgQLUixv_g5MZc2Ua2zKkwB_mw,3678
 nemoguardrails/actions/llm/__init__.py,sha256=rwCvdw7_3gzn8X-0PQDwN1nMw0yJ1xXVO8l77fMkGO0,679
-nemoguardrails/actions/llm/generation.py,sha256=RRxtBlq4JnT1X8nQ_t44Gxc5n8pM8QPFg4HVmOvD5SQ,51162
-nemoguardrails/actions/llm/utils.py,sha256=h040SNg2r2v6hhyHBKp3040SdCSp-FInJS6R7N-b-WQ,17327
+nemoguardrails/actions/llm/generation.py,sha256=dQ-X-pGYYWz6KMZ2CWXFKsMZFsUp2F4cKaB3xsMQLC8,51308
+nemoguardrails/actions/llm/utils.py,sha256=Kfg9UT7jAUpAfqGS241s029k31X8dv50oea9xwfCb8g,17866
 nemoguardrails/actions/v2_x/__init__.py,sha256=rwCvdw7_3gzn8X-0PQDwN1nMw0yJ1xXVO8l77fMkGO0,679
-nemoguardrails/actions/v2_x/generation.py,sha256=W3UjqbrlAdt1U9voyyM4NT30Lu0z2K2QyHk_SuFSiGM,19117
+nemoguardrails/actions/v2_x/generation.py,sha256=QQHW4kPKc2mvI42yDFByAIp_7RQh16JO3CPIcCphPdM,21742
 nemoguardrails/actions/validation/__init__.py,sha256=fiTpXD0bf34FXxNjueIj5pzoaicGx9N-xFhIGfuEV8c,700
 nemoguardrails/actions/validation/base.py,sha256=bLu297t7luwsWdHw1m12OaGVrATzIWwZXXXA1-BYX3g,4518
 nemoguardrails/actions/validation/filter_secrets.py,sha256=JX-ExIb8fzW6DP6vwxjPJA-W6U5fwSgzkiplwwKp7HE,1375
 nemoguardrails/actions_server/__init__.py,sha256=rwCvdw7_3gzn8X-0PQDwN1nMw0yJ1xXVO8l77fMkGO0,679
 nemoguardrails/actions_server/actions_server.py,sha256=JL8PM4aR2CXXKNiZ-YzLF9PQvCzJ231DTbRY-6wayTE,2492
-nemoguardrails/cli/__init__.py,sha256=TnEEkgFsqWYdqsq1w2e8Xh5f2reT7UhKlfmNqtdjh0U,5960
-nemoguardrails/cli/chat.py,sha256=I_YRljulg8vTqVBajLJPurguo3ICGRuOaMnw_KZZaqo,25338
-nemoguardrails/cli/simplify_formatter.py,sha256=PY4bId580iBDqLTG9u-Z0O6BVdndAyg32WJrcdbmnME,2474
+nemoguardrails/cli/__init__.py,sha256=MEe0TUMuWgblyyzUZcjxCZd8FaDHJsg55jrYhCgT7A0,6086
+nemoguardrails/cli/chat.py,sha256=KFiqtXzejJUGW2vW9WAEMSlprM8msFIXabdOlNefqXc,23763
 nemoguardrails/colang/__init__.py,sha256=4FV4iWI7lSWrPkN3SUjk95ZuNICxzpJrqVsPB_pruro,1681
-nemoguardrails/colang/runtime.py,sha256=C0hOx2llTCbwb2PkDRrvV6SYa1vQgwCq3yg8tT2b8YM,3924
+nemoguardrails/colang/runtime.py,sha256=FmvfzaboArSJW_RY2xvfNXfb6YN5tn1HJcqMYodbbB0,4293
 nemoguardrails/colang/v1_0/__init__.py,sha256=rwCvdw7_3gzn8X-0PQDwN1nMw0yJ1xXVO8l77fMkGO0,679
 nemoguardrails/colang/v1_0/lang/__init__.py,sha256=rwCvdw7_3gzn8X-0PQDwN1nMw0yJ1xXVO8l77fMkGO0,679
 nemoguardrails/colang/v1_0/lang/colang_parser.py,sha256=2Eu6AH6Y4Ugm-tnjtEPQD6dcEKw0k0CV9fZyEbM87OY,73671
 nemoguardrails/colang/v1_0/lang/comd_parser.py,sha256=LPGHLkUZT_9pzX2C3HfxtOffkCDhIAWHc3LS4AeabqE,17098
 nemoguardrails/colang/v1_0/lang/coyml_parser.py,sha256=1FNYOYZxWslGl7oxNU6dVZYaLqqXRtQEihgaHA-bY2s,22118
 nemoguardrails/colang/v1_0/lang/parser.py,sha256=8JgaI-ewO37nfbvPi5dhU5dAan0TTn76z-P7_Ng-Guc,4314
 nemoguardrails/colang/v1_0/lang/utils.py,sha256=Cmi78UA-_s0mHaMWvPZgGpi4-reRU0EWoEG7tJl7QLU,11644
@@ -177,47 +283,52 @@
 nemoguardrails/colang/v1_0/runtime/flows.py,sha256=xhp1bskaYg0rd5N7i12-Md7F_33JqtT3vsqps6JGfUE,24260
 nemoguardrails/colang/v1_0/runtime/runtime.py,sha256=9yuckvx8Hzo4vvBwLuHaDR6-QMP-rE8dLgQJKCzbkMM,18621
 nemoguardrails/colang/v1_0/runtime/sliding.py,sha256=Q3_qnXdOxMn3DoP7UOddUXKmXaYETlVF3_VnMyu_F30,4942
 nemoguardrails/colang/v1_0/runtime/utils.py,sha256=G7c95NW6ipTkHKE5_5pDT2m6GmWeGw9AtorrDhMvQws,1881
 nemoguardrails/colang/v2_x/__init__.py,sha256=rwCvdw7_3gzn8X-0PQDwN1nMw0yJ1xXVO8l77fMkGO0,679
 nemoguardrails/colang/v2_x/lang/__init__.py,sha256=rwCvdw7_3gzn8X-0PQDwN1nMw0yJ1xXVO8l77fMkGO0,679
 nemoguardrails/colang/v2_x/lang/colang_ast.py,sha256=4LHKMhLlyymKf5SxeAIFjupplncXIfq3MmYrqKlojMo,10686
-nemoguardrails/colang/v2_x/lang/expansion.py,sha256=bVIcJcrQcnXDzZsL_2CV3OLbi5g0RY51tovuMOnFTLM,38410
-nemoguardrails/colang/v2_x/lang/parser.py,sha256=Iqtz_KJRcynIitdaxQ4Pn0ljrejL61oEBy-5Y-RieAs,5784
-nemoguardrails/colang/v2_x/lang/transformer.py,sha256=RWaDeb3Pq-lBegpt7ndXcg9s1O7-HLaciBXX5qVeaog,20879
+nemoguardrails/colang/v2_x/lang/expansion.py,sha256=dD_jOG4VdlwDMzpVvpyYlxUwEGR_BCnMEwPjUYmCQz4,39684
+nemoguardrails/colang/v2_x/lang/parser.py,sha256=FCiFU03c1TTof0om483g51Upaqvpf2yfg_bP3Nacd_4,6849
+nemoguardrails/colang/v2_x/lang/transformer.py,sha256=cOw1n1w-O91EwOkzyKv8nQlJ4uDCXhQL0oLTyaU6-Rg,20902
 nemoguardrails/colang/v2_x/lang/utils.py,sha256=BAL3l-jGgtOFrKyR27_UEskpOTT0HGOyJmSurRHC4i0,1296
 nemoguardrails/colang/v2_x/lang/grammar/__init__.py,sha256=rwCvdw7_3gzn8X-0PQDwN1nMw0yJ1xXVO8l77fMkGO0,679
-nemoguardrails/colang/v2_x/lang/grammar/colang.lark,sha256=LC78pyg5SGGvzoSDmXM8rRF7XRDPbCN5Nk752JKa9W0,7383
+nemoguardrails/colang/v2_x/lang/grammar/colang.lark,sha256=5tYO2Ol881sthEGqP87CcAayO3EzqjanF4FQjYlBjaw,7421
 nemoguardrails/colang/v2_x/lang/grammar/load.py,sha256=DAb_tzpw0SqqSQqJqjLqxX0cecUscdTrj_f5BXYbcIw,1314
-nemoguardrails/colang/v2_x/library/avatars.co,sha256=LlFw8XPgOqi29XjPGcxV4xEnDcmGg7aDfjugwDdLJ0M,20350
-nemoguardrails/colang/v2_x/library/core.co,sha256=9k1T2LsE21Bxm2M7LpFWc-NIeIDhhuw0rxUCi9bdcJ4,814
-nemoguardrails/colang/v2_x/library/utils.co,sha256=S37wpPD0Hza1fvVLkPq1rv8VbXDTbkrL_mc4cb4DygA,300
+nemoguardrails/colang/v2_x/library/avatars.co,sha256=IgTvaAx5IsbEGPjoQ3HA3CrbeTCk89qEyE0F3c7NEzo,20852
+nemoguardrails/colang/v2_x/library/core.co,sha256=Uj_OpihlOJSFWIOEsZBLKMzWdeOetW9vCajLcEFIfaU,713
+nemoguardrails/colang/v2_x/library/guardrails.co,sha256=idZlSGv9-5E9moHlz0BAC1Bq9eA_H-zaVwYCb3iToeU,1184
+nemoguardrails/colang/v2_x/library/llm.co,sha256=LkwgX4pNfBIM3ejAKQXaAnPXE5uR1TIx2qecsLNvNfE,3931
+nemoguardrails/colang/v2_x/library/utils.co,sha256=w_vdAiQfythZMjrNolj23ZYsBIpUJv-0phBElDOzUFM,683
 nemoguardrails/colang/v2_x/runtime/__init__.py,sha256=rwCvdw7_3gzn8X-0PQDwN1nMw0yJ1xXVO8l77fMkGO0,679
-nemoguardrails/colang/v2_x/runtime/eval.py,sha256=5iU4pQh2Xd-yrbIZV-X7hk9QK4atNJHvfY570VFUo7k,6294
-nemoguardrails/colang/v2_x/runtime/flows.py,sha256=x4bKM9KJ6BIzdFVMMHHvF6FlSflamoEqQWTZZ7ne9NE,26208
-nemoguardrails/colang/v2_x/runtime/runtime.py,sha256=1plXbOA96p0H-50vpT7rK8HtpSJ7OMCwZKSw49z8LhM,24744
-nemoguardrails/colang/v2_x/runtime/statemachine.py,sha256=nuP2AREEzemKuT--w1UNure5n02c42bkJjzUVbe1hCo,88576
+nemoguardrails/colang/v2_x/runtime/errors.py,sha256=vSpF9rCccr7EpyMcXcNk5y3sgpv1dWnG1S1BCItp0WI,553
+nemoguardrails/colang/v2_x/runtime/eval.py,sha256=RAcjQzWaQFQ4RTBL4rlXCjh9DPEyecDbb8NlMs8jEoo,11075
+nemoguardrails/colang/v2_x/runtime/flows.py,sha256=AFuG2zL7FpM0usWteem8Aq736SAw14rWWkBzzHKvQ28,29322
+nemoguardrails/colang/v2_x/runtime/runtime.py,sha256=q4CVxxUC_olye1qeH7nHALcmPDjgwIxwVTmspVSv05U,29461
+nemoguardrails/colang/v2_x/runtime/serialization.py,sha256=kPW1YpqJSJcR8N-6QaVKJRau9b6AxcT-ZyVcI_3W_Zo,7804
+nemoguardrails/colang/v2_x/runtime/statemachine.py,sha256=XAZ4iNowsfe03otcxIEBSpRVOUbv2CdNMZGx_bvxF3s,90268
 nemoguardrails/colang/v2_x/runtime/system_functions.py,sha256=2BR6gie0If838zZgTEe2f0bMfpMmKv9S6_tNin66y1M,1050
 nemoguardrails/colang/v2_x/runtime/utils.py,sha256=y_T_D267oOFbobcW_AIGzL-zLQ7HmEM1I49bphl5B44,1459
 nemoguardrails/embeddings/__init__.py,sha256=rwCvdw7_3gzn8X-0PQDwN1nMw0yJ1xXVO8l77fMkGO0,679
 nemoguardrails/embeddings/basic.py,sha256=NL0qcFFgboXSlgEZ3WRynd3NaD4eOR8xGzYc259iJjU,9994
 nemoguardrails/embeddings/cache.py,sha256=1YgmaFR2jnkri4chg3IL8Fow3hV7vbsyphcqG66J_yc,9773
 nemoguardrails/embeddings/index.py,sha256=hEkJMuYNOgMiNeyV527w9JB2IosEcyl51ARlWN-hRRQ,2438
 nemoguardrails/embeddings/embedding_providers/__init__.py,sha256=FspuMUAXynewIY5oNsQWbclGYKe2inyk73mtN2c-ZfQ,3268
 nemoguardrails/embeddings/embedding_providers/fastembed.py,sha256=1taRkphLqUw3VIcVYMbbtwrBkGPa6ksiEMGCcmJnTA0,3285
 nemoguardrails/embeddings/embedding_providers/openai.py,sha256=sff8lLUEdaNOG6VS_Zywc824tuHOaYVkZAARheOx_n4,3983
 nemoguardrails/embeddings/embedding_providers/sentence_transformers.py,sha256=i42ZsO7Hc3dmHiCJH11MS1LUxX7pv2md9Oekb40WzMA,2776
 nemoguardrails/eval/__init__.py,sha256=rwCvdw7_3gzn8X-0PQDwN1nMw0yJ1xXVO8l77fMkGO0,679
 nemoguardrails/eval/evaluate_factcheck.py,sha256=JjBk3ASvOFEsXGheFKyVRqQz7zJ1PuQouoT3YfoaRqo,9013
-nemoguardrails/eval/evaluate_hallucination.py,sha256=lwTCaoJwZ0NzTbjan3NiGeTYdJSosHUG_qhj5kBHhg8,6953
-nemoguardrails/eval/evaluate_moderation.py,sha256=9IWqQ9PZ4C1ga5oXHPHaumxoFOiJ5hyFIY3oz5Mda58,8580
-nemoguardrails/eval/evaluate_topical.py,sha256=n8YtStZekbgsAcvy5DQxqi60mebzlyWudOygVGc7-QM,17162
+nemoguardrails/eval/evaluate_hallucination.py,sha256=MJdZg4wBlUAzVb6DLtLzavGfC6P3ZCxB95N8ajq8aoA,8606
+nemoguardrails/eval/evaluate_moderation.py,sha256=g7Knwqi1tpGVnmZaEkYmUKS7cQSdau1oC8ag3kro9ho,9657
+nemoguardrails/eval/evaluate_topical.py,sha256=a__SX_z6o90XrqS2w-qXAg_xYMkB1sHpogSd27wg6nw,17715
 nemoguardrails/eval/utils.py,sha256=7I3IG3VtBwH2jjFC0bkdF3CSCszm65_8psfPK547zDY,1685
 nemoguardrails/eval/cli/__init__.py,sha256=rwCvdw7_3gzn8X-0PQDwN1nMw0yJ1xXVO8l77fMkGO0,679
 nemoguardrails/eval/cli/evaluate.py,sha256=TLOoAG4pwhihraBZrbKjKHU8svbOi6GBl_t8PwvHndw,10465
+nemoguardrails/eval/cli/simplify_formatter.py,sha256=ysEsLPcJS_N2U1pPRd4Og7ViZEXAA_1NYe-B0W1qWG4,2572
 nemoguardrails/eval/data/factchecking/README.md,sha256=aNbN_tIYXxm91O1Au363kFDOQPvw4XxEwayPmKt4_0c,4208
 nemoguardrails/eval/data/factchecking/__init__.py,sha256=rwCvdw7_3gzn8X-0PQDwN1nMw0yJ1xXVO8l77fMkGO0,679
 nemoguardrails/eval/data/factchecking/process_msmarco_data.py,sha256=qKsx0n36twfTQoIhf-5OpnG5XTpu47yXFE9iOls3yjs,1539
 nemoguardrails/eval/data/factchecking/sample.json,sha256=fuvKeRyS4TB7z5kuqxRU6QyOmsD6spljrrCnG42mAW0,283
 nemoguardrails/eval/data/hallucination/__init__.py,sha256=rwCvdw7_3gzn8X-0PQDwN1nMw0yJ1xXVO8l77fMkGO0,679
 nemoguardrails/eval/data/hallucination/sample.txt,sha256=eyGDdVJoBqMWN1bpLFbysq2qe1cz8GouC16JD6GzbCM,1071
 nemoguardrails/eval/data/moderation/README.md,sha256=a2WUMaRHqjYVkto4NBT4byRDP3wZz4QQg-3Gn215-xs,3956
@@ -235,15 +346,15 @@
 nemoguardrails/eval/data/topical/chitchat/bot.co,sha256=8Ifvr0FAkdJiYd4Bg7DLfrn9oft-VALwqhPfsk9DXdU,6118
 nemoguardrails/eval/data/topical/chitchat/config.yml,sha256=Bbrfd0knLGwNX91uXQsFiaiuOwV6Fk_YwqpJQ7eWld4,863
 nemoguardrails/eval/data/topical/chitchat/flows.co,sha256=tA9h5YvQHkoAc6Lha3TlxWfRQst3Pqo8h0oWxtnOJ48,6844
 nemoguardrails/eval/data/topical/chitchat/intent_canonical_forms.json,sha256=AQTVpYMJ5NYTyncgBtcTZKoxluIfoLgfA_p0QhzlsnY,3260
 nemoguardrails/eval/data/topical/chitchat/user-other.co,sha256=W8iRGb9R_USi_mtjmGI8rN-OlgNNl0Q2Dx3z-SWMxTQ,199
 nemoguardrails/integrations/__init__.py,sha256=rwCvdw7_3gzn8X-0PQDwN1nMw0yJ1xXVO8l77fMkGO0,679
 nemoguardrails/integrations/langchain/__init__.py,sha256=rwCvdw7_3gzn8X-0PQDwN1nMw0yJ1xXVO8l77fMkGO0,679
-nemoguardrails/integrations/langchain/runnable_rails.py,sha256=5sbQMuQLBGnlHMhQh0Mp_30qFUP-cx7XXScuU_L74RU,9158
+nemoguardrails/integrations/langchain/runnable_rails.py,sha256=VrrIY-NjM_llGkYQUUKZRKNVJEoZ8mQ-gFb2at-QLF8,9237
 nemoguardrails/kb/__init__.py,sha256=rwCvdw7_3gzn8X-0PQDwN1nMw0yJ1xXVO8l77fMkGO0,679
 nemoguardrails/kb/kb.py,sha256=rYPTGjdqIG_-lOHBnaQuOSg0sQtZ9YOaxit7zHrSaU4,7342
 nemoguardrails/kb/utils.py,sha256=FIGcq_v5HLpFCFYsDs4G5NL-f_uAw_klNEDtzrjVyxc,3878
 nemoguardrails/library/__init__.py,sha256=rwCvdw7_3gzn8X-0PQDwN1nMw0yJ1xXVO8l77fMkGO0,679
 nemoguardrails/library/activefence/__init__.py,sha256=rwCvdw7_3gzn8X-0PQDwN1nMw0yJ1xXVO8l77fMkGO0,679
 nemoguardrails/library/activefence/actions.py,sha256=YjZgSu1hHZx8Z2aozeGC2uOaoeKpt6CngRHQC2nYuaY,2430
 nemoguardrails/library/activefence/flows.co,sha256=VlHVO90tsvky_BSsN0hZg7Onqyv3FD-2cF2kjWvKjqU,2249
@@ -252,15 +363,15 @@
 nemoguardrails/library/factchecking/align_score/actions.py,sha256=XmFntdgL6gFUZH0NRGzStenVFtcfUNnKCookd7NgTZc,2222
 nemoguardrails/library/factchecking/align_score/flows.co,sha256=V0cQFwWewqSCuuyqQZxt5llWf8B2kyGDaqqfLP5vvE4,405
 nemoguardrails/library/factchecking/align_score/request.py,sha256=JRR0VIHmdRt4n-M1lg5GMH4kX-SAHXRHbPg0qVHyFTw,1714
 nemoguardrails/library/factchecking/align_score/requirements.txt,sha256=H1vjld230Kr37Rm-38ac1GXXK5LMPIBZaKRvVu5mR_8,147
 nemoguardrails/library/factchecking/align_score/server.py,sha256=ibbUq6ILRUs3JwZ6JfsuY360Jsz40VXleqDzqZqxXVk,3373
 nemoguardrails/library/hallucination/__init__.py,sha256=rwCvdw7_3gzn8X-0PQDwN1nMw0yJ1xXVO8l77fMkGO0,679
 nemoguardrails/library/hallucination/actions.py,sha256=oYOA0mWgDSoWOOTp_lmLSQfShtioEE4mlgkwo7aeaho,5280
-nemoguardrails/library/hallucination/flows.co,sha256=AZ2OFh0kjwHAw1n96V0l_0Ctw7BffaMuz2mjc1bYoQ0,892
+nemoguardrails/library/hallucination/flows.co,sha256=StpFUsfZ6Fx9wfObSj9Q1xSUJVG418jEvtYQ22SoVR4,895
 nemoguardrails/library/jailbreak_detection/__init__.py,sha256=rwCvdw7_3gzn8X-0PQDwN1nMw0yJ1xXVO8l77fMkGO0,679
 nemoguardrails/library/jailbreak_detection/actions.py,sha256=qJCOTWTnOAJrOPfxfNeDaKRt10k4yqjhABfhNsJNqsU,2492
 nemoguardrails/library/jailbreak_detection/flows.co,sha256=ozwr3HATRx1DTnc0Eq7pZrE8X1BdzqDgOKlAt-112AI,322
 nemoguardrails/library/jailbreak_detection/request.py,sha256=i9Mn8H26g_JosvKhRWGzJjRug1DLpSKymqe9VJjODT4,1739
 nemoguardrails/library/jailbreak_detection/requirements.txt,sha256=Yxfu09NnZbQyCv1NlUIG_8eiNmbb2r1Xo96fMUsPYdU,214
 nemoguardrails/library/jailbreak_detection/server.py,sha256=01KcClgREmDLLSYVcNttBrTkwwWvHaIv78jUxTRWF5M,3063
 nemoguardrails/library/jailbreak_detection/heuristics/__init__.py,sha256=rwCvdw7_3gzn8X-0PQDwN1nMw0yJ1xXVO8l77fMkGO0,679
@@ -288,47 +399,49 @@
 nemoguardrails/llm/output_parsers.py,sha256=EsoabSILLZEQk_PvMUVSoeToupW9aO20VBnXUbsQ6jA,2237
 nemoguardrails/llm/params.py,sha256=zd6BjK-VSeCnH0xFn1zm7fPHmcSovlmiolLJkq2nF2Y,3179
 nemoguardrails/llm/prompts.py,sha256=b2qIyNbDs7l2HCI9m8lXdt-N8G_yBzE1AKzvYGG8Sgw,5154
 nemoguardrails/llm/taskmanager.py,sha256=dkuisnJ0uo4YQG5RFCsJhHoUUq5Olp6Z5xthEW1Q2L8,10800
 nemoguardrails/llm/types.py,sha256=wqD-T8bQ5lH6fBbEPuX5Opv0Cp5xOrmzFfc8itHe6_Y,1750
 nemoguardrails/llm/prompts/cohere.yml,sha256=PMMMcq2pRH_dSFwKC-GBoyjlgpaTiN23VTDYgaKbK1w,2608
 nemoguardrails/llm/prompts/dolly.yml,sha256=RYx4KaDiOyXR3pGAiajQp8r2mw3O8X2WKqUe-A4wPh0,2887
-nemoguardrails/llm/prompts/general.yml,sha256=lXVNeJ3U-Hc0P2pE7sxcofNakSyG8QW1BLPy2Q_0nTA,5700
+nemoguardrails/llm/prompts/general.yml,sha256=7EXbAHp1rajvxKG0upLHJ0o28hwDJ4BIg-M8YvG-okw,5739
 nemoguardrails/llm/prompts/mosaic.yml,sha256=uJKYWLkeym8306amEcStRKgE3UQEHW74QpYBuRMPutM,3235
 nemoguardrails/llm/prompts/nemollm.yml,sha256=VIevuO9g3aJzYIm1jJctE4LIpe-uuEh_yBPbOGbCK-g,6149
 nemoguardrails/llm/prompts/openai-chatgpt.yml,sha256=loUBq5UFjJMdw2JrtZhScuc-YOumq8Rj1Hjo6bsZc40,3547
 nemoguardrails/llm/prompts/openai-gpt35-instruct.yml,sha256=AjE-WuduL9nRnt-IJRNkr8sqE1iyAN7zNaKjoGXDdw4,57
 nemoguardrails/llm/providers/__init__.py,sha256=4_INebPA1T4dEcmPKsvG7RELoB0BFjGUWasOvG-oqQM,819
 nemoguardrails/llm/providers/nemollm.py,sha256=fN-34-Ms81g5ysVyi9xaHBwXjw8dldB9ERQ4a32DDos,11487
-nemoguardrails/llm/providers/providers.py,sha256=tzw84LrtQzU7kxS_ENi5511eiDzNZdDicmWXnY0GwSI,8308
+nemoguardrails/llm/providers/providers.py,sha256=YsbRsq996G8yxsZeqa2rmAbuty1obA8mIspJfFH2gyo,9495
 nemoguardrails/llm/providers/huggingface/__init__.py,sha256=YyabYojBgcm6qsXfCCkrTAo5UHANPZTqDzMGQ0iTCwQ,729
 nemoguardrails/llm/providers/huggingface/streamers.py,sha256=OBaiEkK6j9oJRVLvGEvrCBBzFMNC0_XqD4W3G4-itV8,1939
 nemoguardrails/llm/providers/trtllm/__init__.py,sha256=rwCvdw7_3gzn8X-0PQDwN1nMw0yJ1xXVO8l77fMkGO0,679
 nemoguardrails/llm/providers/trtllm/client.py,sha256=cVSk-FEV9_2jdmsTKtG-OuNaPwIxfVlAWQY1rAo9vhY,8353
 nemoguardrails/llm/providers/trtllm/llm.py,sha256=OnJwjpPJhNbKNtsIzi4z3ofNM-8EkpGyuaMlNuHz5mc,5827
 nemoguardrails/logging/__init__.py,sha256=rwCvdw7_3gzn8X-0PQDwN1nMw0yJ1xXVO8l77fMkGO0,679
-nemoguardrails/logging/callbacks.py,sha256=MyOhIhog3c2nJMvWXWewnZ_0Nv_5f4D1VnrF2tQe8SI,9675
+nemoguardrails/logging/callbacks.py,sha256=KtlJXGFz3ZLkri8G92rwz7P0fkggfA6W3795dTEdRjM,9703
 nemoguardrails/logging/explain.py,sha256=05fiYnakypPBXtxTPTxqsb7AamkzI_wal_kNDtMa2C8,3710
 nemoguardrails/logging/processing_log.py,sha256=UdurB_28v55T4tfU6D2yVNyvfd3ccl81HEYTcP3W-7Y,10887
+nemoguardrails/logging/simplify_formatter.py,sha256=LWt53x_2iHwcmzBBaL9XWzNJftsDowfXzv0oyrQAKfU,2041
 nemoguardrails/logging/stats.py,sha256=fNOv0V1zuR5SkRrJwvCF8f_1Qbj1pIrSDsVaExBqxwk,2062
-nemoguardrails/logging/verbose.py,sha256=oB_trbmjIyx9ZDAwny5UVOAZkpyO_ZxAxBRzJd5ZH0w,6887
+nemoguardrails/logging/verbose.py,sha256=Z3Zne0NUvfEWakqzMAlpN_-A-rziJTHPScV1WeA0KwU,9095
 nemoguardrails/rails/__init__.py,sha256=PfR78f2dUwo71w_y7pTO3sj-ellklpKHKCrkmJt9WIQ,751
 nemoguardrails/rails/llm/__init__.py,sha256=rwCvdw7_3gzn8X-0PQDwN1nMw0yJ1xXVO8l77fMkGO0,679
-nemoguardrails/rails/llm/config.py,sha256=v9PKoklk6dq1vRm5IXrmNMI0LflNZzrUYfm2Nuff_u8,34588
+nemoguardrails/rails/llm/config.py,sha256=2w3NuluZnwvdMMxVhW2ip1rmpLgxAsZVLwFBS8Q1-pk,35933
 nemoguardrails/rails/llm/default_config.yml,sha256=M7kbOe96OAxJkUQAqMabxsmRUhwt6IpWjRXZohcR-_4,1906
-nemoguardrails/rails/llm/llm_flows.co,sha256=_RB8GdsaC2iACeCVQljvF7AmvC_mHLeaYWxUiw53aGU,4667
-nemoguardrails/rails/llm/llmrails.py,sha256=yomxtSWNxCwzTxns8a47XWYOgpuh38SZoKvJY_WyNqI,36117
-nemoguardrails/rails/llm/options.py,sha256=wqGQLvcisPYCHTYAa-Yk366SROer76eJmy0A6c14y-k,15237
+nemoguardrails/rails/llm/default_config_v2.yml,sha256=KBYZVjCxx0BQOBz1hvlJYKNC6kmOBoYLm1SXVOzxRs4,3123
+nemoguardrails/rails/llm/llm_flows.co,sha256=hhAKybxtFmjj5AzXnyd-LvjszUsCew2-LMBKWwm2cM0,4668
+nemoguardrails/rails/llm/llmrails.py,sha256=iKrPnvwQgXzWs0P6IQ6TXl7C7nIWdEuuyoxEyAd3qpQ,42747
+nemoguardrails/rails/llm/options.py,sha256=LmZC_ZMidAHyefQvUGdtgUdJVd2hsNMDXRREW3Pnj9A,15405
 nemoguardrails/rails/llm/utils.py,sha256=ebmx4O5vKuTDTdy0L-W2m9vPN22UHwY18jXxThUIS_k,1511
 nemoguardrails/server/__init__.py,sha256=rwCvdw7_3gzn8X-0PQDwN1nMw0yJ1xXVO8l77fMkGO0,679
-nemoguardrails/server/api.py,sha256=pS-iggblxPF-OvLCD0t_BhKpRp-xFwNUV_oTnW_t_2M,18413
+nemoguardrails/server/api.py,sha256=qyaNsFQ2XW2LGUVpytBEe4I-nnlGlPkqVLuJncEoUuU,18827
 nemoguardrails/server/datastore/__init__.py,sha256=rwCvdw7_3gzn8X-0PQDwN1nMw0yJ1xXVO8l77fMkGO0,679
 nemoguardrails/server/datastore/datastore.py,sha256=DKwqS2lvwbLfrE8woFHGPl5l5hg72J__0yQKQp8FQdM,1305
 nemoguardrails/server/datastore/memory_store.py,sha256=bVzbRHjA4cV1IoX7uXJ8g7IoHD0GsAVe115Cx2psz8M,1469
 nemoguardrails/server/datastore/redis_store.py,sha256=GVgfmFOY3On3SNqIUytb0yfKQhQP_VXMaIQYf9R_F-U,2090
-nemoguardrails-0.8.1.dist-info/LICENSE-Apache-2.0.txt,sha256=iVnec2XXUJMmHJsPan8KLNruAjvd6j24-FgWKP79H3I,9861
-nemoguardrails-0.8.1.dist-info/LICENSE.md,sha256=fBN5BZcuYZnkEgcWhR7fM04hEAjA7zaT6yaVeLiirCo,654
-nemoguardrails-0.8.1.dist-info/METADATA,sha256=XSKMCGE2_oRes_k8OFCCMlI-IO9afvxvvlMHWet24bY,20573
-nemoguardrails-0.8.1.dist-info/WHEEL,sha256=GJ7t_kWBFywbagK5eo9IoUwLW6oyOeTKmQ-9iHFVNxQ,92
-nemoguardrails-0.8.1.dist-info/entry_points.txt,sha256=IUJ-3EgGJqAlsmG4WYSKElhzaIBPaO6nvT187zlBgj4,63
-nemoguardrails-0.8.1.dist-info/top_level.txt,sha256=TCafwIaWll55IGWiXdr8YOFFI7rRud8mS-N82_CYL5Q,15
-nemoguardrails-0.8.1.dist-info/RECORD,,
+nemoguardrails-0.8.2.dist-info/LICENSE-Apache-2.0.txt,sha256=iVnec2XXUJMmHJsPan8KLNruAjvd6j24-FgWKP79H3I,9861
+nemoguardrails-0.8.2.dist-info/LICENSE.md,sha256=fBN5BZcuYZnkEgcWhR7fM04hEAjA7zaT6yaVeLiirCo,654
+nemoguardrails-0.8.2.dist-info/METADATA,sha256=RsOiLW0PwZkUPpp9BhFpK0yhnKwK-HttWW4bbufyOKE,20573
+nemoguardrails-0.8.2.dist-info/WHEEL,sha256=GJ7t_kWBFywbagK5eo9IoUwLW6oyOeTKmQ-9iHFVNxQ,92
+nemoguardrails-0.8.2.dist-info/entry_points.txt,sha256=IUJ-3EgGJqAlsmG4WYSKElhzaIBPaO6nvT187zlBgj4,63
+nemoguardrails-0.8.2.dist-info/top_level.txt,sha256=TCafwIaWll55IGWiXdr8YOFFI7rRud8mS-N82_CYL5Q,15
+nemoguardrails-0.8.2.dist-info/RECORD,,
```

